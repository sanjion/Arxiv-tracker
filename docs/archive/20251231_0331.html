<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-31 03:31</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251231_0331</div>
    <div class="row"><div class="card">
<div class="title">Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion</div>
<div class="meta-line">Authors: Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Po-Fan Yu, Yu-Chih Chen, Yu-Lun Liu</div>
<div class="meta-line">First: 2025-12-29T18:59:57+00:00 · Latest: 2025-12-29T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://jamichss.github.io/stream-diffvsr-project-page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23709v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23709v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jamichss.github.io/stream-diffvsr-project-page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Stream-DiffVSR：基于自回归扩散的低延迟流式视频超分辨率</div>
<div class="mono" style="margin-top:8px">基于扩散的视频超分辨率（VSR）方法在感知质量方面表现出色，但由于依赖未来帧和昂贵的多步去噪，它们在延迟敏感的设置中仍不实用。我们提出了Stream-DiffVSR，这是一种因果条件下的扩散框架，用于高效的在线VSR。该框架仅基于过去帧操作，结合了四步精简去噪器以实现快速推理，Auto-regressive Temporal Guidance (ARTG) 模块在潜空间去噪期间注入运动对齐的线索，以及具有Temporal Processor Module (TPM) 的轻量级时空解码器，以增强细节和时间连贯性。Stream-DiffVSR 在 RTX4090 GPU 上处理 720p 帧仅需 0.328 秒，并显著优于先前的基于扩散的方法。与在线 SOTA TMP 相比，它在感知质量（LPIPS +0.095）方面有所提升，同时将延迟降低了超过 130 倍。Stream-DiffVSR 实现了基于扩散的 VSR 中报告的最低延迟，将初始延迟从超过 4600 秒降低到 0.328 秒，从而使其成为第一个适合低延迟在线部署的扩散 VSR 方法。项目页面：https://jamichss.github.io/stream-diffvsr-project-page/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Stream-DiffVSR is a causally conditioned diffusion framework for efficient online video super-resolution, addressing the latency issues of previous methods. It uses a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance module to inject motion-aligned cues, and a lightweight temporal-aware decoder with a Temporal Processor Module. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU, significantly outperforming prior methods in terms of perceptual quality and latency, making it suitable for low-latency online deployment.</div>
<div class="mono" style="margin-top:8px">Stream-DiffVSR 是一种因果条件下的扩散框架，用于高效的在线视频超分辨率，解决了之前方法的延迟问题。它使用四步蒸馏去噪器、自回归时空引导模块和具有时空处理器模块的轻量级时空感知解码器。Stream-DiffVSR 在 RTX4090 GPU 上处理 720p 帧仅需 0.328 秒，显著优于先前方法在感知质量和延迟方面的表现，使其适合低延迟的在线部署。</div>
</details>
</div>
<div class="card">
<div class="title">Training AI Co-Scientists Using Rubric Rewards</div>
<div class="meta-line">Authors: Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse</div>
<div class="meta-line">First: 2025-12-29T18:59:33+00:00 · Latest: 2025-12-29T18:59:33+00:00</div>
<div class="meta-line">Comments: 11 pages in the main paper, total 119 including sample outputs in the Appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23707v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用评分标准奖励训练AI合科学家</div>
<div class="mono" style="margin-top:8px">AI合科学家正在成为一种辅助人类研究人员实现研究目标的工具。这些AI合科学家的关键特征是能够根据目标和约束生成研究计划。该计划可用于头脑风暴，甚至在进一步完善后实施。然而，当前的语言模型在生成遵循所有约束和隐含要求的研究计划方面存在困难。在本研究中，我们探讨了如何利用大量现有的研究论文来训练能够生成更好研究计划的语言模型。我们通过自动从多个领域论文中提取研究目标和目标特定的评分标准，构建了一个可扩展且多样化的训练语料库。然后，我们通过自我评分的强化学习训练研究计划生成模型。在训练过程中，冻结的初始策略充当评分者，评分标准创建生成者-验证者差距，从而在无需外部人类监督的情况下实现改进。为了验证此方法，我们在机器学习研究目标上进行了为期225小时的人类专家研究，其中70%的研究目标，专家更偏好我们微调的Qwen3-30B-A3B模型生成的计划，且84%的自动提取的目标特定评分标准被批准。为了评估其普适性，我们还将此方法扩展到医学论文和新的arXiv预印本的研究目标上，并使用前沿模型组成的陪审团进行评估。我们的微调带来了12-22%的相对改进，并且在跨领域泛化方面表现出显著效果，即使在执行反馈不可行的问题设置（如医学研究）中也证明了其有效性。这些发现共同证明了可扩展的自动化训练食谱作为提高通用AI合科学家的一个步骤的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to enhance the capability of AI co-scientists to generate research plans that adhere to constraints and implicit requirements. The authors developed a scalable training corpus by extracting research goals and grading rubrics from various domains. Using reinforcement learning with self-grading, they trained models to generate better research plans. Human experts validated the approach, preferring plans generated by the finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals and approving 84% of the grading rubrics. The method also showed significant cross-domain generalization, improving performance by 12-22% in medical research settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决训练AI合作者生成符合约束和隐含要求的研究计划的挑战。通过利用研究论文的大规模语料库和目标特定的评分标准，作者使用自我评分的强化学习来训练模型。经过微调的Qwen3-30B-A3B模型在70%的机器学习研究目标中优于初始模型，并且在跨领域的泛化方面表现出显著改进，即使在执行反馈不可行的医学研究中也是如此。该方法展示了自动化训练在提高AI合作者方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</div>
<div class="meta-line">Authors: Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao</div>
<div class="meta-line">First: 2025-12-29T18:59:24+00:00 · Latest: 2025-12-29T18:59:24+00:00</div>
<div class="meta-line">Comments: Project Page: https://daniellli.github.io/projects/DKT/; Code: https://github.com/Daniellli/DKT; Dataset: https://huggingface.co/datasets/Daniellesry/TransPhy3D</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23705v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23705v1">PDF</a> · <a href="https://github.com/Daniellli/DKT">Code1</a> · <a href="https://huggingface.co/datasets/Daniellesry/TransPhy3D">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a> · <a href="https://daniellli.github.io/projects/DKT/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT&#x27;s depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: &quot;Diffusion knows transparency.&quot; Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散技术揭示透明性：重新利用视频扩散技术进行透明物体深度和法线估计</div>
<div class="mono" style="margin-top:8px">透明物体对感知系统来说一直非常难以处理：折射、反射和穿透破坏了立体视觉、ToF和纯判别单目深度背后的假设，导致空洞和时间上不稳定的估计。我们的关键观察是，现代视频扩散模型已经合成了令人信服的透明现象，表明它们已经内化了光学规则。我们构建了TransPhy3D，一个透明/反射场景的合成视频数据集：包含11000个序列，使用Blender/Cycles渲染。场景由一个精心挑选的类别丰富的静态资产库和形状丰富的程序化资产组成，这些资产与玻璃/塑料/金属材料配对。我们使用基于物理的光线追踪渲染RGB + 深度 + 法线，并使用OptiX降噪。从一个大型视频扩散模型开始，我们通过轻量级LoRA适配器学习了一个视频到视频的深度（和法线）翻译器。在训练过程中，我们在DiT主干中连接RGB和（有噪声的）深度潜在变量，并在TransPhy3D和现有的帧级合成数据集上共同训练，从而为任意长度的输入视频生成时间上一致的预测。最终模型DKT在涉及透明性的实际和合成视频基准测试中实现了零样本最佳效果：ClearPose、DREDS（CatKnown/CatNovel）和TransPhy3D-Test。它在准确性和时间一致性方面优于强大的图像/视频基线，并且法线变体在ClearPose上设定了最佳的视频法线估计结果。紧凑的1.3B版本运行速度约为每帧0.17秒。集成到抓取堆栈中，DKT的深度提高了对透明、反射和漫反射表面的成功率，优于先前的估计器。这些结果共同支持一个更广泛的主张：“扩散技术了解透明性。”生成的视频先验可以高效且无标签地重新利用，以实现稳健的时间上一致的感知，用于具有挑战性的实际世界操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of estimating depth and normals for transparent objects, which are difficult for traditional perception systems due to refraction and reflection. The authors propose TransPhy3D, a synthetic video corpus, and a video-to-video translator, DKT, trained using a lightweight LoRA adapter. DKT achieves state-of-the-art results on real and synthetic video benchmarks involving transparency, improving accuracy and temporal consistency over existing methods. The model integrates well into a grasping stack, enhancing success rates for various surface types.</div>
<div class="mono" style="margin-top:8px">论文解决了透明物体深度和法线估计的难题，由于折射和反射，这些物体对感知系统来说非常难以处理。研究提出了TransPhy3D合成视频数据集，并使用大型视频扩散模型和轻量级LoRA适配器训练了一个视频到视频的翻译器DKT。DKT在真实和合成基准测试中取得了最先进的成果，提高了准确性和时间一致性，并且在抓取堆栈中表现良好，提升了各种表面类型的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Eliciting Behaviors in Multi-Turn Conversations</div>
<div class="meta-line">Authors: Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews, John Lambert</div>
<div class="meta-line">First: 2025-12-29T18:57:10+00:00 · Latest: 2025-12-29T18:57:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23701v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多轮对话中行为诱引</div>
<div class="mono" style="margin-top:8px">在对话环境中识别大型语言模型（LLMs）的具体且通常复杂的特定行为对于其评估至关重要。最近的工作提出了新颖的技术来找到诱导目标模型产生特定行为的自然语言提示，但这些研究主要集中在单轮设置中。在本工作中，我们研究了多轮对话中的行为诱引。我们首先提供了一个分析框架，将现有方法分为三类，基于它们与目标模型的交互方式：仅使用先验知识的方法、使用离线交互的方法以及从在线交互中学习的方法。然后，我们引入了一种多轮在线方法的一般化形式，统一了单轮和多轮诱引。我们评估了这三类方法在自动生成多轮测试案例方面的表现。我们通过分析查询预算，即与目标模型交互的次数，与成功率，即行为诱引输入的发现率之间的权衡，来研究这些方法的效率。我们发现，在三个任务中，只需几千次查询，基于在线的方法就能实现45/19/77%的平均成功率，而现有的多轮对话基准中的静态方法在这些任务中发现的失败案例很少甚至没有。我们的工作突显了行为诱引方法在多轮对话评估中的新应用，并强调了社区需要转向动态基准的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of eliciting specific behaviors in multi-turn conversations from large language models (LLMs) by proposing an analytical framework to categorize existing methods into three families based on their interactions with the target model. The research introduces a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. Experimental results show that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries, outperforming static methods in three tasks where few or no failure cases were found previously.</div>
<div class="mono" style="margin-top:8px">本文通过提出一种分析框架，将现有方法按其与目标模型的交互方式分为三类，来解决在多轮对话中从大型语言模型（LLMs）中引出特定行为的挑战。研究引入了一种多轮在线方法的通用形式，统一了单轮和多轮引出。实验结果显示，与静态方法相比，在线方法仅用几千次查询就能在三个任务中实现45/19/77%的成功率，而在这些任务中，静态方法几乎没有或根本没有发现失败案例。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization</div>
<div class="meta-line">Authors: Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</div>
<div class="meta-line">First: 2025-10-09T17:58:07+00:00 · Latest: 2025-12-29T18:55:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08554v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08554v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce Group Diffusion Policy Optimization (GDPO), a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过组扩散策略优化提高扩散语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）能够以迭代细化的方式进行并行、无序生成，提供了一种灵活的替代自回归大型语言模型（LLMs）的选择。然而，将强化学习（RL）微调适应DLMs仍然是一个开放的挑战，因为难以计算似然性。早期工作如diffu-GRPO通过一次去遮蔽估计了令牌级别的似然性。尽管计算效率高，但这种方法严重有偏。更合理的基础在于序列级别的似然性，其中证据下界（ELBO）作为替代。尽管存在这种清晰的数学联系，基于ELBO的方法由于似然性评估成本高昂而受到限制。在本文中，我们重新审视了ELBO估计，并将其方差来源进行了分解。这种分解促使我们通过快速、确定性的积分近似来减少方差，沿着几个关键维度。基于这一见解，我们引入了组扩散策略优化（GDPO），这是一种新的针对DLMs的RL算法。GDPO利用简单的半确定性蒙特卡洛方案来缓解在常规双蒙特卡洛采样下ELBO估计器的方差爆炸，从而在严格的评估预算下提供一个方差更低的估计器。实验上，GDPO在预训练检查点上实现了持续的收益，并在大多数数学、推理和编码基准测试中优于diffu-GRPO，这是当前最先进的基线之一。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of applying reinforcement learning (RL) fine-tuning to diffusion language models (DLMs) by revisiting the estimation of sequence-level likelihoods. The authors introduce Group Diffusion Policy Optimization (GDPO), which uses semi-deterministic Monte Carlo schemes to reduce the variance of ELBO estimators, leading to a more efficient and lower-variance RL algorithm. GDPO outperforms diffu-GRPO and pretrained checkpoints on math, reasoning, and coding benchmarks.</div>
<div class="mono" style="margin-top:8px">本文解决了将强化学习（RL）微调应用于扩散语言模型（DLMs）的挑战，通过重新审视序列级似然性的估计。作者引入了Group Diffusion Policy Optimization（GDPO），使用半确定性蒙特卡洛方案来减少ELBO估计器的方差，从而在严格的评估预算下获得方差更低的估计器。GDPO在数学、推理和编码基准测试中优于diffu-GRPO，并且在预训练检查点上实现了持续的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Bellman Calibration for V-Learning in Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Lars van der Laan, Nathan Kallus</div>
<div class="meta-line">First: 2025-12-29T18:52:18+00:00 · Latest: 2025-12-29T18:52:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23694v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model&#x27;s predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝尔曼校准在离线强化学习中V学习的应用</div>
<div class="mono" style="margin-top:8px">我们引入了迭代贝尔曼校准，这是一种简单的、模型无关的后处理程序，用于校准无限时域马尔可夫决策过程中的离策价值预测。贝尔曼校准要求具有相似长期回报的态在一步回报上与目标策略下的贝尔曼方程一致。我们通过反复将拟合的贝尔曼目标回归到模型的预测，并使用双重稳健的伪结果来处理离策数据，将经典的直方图校准和等向性校准适应到动态的、反事实的环境中。这产生了一维拟合值迭代方案，可以应用于任何价值估计器。我们的分析在弱假设下提供了校准和预测的有限样本保证，并且关键地，无需贝尔曼完备性或现实性假设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research introduces Iterated Bellman Calibration, a method for calibrating off-policy value predictions in reinforcement learning. This method involves repeatedly regressing fitted Bellman targets onto a model&#x27;s predictions using a doubly robust pseudo-outcome to handle off-policy data. The key finding is that this approach provides finite-sample guarantees for both calibration and prediction under weak assumptions, without needing Bellman completeness or realizability.</div>
<div class="mono" style="margin-top:8px">研究引入了迭代贝尔曼校准方法，用于强化学习中的离策价值预测校准。该方法通过反复将拟合的贝尔曼目标回归到模型的预测上来确保具有相似长期回报的状态具有一致的一步回报。关键发现包括在弱假设下对校准和预测的有限样本保证，无需贝尔曼完备性或现实性。</div>
</details>
</div>
<div class="card">
<div class="title">Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection</div>
<div class="meta-line">Authors: Nico Baumgart, Markus Lange-Hegermann, Mike Mücke</div>
<div class="meta-line">First: 2024-03-06T18:33:27+00:00 · Latest: 2025-12-29T18:31:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.04809v2">Abs</a> · <a href="https://arxiv.org/pdf/2403.04809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In industrial manufacturing, deploying deep learning models for visual inspection is mostly hindered by the high and often intractable cost of collecting and annotating large-scale training datasets. While image synthesis from 3D CAD models is a common solution, the individual techniques of domain and rendering randomization to create rich synthetic training datasets have been well studied mainly in simple domains. Hence, their effectiveness on complex industrial tasks with densely arranged and similar objects remains unclear. In this paper, we investigate the sim-to-real generalization performance of standard object detectors on the complex industrial application of terminal strip object detection, carefully combining randomization and domain knowledge. We describe step-by-step the creation of our image synthesis pipeline that achieves high realism with minimal implementation effort and explain how this approach could be transferred to other industrial settings. Moreover, we created a dataset comprising 30.000 synthetic images and 300 manually annotated real images of terminal strips, which is publicly available for reference and future research. To provide a baseline as a lower bound of the expectable performance in these challenging industrial parts detection tasks, we show the sim-to-real generalization performance of standard object detectors on our dataset based on a fully synthetic training. While all considered models behave similarly, the transformer-based DINO model achieves the best score with 98.40 % mean average precision on the real test set, demonstrating that our pipeline enables high quality detections in complex industrial environments from existing CAD data and with a manageable image synthesis effort.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>工业终端条目检测中合成训练数据影响的研究</div>
<div class="mono" style="margin-top:8px">在工业制造中，部署用于视觉检测的深度学习模型主要受到大规模训练数据集收集和标注的高昂且难以解决的成本阻碍。虽然从3D CAD模型生成图像是一种常见解决方案，但用于创建丰富合成训练数据集的领域和渲染随机化技术主要在简单领域中得到了充分研究。因此，它们在具有密集排列和相似对象的复杂工业任务中的有效性尚不清楚。在本文中，我们研究了标准对象检测器在复杂工业应用中的从仿真到现实的泛化性能，结合了随机化和领域知识。我们详细描述了实现高度逼真图像合成管道的方法，并解释了该方法如何应用于其他工业环境。此外，我们创建了一个包含30,000张合成图像和300张手动标注的真实图像的数据集，该数据集已公开供参考和未来研究使用。为了提供一个基准作为这些具有挑战性的工业部件检测任务中预期性能的下限，我们展示了在我们的数据集上基于完全合成训练的标准对象检测器的从仿真到现实的泛化性能。虽然所有考虑的模型表现相似，但基于DINO模型的变压器模型在真实测试集上的平均精度最高，达到98.40%，证明了我们的管道能够从现有CAD数据中在复杂工业环境中实现高质量的检测，并且具有可管理的图像合成努力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the effectiveness of synthetic training data in industrial terminal strip object detection. The authors developed a high-realism image synthesis pipeline using 3D CAD models and domain knowledge, creating a dataset of 30,000 synthetic images and 300 real images. They found that transformer-based DINO achieved the highest mean average precision of 98.40% on the real test set, indicating that their approach can enable high-quality object detection in complex industrial environments with minimal image synthesis effort.</div>
<div class="mono" style="margin-top:8px">本文研究了合成训练数据在工业终端条形码物体检测中的有效性，解决了实际数据收集成本高的问题。通过结合领域知识和随机化技术，作者创建了一个现实的图像合成管道，并在包含30,000张合成和300张真实图像的数据集上评估了标准物体检测器的性能。基于完全合成训练的数据，基于DINO模型在真实测试集上的平均精度最高，达到了98.40%，表明合成数据可以在复杂工业环境中支持高质量的物体检测，且所需合成图像的努力较小。</div>
</details>
</div>
<div class="card">
<div class="title">Random Controlled Differential Equations</div>
<div class="meta-line">Authors: Francesco Piatti, Thomas Cass, William F. Turner</div>
<div class="meta-line">First: 2025-12-29T18:25:10+00:00 · Latest: 2025-12-29T18:25:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23670v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机控制微分方程</div>
<div class="mono" style="margin-top:8px">我们提出了一种结合随机特征与控制微分方程（CDEs）的时间序列学习高效框架。在此方法中，大型随机参数化CDEs作为连续时间蓄水池，将输入路径映射到丰富的表示。仅训练一层线性读出层，从而得到快速、可扩展且具有强归纳偏置的模型。在此基础上，我们提出了两种变体：(i) 随机傅里叶CDEs（RF-CDEs）：这些方法在动态学之前使用随机傅里叶特征提升输入信号，提供了一种无核近似RBF增强序列模型的方法；(ii) 随机粗糙DEs（R-RDEs）：这些方法直接在粗糙路径输入上通过对数ODE离散化操作，使用对数符号捕捉高阶时间交互，同时保持稳定性和效率。我们证明，在无限宽度极限下，这些模型分别诱导RBF提升符号核和粗糙符号核，提供了一种随机特征蓄水池、连续时间深度架构和路径符号理论的统一视角。我们在这两种模型在一系列时间序列基准上进行了评估，展示了具有竞争力或最先进的性能。这些方法提供了一种实用的替代显式符号计算的方法，同时保留其归纳偏置并受益于随机特征的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a training-efficient framework for time-series learning using random features and controlled differential equations (CDEs). The framework uses large randomly parameterized CDEs to generate rich representations, with only a linear readout layer trained, leading to fast and scalable models. Two variants are proposed: Random Fourier CDEs (RF-CDEs) and Random Rough DEs (R-RDEs), which provide kernel-free approximations and capture higher-order temporal interactions, respectively. Experimental results show competitive or state-of-the-art performance across various time-series benchmarks, offering a practical alternative to explicit signature computations while maintaining efficiency and inductive bias.</div>
<div class="mono" style="margin-top:8px">研究引入了一种结合随机特征和控制微分方程（CDEs）的时间序列学习框架。提出了两种变体：随机傅里叶CDEs（RF-CDEs）和随机粗糙DEs（R-RDEs）。RF-CDEs使用随机傅里叶特征来近似RBF增强序列模型，而R-RDEs通过对数微分方程离散化直接操作粗糙路径输入，以捕捉更高阶的时间交互。实验表明，在各种时间序列基准测试中表现出竞争性或最先进的性能，提供了一种实用的替代显式签名计算的方法，同时保持了效率和归纳偏置。</div>
</details>
</div>
<div class="card">
<div class="title">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</div>
<div class="meta-line">Authors: Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</div>
<div class="meta-line">First: 2025-12-29T17:59:19+00:00 · Latest: 2025-12-29T17:59:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23649v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying &quot;understand before you imitate&quot;. Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboMirror: 在模仿之前理解——从视频到类人行走的框架</div>
<div class="mono" style="margin-top:8px">人类通过视觉观察学习行走，先理解视觉内容再模仿动作。然而，最先进的类人行走系统依赖于精心策划的运动捕捉轨迹或稀疏的文本命令，导致视觉理解和控制之间存在关键差距。基于文本的运动方法受到语义稀疏性和阶段化流水线错误的影响，而基于视频的方法仅进行机械姿态模仿，缺乏真正的视觉理解。我们提出了RoboMirror，这是第一个无需重新目标化的从视频到行走的框架，它体现了“理解后再模仿”的理念。利用VLMs，它将第一人称/第三人称视频提炼为视觉运动意图，直接条件化扩散机制策略生成物理上合理且语义上对齐的行走，无需显式的姿态重建或重新目标化。广泛的实验验证了RoboMirror的有效性，它通过第一人称视频实现远程存在，将第三人称控制延迟降低了80%，并且在任务成功率上比基线高出3.7%。通过将类人控制重新构架为视频理解，我们弥合了视觉理解和动作之间的差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboMirror is a video-to-locomotion framework that interprets visual content before imitating actions, addressing the gap between visual understanding and control in humanoid systems. It uses VLMs to distill raw videos into visual motion intents, directly conditioning a diffusion-based policy to generate physically plausible and semantically aligned locomotion. Experiments show that RoboMirror reduces third-person control latency by 80% and achieves a 3.7% higher task success rate compared to baselines, enabling telepresence via egocentric videos.</div>
<div class="mono" style="margin-top:8px">RoboMirror 是一种利用视觉语言模型解析原始第一人称和第三人称视频，并直接条件化扩散模型生成物理上合理且语义对齐的机器人行走动作的框架。这种方法弥合了视觉理解和动作之间的差距，将第三人称控制延迟减少了 80%，并且在任务成功率上比基线方法高出 3.7%。通过关注视频理解，RoboMirror 实现了远程存在感并使机器人的行走更加自然。</div>
</details>
</div>
<div class="card">
<div class="title">Nested Browser-Use Learning for Agentic Information Seeking</div>
<div class="meta-line">Authors: Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang</div>
<div class="meta-line">First: 2025-12-29T17:59:14+00:00 · Latest: 2025-12-29T17:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23647v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23647v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理信息检索中的嵌套浏览器使用学习</div>
<div class="mono" style="margin-top:8px">信息检索(IS)代理在广泛而深入的搜索任务中取得了强大的性能，但它们的工具使用主要局限于API级别的片段检索和基于URL的页面获取，限制了对通过实际浏览可获得的更丰富信息的访问。虽然全面的浏览器交互可以解锁更深层次的能力，但其精细的控制和冗长的页面内容返回引入了显著的复杂性，对于ReAct风格的功能调用代理来说尤其如此。为了弥合这一差距，我们提出了嵌套浏览器使用学习(NestBrowse)，它引入了一个最小且完整的浏览器操作框架，通过嵌套结构将交互控制与页面探索脱钩。这种设计简化了代理推理，同时使有效的深网信息获取成为可能。在具有挑战性的深网IS基准上的实证结果表明，NestBrowse在实践中提供了明显的益处。进一步的深入分析强调了其效率和灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance information-seeking agents by allowing them to use browsers more effectively, which can access richer information than API-level snippet retrieval. The method involves Nested Browser-Use Learning (NestBrowse), which simplifies interaction control and page exploration through a nested structure, making it easier for agents to perform complex tasks. Key findings show that NestBrowse improves performance on deep information-seeking benchmarks, demonstrating its practical benefits and flexibility.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使信息寻求代理能够更有效地使用浏览器来增强其能力，浏览器可以提供比简单API调用更丰富的信息。方法是采用Nested Browser-Use Learning（NestBrowse），通过使用嵌套结构将控制与探索分离来简化浏览器交互。关键发现表明，NestBrowse在深网信息获取任务上比现有方法表现出色，突显了其效率和灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</div>
<div class="meta-line">Authors: Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</div>
<div class="meta-line">First: 2025-12-29T17:59:05+00:00 · Latest: 2025-12-29T17:59:05+00:00</div>
<div class="meta-line">Comments: Website:https://kd-tao.github.io/OmniAgent/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23646v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23646v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kd-tao.github.io/OmniAgent/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniAgent：基于音频引导的多模态音频视频理解主动感知代理</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型在统一音频和视觉模态方面取得了显著进展；然而，它们往往缺乏细粒度的跨模态理解，并且难以实现多模态对齐。为了解决这些限制，我们引入了OmniAgent，这是一种完全基于音频的主动感知代理，能够动态协调专门的工具以实现更细粒度的视听推理。与依赖于僵硬的静态工作流和密集的帧字幕的先前工作不同，本文展示了从被动响应生成到主动多模态查询的范式转变。OmniAgent采用动态规划，自主地在需要时调用工具，战略性地将感知注意力集中在与任务相关的线索上。我们方法的核心是一种新颖的从粗到细的基于音频的感知范式，利用音频线索定位时间事件并指导后续推理。在三个音频视频理解基准上的广泛实证评估表明，OmniAgent达到了最先进的性能，比领先的开源和专有模型在准确率上高出10%-20%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">OmniAgent is an audio-guided active perception agent that dynamically orchestrates specialized tools for fine-grained audio-visual reasoning, surpassing existing models by 10% to 20% in accuracy across three benchmarks. It employs a coarse-to-fine audio-guided perception paradigm to localize temporal events and guide reasoning, shifting from passive response generation to active multimodal inquiry.</div>
<div class="mono" style="margin-top:8px">由于大规模语言模型在跨模态理解和多模态对齐方面存在局限性，本文提出了OmniAgent，这是一种基于音频的主动感知代理，能够动态使用专门工具进行细粒度的视听推理。它采用从粗到细的音频引导感知范式来定位时间事件并指导推理。实证评估表明，OmniAgent在三个基准测试中的准确率比现有模型高出10%到20%。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</div>
<div class="meta-line">Authors: Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-29T17:48:56+00:00 · Latest: 2025-12-29T17:48:56+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23635v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23635v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考端到端3D感知的空间-时间对齐</div>
<div class="mono" style="margin-top:8px">空间-时间对齐对于自主驾驶（AD）中端到端（E2E）感知的时序建模至关重要，提供了有价值的结构和纹理先验信息。现有方法通常依赖注意力机制跨帧对齐物体，简化了统一的显式物理模型（恒定速度等）。这些方法倾向于使用语义特征进行隐式对齐，挑战了传统感知范式中显式运动建模的重要性。然而，不同类别和帧之间运动状态和物体特征的变化使得这种对齐效果不佳。为了解决这一问题，我们提出了HAT，这是一种空间-时间对齐模块，允许每个物体自适应地从多个假设中解码最佳对齐提案，无需直接监督。具体而言，HAT 首先利用多个显式运动模型生成历史实例的空间锚点和运动感知特征提案，然后通过嵌入在缓存对象查询中的语义和运动线索进行多假设解码，最终为目标帧提供最佳对齐提案。在nuScenes上，HAT 在各种基线中一致地提高了3D时序检测器和跟踪器的性能。当与DETR3D检测器配对时，它在测试集上实现了46.0%的AMOTA最佳跟踪结果。在以对象为中心的E2E AD方法中，HAT 提高了感知准确性（+1.3% mAP，+3.1% AMOTA）并降低了碰撞率（32%）。当语义被破坏（nuScenes-C）时，HAT 对运动建模的增强使E2E AD中的感知和规划更加稳健。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of spatio-temporal alignment in end-to-end 3D perception for autonomous driving, where existing methods rely on simplified motion models and semantic features. To improve this, the authors propose HAT, a module that allows each object to adaptively decode optimal alignment proposals from multiple hypotheses. HAT uses explicit motion models to generate spatial anchors and motion-aware feature proposals, and then performs multi-hypothesis decoding to provide the best alignment for the target frame. Experiments on nuScenes show that HAT enhances 3D temporal detectors and trackers, achieving state-of-the-art tracking results and improving perception accuracy and collision rate reduction in an object-centric E2E AD method.</div>
<div class="mono" style="margin-top:8px">论文针对现有端到端感知方法在自动驾驶中时空对齐的局限性，这些方法依赖于简化的运动模型和语义特征。它提出了HAT模块，允许对象从多个假设中自适应地解码最优对齐提案。HAT使用显式的运动模型生成空间锚点和运动感知特征提案，然后通过嵌入在缓存对象查询中的语义和运动线索进行多假设解码。在nuScenes上，HAT提高了3D时空检测器和跟踪器的性能，实现了最先进的跟踪结果，并在基于对象的端到端自动驾驶方法中提高了感知准确性和降低了碰撞率。</div>
</details>
</div>
<div class="card">
<div class="title">BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization</div>
<div class="meta-line">Authors: Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong</div>
<div class="meta-line">First: 2025-12-29T17:41:11+00:00 · Latest: 2025-12-29T17:41:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23631v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23631v1">PDF</a> · <a href="https://github.com/iamxjy/BOAD-SWE-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BOAD：通过多臂老虎机优化发现层次化的软件工程代理</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）展示了强大的推理和编程能力，但在处理长周期和分布外的实际软件工程（SWE）问题时却表现不佳。现有系统通常依赖单一代理来处理整个工作流程，包括解释问题、导航大型代码库和实施修复，这在单一推理链中进行。这种单一设计迫使模型保留无关上下文，导致虚假相关性和较差的泛化能力。受人类工程师如何分解复杂问题的启发，我们建议将SWE代理结构化为协调专门化子代理的协调者，这些子代理负责子任务，如定位、编辑和验证。挑战在于自动发现有效的层次结构：随着子代理数量的增加，搜索空间变得组合化，难以在团队内部为个别子代理分配信用。我们通过将层次结构发现形式化为多臂老虎机（MAB）问题来应对这些挑战，其中每个臂代表一个候选子代理，奖励衡量其与其他代理合作时的有用性。该框架称为代理设计的多臂老虎机优化（BOAD），在有限的评估预算下能够高效探索子代理设计。在SWE-bench-Verified上，BOAD优于单一代理和手动设计的多代理系统。在SWE-bench-Live上，包含更多近期和分布外的问题，我们的36B系统在评估时排名第二，超过了如GPT-4和Claude等更大模型。这些结果表明，自动发现的层次化多代理系统在处理具有挑战性的长周期SWE任务时显著提高了泛化能力。代码可在https://github.com/iamxjy/BOAD-SWE-Agent/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of using large language models for real-world software engineering tasks by proposing a hierarchical agent design approach called BOAD. This method uses bandit optimization to automatically discover effective hierarchies of specialized sub-agents, which outperform single-agent and manually designed multi-agent systems on both SWE-bench-Verified and SWE-bench-Live. Specifically, on SWE-bench-Live, a 36B model using BOAD ranks second, surpassing larger models like GPT-4 and Claude.</div>
<div class="mono" style="margin-top:8px">论文提出了使用多臂老虎机方法进行自动层级软件工程代理结构发现的BOAD方法。它通过将任务分解为专门化的子代理来解决单体代理的局限性，并由一个总体代理进行协调。BOAD将层级结构的发现形式化为一个多臂老虎机问题，从而在有限的评估预算下高效探索子代理的设计。实验结果表明，BOAD在SWE-bench-Verified和SWE-bench-Live上优于单体代理和手动设计的多代理系统，特别是在更近期和分布外的问题上表现出更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Memorization in 3D Shape Generation: An Empirical Study</div>
<div class="meta-line">Authors: Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu</div>
<div class="meta-line">First: 2025-12-29T17:39:21+00:00 · Latest: 2025-12-29T17:39:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23628v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23628v1">PDF</a> · <a href="https://github.com/zlab-princeton/3d_mem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3D形状生成中的记忆化：一项实证研究</div>
<div class="mono" style="margin-top:8px">生成模型在3D视觉中越来越多地用于合成新的形状，但尚不清楚它们的生成是否依赖于记忆训练形状。理解其记忆化有助于防止训练数据泄露并提高生成结果的多样性。在本文中，我们设计了一种评估框架来量化3D生成模型的记忆化，并研究了不同数据和建模设计对记忆化的影响。我们首先将该框架应用于量化现有方法的记忆化。接下来，通过使用一个潜在向量集（Vecset）扩散模型的受控实验，我们发现，在数据方面，记忆化取决于数据模态，并随着数据多样性和更精细的条件而增加；在建模方面，它在适度的指导规模下达到峰值，并可以通过更长的Vecset和简单的旋转增强来减轻。总体而言，我们的框架和分析为3D生成模型中的记忆化提供了实证理解，并建议了一些简单而有效的策略来减少它而不降低生成质量。我们的代码可在https://github.com/zlab-princeton/3d_mem获取</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether 3D generative models memorize training shapes during synthesis, which is crucial for preventing data leakage and enhancing generation diversity. The authors develop an evaluation framework to measure memorization and conduct experiments on existing methods and a latent vector-set (Vecset) diffusion model. They find that memorization is influenced by data modality and diversity, and can be reduced by using longer Vecsets and simple rotation augmentation. This framework and analysis offer practical insights into reducing memorization in 3D generative models without compromising generation quality.</div>
<div class="mono" style="margin-top:8px">本文研究了3D生成模型在合成过程中是否依赖于记忆训练样本。提出了一种评估框架来量化记忆，并发现记忆依赖于数据模态和多样性，且在适度的指导尺度上达到峰值。研究还表明，较长的Vecset和简单的旋转增强可以减轻记忆。这些发现有助于防止数据泄露并提高生成3D形状的多样性。</div>
</details>
</div>
<div class="card">
<div class="title">Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning</div>
<div class="meta-line">Authors: Deniz Akdemir</div>
<div class="meta-line">First: 2025-12-29T17:21:44+00:00 · Latest: 2025-12-29T17:21:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23617v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23617v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing &quot;negative transfer&quot; that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam&#x27;s theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Le Cam失真：一种稳健的迁移学习决策理论框架</div>
<div class="mono" style="margin-top:8px">分布偏移是现实世界机器学习的核心挑战。主导的范式——无监督领域适应（UDA）——通过最小化对称偏差来使源和目标表示一致，从而确保特征不变性 [Ganin et al., 2016]。我们证明了这种方法从根本上是错误的：当领域信息量不均衡（例如高质量传感器与降级传感器）时，严格的不变性会导致信息破坏，从而在关键安全应用中产生“负迁移”，这可能是灾难性的 [Wang et al., 2019]。
我们提出了一种基于Le Cam统计试验理论 [Le Cam, 1986] 的决策理论框架，使用构造性近似来替代对称不变性，采用方向模拟。我们引入了Le Cam失真，通过缺陷距离 $δ(E_1, E_2)$ 定量，作为在模拟条件下转移风险的严格上界。我们的框架通过学习一个内核，该内核能够从源模拟目标，从而在五个实验（基因组学、视觉、强化学习）中实现：(1) HLA基因组学中近乎完美的频率估计（相关系数 $r=0.999$，与经典方法匹配），(2) CIFAR-10图像分类中零源效用损失（准确率保持在81.2%，而CycleGAN的准确率下降了34.7%），以及(3) 强化学习控制中基于不变性的方法遭受灾难性崩溃的安全策略转移。Le Cam失真提供了第一个在负迁移不可接受的领域中进行风险控制的转移学习原理框架：医学成像、自主系统和精准医疗。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of distribution shift in machine learning by proposing a decision-theoretic framework called Le Cam Distortion, which uses Le Cam&#x27;s theory of statistical experiments to replace feature invariance with directional simulability. The framework achieves near-perfect frequency estimation in genomics, preserves source utility in image classification, and enables safe policy transfer in reinforcement learning, outperforming invariance-based methods in terms of transfer risk control.</div>
<div class="mono" style="margin-top:8px">该论文通过提出基于Le Cam统计实验理论的Le Cam Distortion框架，解决了机器学习中的分布偏移问题，该框架用方向可模拟性替代了特征不变性。该方法使用缺陷距离量化转移风险，并能够在不降低源数据效用的情况下实现迁移学习。实验结果表明，Le Cam Distortion在基因组学、视觉和强化学习领域实现了近乎完美的频率估计，保持了图像分类中的源数据效用，并在强化学习控制中确保了安全的策略转移，优于基于不变性的方法在转移风险控制方面的表现。</div>
</details>
</div>
<div class="card">
<div class="title">ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery</div>
<div class="meta-line">Authors: Qinfeng Zhu, Yunxi Jiang, Lei Fan</div>
<div class="meta-line">First: 2025-04-30T10:19:21+00:00 · Latest: 2025-12-29T17:14:48+00:00</div>
<div class="meta-line">Comments: Accpted by Neural Networks</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.21491v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.21491v2">PDF</a> · <a href="https://github.com/zhuqinfeng1999/ClassWise-CRF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a result-level category-specific fusion architecture called ClassWise-CRF. This architecture employs a two-stage process: first, it selects expert networks that perform well in specific categories from a pool of candidate networks using a greedy algorithm; second, it integrates the segmentation predictions of these selected networks by adaptively weighting their contributions based on their segmentation performance in each category. Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture treats the segmentation predictions from multiple networks as confidence vector fields. It leverages segmentation metrics (such as Intersection over Union) from the validation set as priors and employs an exponential weighting strategy to fuse the category-specific confidence scores predicted by each network. This fusion method dynamically adjusts the weights of each network for different categories, achieving category-specific optimization. Building on this, the architecture further optimizes the fused results using unary and pairwise potentials in CRF to ensure spatial consistency and boundary accuracy. To validate the effectiveness of ClassWise-CRF, we conducted experiments on two remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced semantic segmentation networks. The results show that the ClassWise-CRF architecture significantly improves segmentation performance: on the LoveDA dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on the validation set and by 0.68% on the test set; on the Vaihingen dataset, the mIoU improved by 0.87% on the validation set and by 0.91% on the test set. These results fully demonstrate the effectiveness and generality of the ClassWise-CRF architecture in semantic segmentation of remote sensing images. The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ClassWise-CRF：针对特定类别的融合架构以增强遥感影像语义分割</div>
<div class="mono" style="margin-top:8px">我们提出了一种结果级别的针对特定类别的融合架构，称为ClassWise-CRF。该架构采用两阶段过程：首先，使用贪婪算法从候选网络池中选择在特定类别上表现良好的专家网络；其次，通过根据每个类别中分割性能适配加权这些网络的分割预测来整合这些选择的网络的分割预测。受到条件随机场(CRF)的启发，ClassWise-CRF架构将多个网络的分割预测视为置信向量场。它利用验证集上的分割指标（如交并比）作为先验，并采用指数加权策略融合每个网络预测的类别特定置信分数。这种融合方法动态调整每个网络在不同类别中的权重，实现类别特定优化。在此基础上，架构进一步使用CRF中的单元势和对势优化融合结果，以确保空间一致性与边界准确性。为了验证ClassWise-CRF的有效性，我们在两个遥感数据集LoveDA和Vaihingen上使用八种经典和先进的语义分割网络进行了实验。结果显示，ClassWise-CRF架构显著提高了分割性能：在LoveDA数据集上，验证集的平均交并比(mIoU)提高了1.00%，测试集提高了0.68%；在Vaihingen数据集上，验证集的mIoU提高了0.87%，测试集提高了0.91%。这些结果充分证明了ClassWise-CRF架构在遥感影像语义分割中的有效性和普适性。完整的代码可在https://github.com/zhuqinfeng1999/ClassWise-CRF获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance semantic segmentation of remote sensing imagery by proposing a category-specific fusion architecture called ClassWise-CRF. It uses a two-stage process: first, selecting expert networks for specific categories and second, integrating their predictions through adaptive weighting based on segmentation performance. Experiments on LoveDA and Vaihingen datasets showed that ClassWise-CRF improved mean Intersection over Union (mIoU) by 1.00% and 0.68% on the validation and test sets of LoveDA, and by 0.87% and 0.91% on the validation and test sets of Vaihingen, respectively. This demonstrates the effectiveness and generality of ClassWise-CRF in remote sensing image segmentation.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出一种类别特定融合架构ClassWise-CRF来提升遥感图像的语义分割。该架构采用两阶段过程：首先选择在特定类别表现优秀的网络，然后通过基于分割性能的自适应加权整合这些网络的预测。在LoveDA和Vaihingen数据集上的实验表明，ClassWise-CRF在LoveDA验证集和测试集上的平均交并比（mIoU）分别提高了1.00%和0.68%，在Vaihingen验证集和测试集上分别提高了0.87%和0.91%。这表明ClassWise-CRF在遥感图像语义分割中的有效性和普适性。</div>
</details>
</div>
<div class="card">
<div class="title">OM4OV: Leveraging Ontology Matching for Ontology Versioning</div>
<div class="meta-line">Authors: Zhangcheng Qiang, Kerry Taylor, Weiqing Wang</div>
<div class="meta-line">First: 2024-09-30T14:00:04+00:00 · Latest: 2025-12-29T17:05:52+00:00</div>
<div class="meta-line">Comments: 16 pages, 8 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.20302v7">Abs</a> · <a href="https://arxiv.org/pdf/2409.20302v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to the dynamic nature of the Semantic Web, version control is necessary to manage changes in widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many approaches treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse similarities and differences between OM and OV and formalise the OM4OV pipeline to offer more advanced OV support. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary extensions, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and limited explainability of false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, which builds on existing OM alignments to reduce the number of matching candidates and to improve overall OV performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OM4OV：利用本体匹配进行本体版本管理</div>
<div class="mono" style="margin-top:8px">由于语义网的动态性质，版本控制对于管理广泛使用的本体中的更改是必要的。尽管本体版本管理（OV）作为有效本体管理的关键组成部分已有长期的认识，但许多方法仍将OV视为类似于本体匹配（OM），并直接重用OM系统来执行OV任务。在本研究中，我们系统地分析了OM和OV之间的相似性和差异，并形式化了OM4OV管道，以提供更高级的OV支持。该管道在最先进的OM系统Agent-OM中实现和评估。实验结果表明，OM系统可以重用于OV任务，但如果没有必要的扩展，当前的OM4OV管道会产生失真的测量结果，检测更新实体的性能较差，并且对错误映射的解释性有限。为了解决这些问题，我们提出了一种优化方法，称为交叉引用（CR）机制，该机制基于现有的OM对齐来减少匹配候选的数量，并提高整体的OV性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need for efficient ontology versioning (OV) in the Semantic Web by analyzing the differences and similarities between ontology matching (OM) and OV. The researchers developed an OM4OV pipeline and evaluated it using the Agent-OM system. The results showed that while OM systems can be reused for OV tasks, they require extensions to avoid skewed measurements, poor performance in detecting update entities, and limited explainability of false mappings. To improve performance, the study proposes a cross-reference (CR) mechanism that reduces matching candidates and enhances overall OV performance.</div>
<div class="mono" style="margin-top:8px">该研究针对语义网中高效的本体版本控制（OV）需求，分析了本体匹配（OM）与OV之间的差异和相似之处。研究人员开发了OM4OV管道并在Agent-OM系统中进行了评估。结果显示，虽然OM系统可以用于OV任务，但需要扩展以避免测量偏差、检测更新实体的性能不佳以及解释错误映射的局限性。为了提高性能，研究提出了一种交叉引用（CR）机制，该机制减少了匹配候选者并提高了整体OV性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Refocus with Video Diffusion Models</div>
<div class="meta-line">Authors: SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, Shumian Xin</div>
<div class="meta-line">Venue: SIGGRAPH Asia 2025</div>
<div class="meta-line">First: 2025-12-22T19:29:57+00:00 · Latest: 2025-12-29T17:04:36+00:00</div>
<div class="meta-line">Comments: Code and data are available at https://learn2refocus.github.io . SIGGRAPH Asia 2025, Dec. 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19823v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.19823v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://learn2refocus.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习使用视频扩散模型重新聚焦</div>
<div class="mono" style="margin-top:8px">对焦是摄影的基础，但自动对焦系统往往无法捕捉到预期的主体，用户经常希望在拍摄后调整对焦。我们提出了一种使用视频扩散模型进行现实后对焦的新方法。从单张失焦图像出发，我们的方法生成了一组感知上准确的焦距堆栈，表示为视频序列，支持交互式重新对焦并解锁一系列下游应用。我们发布了一个大规模的焦距堆栈数据集，以支持这项工作和未来的研究。我们的方法在感知质量和在具有挑战性的场景中的鲁棒性方面均优于现有方法，为日常摄影中的更高级对焦编辑能力铺平了道路。代码和数据可在www.learn2refocus.github.io获取</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces a novel method for post-capture refocusing using video diffusion models. Given a defocused image, the approach generates a perceptually accurate focal stack, allowing for interactive refocusing. The method outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, and a large-scale dataset is provided to support this work and future research.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用视频扩散模型开发一种后捕获对焦方法，解决摄影中自动对焦系统的局限性。该方法可以从单张失焦图像生成感知上准确的焦距堆栈，实现交互式对焦并支持多种下游应用。该方法在感知质量和鲁棒性方面均优于现有方法，并提供了一个大规模的焦距堆栈数据集以支持这项工作和未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">The Nonstationarity-Complexity Tradeoff in Return Prediction</div>
<div class="meta-line">Authors: Agostino Capponi, Chengpiao Huang, J. Antonio Sidaoui, Kaizheng Wang, Jiacheng Zou</div>
<div class="meta-line">First: 2025-12-29T16:49:19+00:00 · Latest: 2025-12-29T16:49:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23596v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23596v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>收益预测中的非平稳性-复杂性权衡</div>
<div class="mono" style="margin-top:8px">我们研究了在非平稳环境下股票收益预测的机器学习模型，揭示了一种基本的非平稳性-复杂性权衡：复杂的模型减少了模型设定误差，但需要更长的训练窗口，从而引入更强的非平稳性。我们通过一种新颖的模型选择方法解决了这一矛盾，该方法使用锦标赛程序联合优化模型类别和训练窗口大小，并在非平稳验证数据上适应性地评估候选模型。我们的理论分析表明，这种方法平衡了模型设定误差、估计方差和非平稳性，接近于事后最佳模型的表现。将我们的方法应用于17个行业投资组合收益，我们一致优于标准滚动窗口基准，平均提高离样本$R^2$ 14-23%。在NBER定义的经济衰退期间，改进尤为显著：在海湾战争衰退期间，我们的方法在基准为负的情况下实现了正的$R^2$，而在2001年衰退期间，$R^2$绝对值至少提高了80bps，且在2008年金融危机期间表现更优。经济上，基于我们选择的模型的交易策略在各行业中平均累计回报率提高了31%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores machine learning models for stock return prediction in non-stationary environments, identifying a tradeoff between model complexity and non-stationarity. The authors introduce a novel model selection method that optimizes both model class and training window size, balancing misspecification error, estimation variance, and non-stationarity. Experiments on 17 industry portfolios show that their method outperforms standard benchmarks, improving out-of-sample $R^2$ by 14-23% and achieving positive $R^2$ during recessions where benchmarks fail.</div>
<div class="mono" style="margin-top:8px">研究探讨了在非平稳环境下使用机器学习模型进行股票回报预测的问题，发现模型复杂性和非平稳性之间存在权衡。作者提出了一种新的模型选择方法，同时优化模型类别和训练窗口大小，通过在非平稳验证数据上进行tournament程序评估候选模型。该方法平衡了模型偏差、估计方差和非平稳性，相比标准滚动窗口基准提高了14-23%的$R^2$。在经济衰退期间，该方法显示出显著的改进，例如在海湾战争衰退期间实现了正的$R^2$，而在2001年和2008年危机期间则显著提高了$R^2$。基于选定模型的交易策略平均在各个行业中产生了31%更高的累计回报。</div>
</details>
</div>
<div class="card">
<div class="title">How Safe Are AI-Generated Patches? A Large-scale Study on Security Risks in LLM and Agentic Automated Program Repair on SWE-bench</div>
<div class="meta-line">Authors: Amirali Sajadi, Kostadin Damevski, Preetha Chatterjee</div>
<div class="meta-line">First: 2025-06-30T21:10:19+00:00 · Latest: 2025-12-29T16:44:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02976v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.02976v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) and their agentic frameworks are increasingly adopted to perform development tasks such as automated program repair (APR). While prior work has identified security risks in LLM-generated code, most have focused on synthetic, simplified, or isolated tasks that lack the complexity of real-world program repair. In this study, we present the first large-scale security analysis of LLM-generated patches using 20,000+ GitHub issues. We evaluate patches proposed by developers, a standalone LLM (Llama 3.3 Instruct-70B), and three top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb). Finally, we analyze a wide range of code, issue, and project-level factors to understand the conditions under which generating insecure patches is more likely. Our findings reveal that Llama introduces many new vulnerabilities, exhibiting unique patterns not found in developers&#x27; code. Agentic workflows also generate a number of vulnerabilities, particularly when given more autonomy. We find that vulnerabilities in LLM-generated patches are associated with distinctive code characteristics and are commonly observed in issues missing specific types of information. These results suggest that contextual factors play a critical role in the security of the generated patches and point toward the need for proactive risk assessment methods that account for both issue and code-level information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI生成补丁的安全性如何？基于SWE-bench的大规模研究</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）及其自主框架越来越多地被用于执行开发任务，如自动程序修复（APR）。尽管先前的工作已经识别出LLM生成代码中的安全风险，但大多数研究都集中在合成、简化或孤立的任务上，缺乏真实世界程序修复的复杂性。在本研究中，我们首次使用20,000多个GitHub问题对LLM生成的补丁进行大规模安全分析。我们评估了开发人员、独立LLM（Llama 3.3 Instruct-70B）和三个表现最佳的自主框架（OpenHands、AutoCodeRover、HoneyComb）提出的补丁。最后，我们分析了代码、问题和项目层面的各种因素，以了解生成不安全补丁的条件。我们的研究发现，Llama引入了许多新的漏洞，表现出不同于开发人员代码的独特模式。自主工作流程也会生成一些漏洞，尤其是在给予更多自主权时。我们发现，LLM生成补丁中的漏洞与特定的代码特征相关，并且通常出现在缺少特定类型信息的问题中。这些结果表明，上下文因素在生成补丁的安全性中起着关键作用，并指出了需要考虑问题和代码层面信息的主动风险评估方法的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the security risks in AI-generated patches using large language models (LLMs) and agentic frameworks on 20,000+ GitHub issues. It evaluates patches from developers, a standalone LLM, and three agentic frameworks, revealing that LLMs introduce many new vulnerabilities not found in human-developed code, and agentic workflows generate more vulnerabilities when given more autonomy. The research highlights the importance of contextual factors in the security of generated patches and suggests the need for proactive risk assessment methods.</div>
<div class="mono" style="margin-top:8px">本研究通过分析20,000+ GitHub 问题，评估了使用大型语言模型（LLMs）和代理框架生成的补丁的安全风险。研究对比了开发者、一个独立的LLM以及三个顶级代理框架生成的补丁，发现LLMs在获得更多自主权时会引入许多新的漏洞。研究发现，LLM生成的补丁中的漏洞与特定的代码特征和缺失的问题信息有关，强调了需要考虑问题和代码层面信息的主动风险评估方法的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Same or Not? Enhancing Visual Perception in Vision-Language Models</div>
<div class="meta-line">Authors: Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari</div>
<div class="meta-line">First: 2025-12-29T16:43:47+00:00 · Latest: 2025-12-29T16:43:47+00:00</div>
<div class="meta-line">Comments: Project webpage: https://glab-caltech.github.io/twin/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23592v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23592v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://glab-caltech.github.io/twin/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition (&quot;Is it a cat or a dog?&quot;) over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同或不同？提升视觉语言模型的视觉感知能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在广泛的视觉理解方面表现出色，但仍然较为粗略，存在视觉偏见，并且忽略了一些细微的视觉细节。现有的训练语料库通过强调一般识别（“是猫还是狗？”）而非精细的感知，强化了这一局限性。为了解决这一问题，我们引入了一个新的训练语料库和任务，旨在增强VLMs的感知能力。TWIN是一个包含561,000个图像对查询的大规模数据集，要求模型判断两个视觉相似的图像是否描绘同一个物体，鼓励关注细微的视觉线索。该数据集涵盖了各种日常物体在不同上下文、视角和外观下的广泛范围。在TWIN上微调VLMs在精细识别方面取得了显著进步，即使在未见过的领域如艺术、动物、植物和地标上也是如此。为了量化这些进步，我们引入了FGVQA，这是一个包含12,000个查询的基准套件，重新利用了多个领域中的精细识别和检索数据集。虽然现有的VLMs在FGVQA上表现不佳，但在TWIN上微调后，性能提高了高达19.3%，而不会影响通用VQA基准的性能。最后，我们的TWIN数据集在对象注释方面具有可扩展性，我们的分析表明，规模是性能的关键。我们设想TWIN作为开源VLM训练语料库的即插即用补充，推动未来模型感知精度的提升。项目网页：https://glab-caltech.github.io/twin/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of vision-language models (VLMs) in fine-grained visual perception by introducing a new dataset called TWIN, which consists of 561,000 image-pair queries to enhance models&#x27; ability to recognize subtle visual details. Fine-tuning VLMs on TWIN improves their performance in fine-grained recognition tasks, even in unseen domains, with up to 19.3% improvement on a benchmark suite called FGVQA without affecting general VQA performance. The study also highlights the importance of scale in dataset size for better model performance.</div>
<div class="mono" style="margin-top:8px">研究旨在通过引入一个新的名为TWIN的数据集来提高视觉语言模型（VLMs）的细粒度视觉感知能力，该数据集包含561,000对图像查询。TWIN鼓励模型区分两幅相似图像是否描绘同一个物体，关注细微的视觉细节。在TWIN上微调VLMs可以显著提高细粒度识别能力，即使在未见过的领域也是如此。FGVQA基准测试表明，与现有VLMs相比，经过TWIN微调的模型在细粒度识别和检索上的表现最多可提高19.3%，同时不影响其一般VQA性能。研究强调了在数据集注释中规模的重要性，以提高VLMs的感知精度。</div>
</details>
</div>
<div class="card">
<div class="title">LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation</div>
<div class="meta-line">Authors: Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu</div>
<div class="meta-line">First: 2025-12-29T16:17:36+00:00 · Latest: 2025-12-29T16:17:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23576v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23576v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiveTalk: 通过改进的在线策略蒸馏实现实时多模态交互视频扩散</div>
<div class="mono" style="margin-top:8px">通过扩散模型实时生成视频对于构建通用多模态交互AI系统至关重要。然而，通过迭代过程中的双向注意力同时对所有视频帧进行去噪会阻碍实时交互。虽然现有的蒸馏方法可以使模型自回归并减少采样步骤以缓解这一问题，但它们主要关注文本到视频生成，使得人机交互显得不自然且效率较低。本文旨在针对基于多模态上下文（包括文本、图像和音频）的实时交互视频扩散，以弥合这一差距。鉴于观察到领先的在线策略蒸馏方法Self Forcing在多模态条件下遇到挑战（如视觉伪影、黑屏和质量下降），我们研究了一种改进的蒸馏方案，强调条件输入的质量以及在线策略优化的初始化和调度。在包括HDTF、AVSpeech和CelebV-HQ的多模态条件（音频、图像和文本）生成头像视频的基准测试中，我们的蒸馏模型在视觉质量上与全步骤双向基线相当或更大规模的模型相当，但推理成本和延迟降低了20倍。此外，我们将模型与音频语言模型和长视频推理技术Anchor-Heavy Identity Sinks结合，构建了LiveTalk，一个实时多模态交互头像系统。在我们策划的多轮交互基准测试中的系统级评估显示，LiveTalk在多轮视频连贯性和内容质量方面优于Sora2和Veo3等最先进的模型，同时将响应延迟从1到2分钟缩短到实时生成，从而实现无缝的人机多模态交互。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of real-time video generation via diffusion models for multimodal interactive AI systems. It introduces an improved on-policy distillation method to enhance the quality of condition inputs and optimize the initialization and schedule for better performance. The distilled model achieves visual quality comparable to full-step baselines with 20 times less inference cost and latency, and integrates with audio language models and long-form video inference techniques to create LiveTalk, a real-time multimodal interactive avatar system that outperforms state-of-the-art models in multi-turn video coherence and content quality.</div>
<div class="mono" style="margin-top:8px">本文旨在解决通过扩散模型进行实时视频生成以构建多模态交互AI系统的挑战。它引入了一种改进的在线策略蒸馏方法，以提高条件输入的质量并优化初始化和调度，从而获得更好的性能。蒸馏模型在视觉质量上与全步长基线相当，但只需20倍少的推理成本和延迟，并结合了音频语言模型和长视频推理技术，构建了实时多模态交互头像系统LiveTalk，该系统在多轮视频连贯性和内容质量上优于最先进的模型。</div>
</details>
</div>
<div class="card">
<div class="title">Predicting large scale cosmological structure evolution with generative adversarial network-based autoencoders</div>
<div class="meta-line">Authors: Marion Ullmo, Nabila Aghanim, Aurélien Decelle, Miguel Aragon-Calvo</div>
<div class="meta-line">First: 2024-03-04T16:17:43+00:00 · Latest: 2025-12-29T16:15:13+00:00</div>
<div class="meta-line">Comments: 13 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.02171v2">Abs</a> · <a href="https://arxiv.org/pdf/2403.02171v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predicting the nonlinear evolution of cosmic structure from initial conditions is typically approached using Lagrangian, particle-based methods. These techniques excel in terms of tracking individual trajectories, but they might not be suitable for applications where point-based information is unavailable or impractical. In this work, we explore an alternative, field-based approach using Eulerian inputs. Specifically, we developed an autoencoder architecture based on a generative adversarial network (GAN) and trained it to evolve density fields drawn from dark matter N-body simulations. We tested this method on both 2D and 3D data. We find that while predictions on 2D density maps perform well based on density alone, accurate 3D predictions require the inclusion of associated velocity fields. Our results demonstrate the potential of field-based representations to model cosmic structure evolution, offering a complementary path to Lagrangian methods in contexts where field-level data is more accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用生成对抗网络自编码器预测大规模宇宙结构演化</div>
<div class="mono" style="margin-top:8px">从初始条件预测非线性宇宙结构的演化通常使用拉格朗日、粒子基方法。这些技术在追踪单个轨迹方面表现出色，但在点基信息不可用或不切实际的应用中可能不太适用。在本工作中，我们探索了一种替代的场基方法，使用欧拉输入。具体而言，我们基于生成对抗网络（GAN）开发了一种自编码器架构，并训练其演化来自暗物质N体模拟的密度场。我们在2D和3D数据上测试了该方法。我们发现，仅基于密度的2D密度图预测表现良好，而准确的3D预测需要包含相关的速度场。我们的结果表明，场基表示法在建模宇宙结构演化方面具有潜力，为在场级数据更易获取的情况下提供了一种替代路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to predict the nonlinear evolution of cosmic structure using a field-based approach with a GAN-based autoencoder, as an alternative to Lagrangian particle-based methods. The method was tested on 2D and 3D data from dark matter N-body simulations, showing that while 2D predictions based solely on density are accurate, 3D predictions require the inclusion of velocity fields. The results highlight the potential of field-based representations for modeling cosmic structure evolution, providing a complementary method to Lagrangian approaches when field-level data is more accessible.</div>
<div class="mono" style="margin-top:8px">该研究旨在使用基于生成对抗网络（GAN）的自编码器，以场为基础的方法来预测宇宙结构的非线性演化，作为拉格朗日粒子方法的替代方案。该方法在来自暗物质N体模拟的2D和3D数据上进行了测试。结果表明，虽然基于密度的2D预测是准确的，但3D预测需要包含速度场才能准确。这表明场基础表示在某些情况下可以作为拉格朗日方法的有益补充。</div>
</details>
</div>
<div class="card">
<div class="title">ProGuard: Towards Proactive Multimodal Safeguard</div>
<div class="meta-line">Authors: Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao</div>
<div class="meta-line">First: 2025-12-29T16:13:23+00:00 · Latest: 2025-12-29T16:13:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23573v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23573v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProGuard: 向前瞻多模态保护迈进</div>
<div class="mono" style="margin-top:8px">生成模型的迅速发展导致了持续出现的多模态安全风险，暴露了现有防御方法的局限性。为应对这些挑战，我们提出了ProGuard，这是一种前瞻性的视觉-语言保护措施，能够在无需传统反应式方法所需的模型调整的情况下，识别和描述离分布（OOD）的安全风险。我们首先构建了一个包含87,000个样本的模态平衡数据集，每个样本都标注了二元安全标签和在多层次多模态安全分类学下的风险类别，有效缓解了模态偏差，确保了对文本、图像和图文输入的一致性审查。基于此数据集，我们通过强化学习（RL）训练我们的视觉-语言基础模型，以实现高效和简洁的推理。为了在受控环境中模拟前瞻性的安全场景，我们进一步引入了离分布安全类别推断任务，并通过基于同义词库的相似性奖励来增强RL目标，鼓励模型为未见过的不安全类别生成简洁描述。实验结果表明，ProGuard在二元安全分类上的性能与闭源大型模型相当，在不安全内容分类上显著优于现有开源保护模型。最值得注意的是，ProGuard提供了强大的前瞻保护能力，将离分布风险检测提高了52.6%，离分布风险描述提高了64.8%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ProGuard is designed to address the emerging multimodal safety risks by identifying out-of-distribution safety issues without model adjustments. It uses a modality-balanced dataset with hierarchical safety annotations and trains a vision-language model via reinforcement learning to generate concise descriptions. ProGuard significantly outperforms existing models in unsafe content categorization and shows a 52.6% improvement in OOD risk detection and 64.8% in OOD risk description.</div>
<div class="mono" style="margin-top:8px">ProGuard旨在通过识别和描述超出分布（OOD）的风险来应对生成模型中的新兴多模态安全风险，而无需对模型进行调整。它使用一个包含87K样本的模态平衡数据集，每个样本都标注了安全标签和风险类别，并通过强化学习训练一个视觉-语言模型来生成简洁的描述。ProGuard在不安全内容分类上优于现有的开源防护模型，并在OOD风险检测上提高了52.6%，在OOD风险描述上提高了64.8%。</div>
</details>
</div>
<div class="card">
<div class="title">ThinkGen: Generalized Thinking for Visual Generation</div>
<div class="meta-line">Authors: Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei</div>
<div class="meta-line">First: 2025-12-29T16:08:50+00:00 · Latest: 2025-12-29T16:08:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23568v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23568v1">PDF</a> · <a href="https://github.com/jiaosiyuu/ThinkGen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM&#x27;s CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ThinkGen: 通用视觉生成的思考驱动方法</div>
<div class="mono" style="margin-top:8px">多模态大型语言模型（MLLMs）的近期进展表明，链式思考（CoT）推理能够系统地解决复杂的理解任务。然而，其在生成任务中的扩展仍处于初级阶段，并受到特定场景机制的限制，这阻碍了泛化和适应。在本文中，我们提出了ThinkGen，这是第一个通过明确利用MLLM的CoT推理来处理各种生成场景的思考驱动视觉生成框架。ThinkGen采用解耦架构，包括一个预训练的MLLM和一个扩散变换器（DiT），其中MLLM根据用户意图生成定制化的指令，DiT则根据这些指令生成高质量的图像。我们进一步提出了一种分离的GRPO训练范式（SepGRPO），交替强化学习MLLM和DiT模块。这种灵活的设计使得跨多种数据集的联合训练成为可能，从而促进广泛生成场景中的有效CoT推理。大量实验表明，ThinkGen在多个生成基准测试中实现了稳健的、最先进的性能。代码可在：https://github.com/jiaosiyuu/ThinkGen 获取</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ThinkGen is a think-driven visual generation framework that leverages the Chain-of-Thought reasoning of Multimodal Large Language Models (MLLMs) to generate images in various scenarios. It uses a decoupled architecture with a pretrained MLLM generating instructions and a Diffusion Transformer producing images. ThinkGen employs a separable GRPO-based training paradigm to enable joint training across diverse datasets, enhancing CoT reasoning. Experiments show that ThinkGen outperforms existing methods on multiple generation benchmarks.</div>
<div class="mono" style="margin-top:8px">ThinkGen 是一种基于 Chain-of-Thought 理论的视觉生成框架，利用多模态大型语言模型进行推理以生成图像。它由一个预训练的 MLLM 生成基于用户意图的指令，以及一个由这些指令引导的扩散变换器生成高质量图像。ThinkGen 采用 SepGRPO 训练范式，实现跨多种数据集的联合训练，增强推理能力。实验表明，ThinkGen 在多个生成基准上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints</div>
<div class="meta-line">Authors: Dimitra Maoutsa</div>
<div class="meta-line">First: 2025-12-29T16:06:08+00:00 · Latest: 2025-12-29T16:06:08+00:00</div>
<div class="meta-line">Comments: 12+50 pages, 6 figures; An earlier account of this work has previously appeared in arXiv:2301.08102 and arXiv:2304.00423 ; main methodology remains the same, this version includes additional numerical experiments and theory</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23566v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23566v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we learn the laws underlying the dynamics of stochastic systems when their trajectories are sampled sparsely in time? Existing methods either require temporally resolved high-frequency observations, or rely on geometric arguments that apply only to conservative systems, limiting the range of dynamics they can recover. Here, we present a new framework that reconciles these two perspectives by reformulating inference as a stochastic control problem. Our method uses geometry-driven path augmentation, guided by the geometry in the system&#x27;s invariant density to reconstruct likely trajectories and infer the underlying dynamics without assuming specific parametric models. Applied to overdamped Langevin systems, our approach accurately recovers stochastic dynamics even from extremely undersampled data, outperforming existing methods in synthetic benchmarks. This work demonstrates the effectiveness of incorporating geometric inductive biases into stochastic system identification methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从几何学到动力学：基于几何约束的稀疏观测过阻尼朗之万动力学学习</div>
<div class="mono" style="margin-top:8px">当随机系统的轨迹在时间上稀疏采样时，我们如何学习其动力学背后的规律？现有方法要么需要高频率的时间分辨观测，要么依赖仅适用于保守系统的几何论证，限制了它们能恢复的动力学范围。本文提出了一种新的框架，通过将推理重新表述为随机控制问题来弥合这两种观点。该方法利用几何驱动的路径增强，通过系统不变密度中的几何结构来重构可能的轨迹并推断出动力学，无需假设特定的参数模型。应用于过阻尼朗之万系统时，该方法即使在极度欠采样的数据中也能准确恢复随机动力学，优于现有方法在合成基准测试中的表现。本文展示了将几何归纳偏置纳入随机系统识别方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of inferring the dynamics of stochastic systems from sparse temporal data. It introduces a framework that combines geometric insights with stochastic control to reconstruct likely trajectories and infer underlying dynamics, without requiring high-frequency data or specific parametric models. The method performs well on overdamped Langevin systems, accurately recovering dynamics from undersampled data and outperforming existing techniques in synthetic benchmarks.</div>
<div class="mono" style="margin-top:8px">该研究解决了从稀疏时间数据中推断随机系统动力学的挑战。它提出了一种结合几何洞察与随机控制的框架，以重构可能的轨迹并推断潜在的动力学，无需高频率数据或特定参数模型。该方法在过阻尼朗之万系统中表现良好，能够从欠采样数据中准确恢复动力学，并在合成基准测试中优于现有技术。</div>
</details>
</div>
<div class="card">
<div class="title">RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature</div>
<div class="meta-line">Authors: Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke</div>
<div class="meta-line">First: 2025-12-29T16:05:38+00:00 · Latest: 2025-12-29T16:05:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23565v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23565v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RxnBench：用于评估大型语言模型在科学文献中理解化学反应能力的多模态基准</div>
<div class="mono" style="margin-top:8px">将多模态大型语言模型（MLLMs）引入化学领域有望彻底改变科学发现，但它们理解科学文献中密集的反应图形语言的能力仍鲜有探索。在此，我们介绍了RxnBench，这是一个多层次基准，旨在严格评估MLLMs在化学反应理解方面的表现。RxnBench 包含两个任务：单图问答（SF-QA），测试细粒度的视觉感知和机制推理，使用来自305个精心策划的反应方案的1,525个问题；全文问答（FD-QA），挑战模型从108篇文章中综合信息，需要跨模态整合文本、方案和表格。我们的评估表明，模型在提取显式文本方面表现出色，但在深入的化学逻辑和精确的结构识别方面存在关键能力差距。值得注意的是，具有推理时推理能力的模型显著优于标准架构，但没有一个在FD-QA上达到50%的准确率。这些发现强调了迫切需要领域特定的视觉编码器和更强的推理引擎，以推进自主人工智能化学家的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to evaluate Multimodal Large Language Models (MLLMs) in understanding chemical reactions from scientific literature, addressing the gap in their ability to comprehend dense, graphical language. RxnBench, a benchmark with two tasks—Single-Figure QA and Full-Document QA—was developed to test visual perception and mechanistic reasoning. The evaluation shows that while models perform well in extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Models with inference-time reasoning outperform standard architectures, but none achieve 50% accuracy on the Full-Document QA task, highlighting the need for domain-specific visual encoders and stronger reasoning engines.</div>
<div class="mono" style="margin-top:8px">RxnBench 是一个基准，用于评估多模态大型语言模型（MLLMs）理解科学文献中化学反应的能力。它包括两个任务：单图问答（SF-QA）和全文档问答（FD-QA）。SF-QA 测试视觉感知和机制推理，而 FD-QA 挑战模型整合来自文本、反应方案和表格的信息。评估显示，虽然模型在提取显式文本方面表现良好，但在深入的化学逻辑和精确的结构识别方面存在困难。具有推理时推理的模型优于标准架构，但没有一个在 FD-QA 上达到 50% 的准确率，这突显了需要更好的视觉编码器和推理引擎以推进自主人工智能化学家的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">VL-RouterBench: A Benchmark for Vision-Language Model Routing</div>
<div class="meta-line">Authors: Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang</div>
<div class="meta-line">First: 2025-12-29T16:01:19+00:00 · Latest: 2025-12-29T16:01:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23562v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23562v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VL-RouterBench：视觉-语言模型路由基准</div>
<div class="mono" style="margin-top:8px">多模型路由已从工程技术演变为必不可少的基础架构，但现有工作缺乏系统且可重复的基准来评估视觉-语言模型（VLMs）。我们提出了VL-RouterBench以系统地评估VLM路由系统的整体能力。基准以视觉-语言模型的原始推理和评分日志为基础，构建了样本-模型对的质量和成本矩阵。在规模上，VL-RouterBench覆盖了3个任务组中的14个数据集，总计30,540个样本，包括15个开源模型和2个API模型，产生了519,180个样本-模型对，总输入输出标记量为34,494,977。评估协议联合测量平均准确率、平均成本和吞吐量，并通过归一化成本和准确率的调和平均值构建排名分数，以在不同的路由配置和成本预算下进行比较。在该基准上，我们评估了10种路由方法和基线，并观察到显著的可路由性提升，而当前最佳的路由器仍然与理想的Oracle存在明显差距，表明通过更精细的视觉线索和文本结构建模，路由架构仍有很大的改进空间。我们将开源完整的数据构建和评估工具链，以促进多模态路由研究中的可比性、可重复性和实际部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VL-RouterBench is designed to evaluate the performance of vision-language model routing systems by analyzing raw inference and scoring logs from 15 open-source and 2 API models across 14 datasets. The benchmark evaluates 10 routing methods and baselines, covering 30,540 samples and 519,180 sample-model pairs, showing significant improvements in routability but still falling short of the ideal Oracle. The evaluation protocol measures accuracy, cost, and throughput, ranking router configurations based on a harmonic mean of normalized cost and accuracy.</div>
<div class="mono" style="margin-top:8px">VL-RouterBench 通过分析来自15个开源和2个API模型的14个数据集的原始推理和评分日志，评估视图语言模型路由系统的性能，涵盖了30,540个样本和519,180个样本-模型对。该基准评估了10种路由方法和基线，显示了显著的可路由性提升，但仍低于理想的Oracle。评估协议衡量准确率、成本和吞吐量，基于标准化准确率和成本的调和平均值对路由配置进行排名。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks</div>
<div class="meta-line">Authors: Toqeer Ali Syed, Mishal Ateeq Almutairi, Mahmoud Abdel Moaty</div>
<div class="meta-line">First: 2025-12-29T15:54:33+00:00 · Latest: 2025-12-29T15:54:33+00:00</div>
<div class="meta-line">Comments: It is accepted in a conference paper, ICCA 2025 in Bahrain on 21 to 23 December</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23557v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23557v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向可信赖的代理AI：多模态预防提示注入攻击框架</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）、视觉-语言模型（VLMs）和新的代理AI系统（如LangChain和GraphChain）使得能够使用和在众多工具和代理之间进行推理、规划和对话的强大自主系统成为可能。然而，这种代理环境增加了多模态提示注入（PI）攻击的可能性，其中隐藏或恶意指令可能通过图传播，导致意外行为、政策违规或状态破坏。为了减轻这些风险，本文提出了一种跨代理多模态来源感知防御框架，该框架对所有提示（无论是用户生成的还是上游代理生成的）进行清理，并在发送给下游节点之前独立验证由LLM生成的所有输出。该框架包括一个文本清理代理、视觉清理代理和输出验证代理，所有这些代理都由一个来源账本协调，该账本在整个代理网络中保留模态、来源和信任级别的元数据。这种架构确保了代理间的通信遵守清晰的信任框架，防止注入指令在LangChain或GraphChain风格的工作流中传播。实验评估表明，多模态注入检测准确性显著提高，跨代理信任泄露最小化，代理执行路径变得稳定。该框架将来源跟踪和验证的概念扩展到多代理编排，增强了安全、可理解且可靠的代理AI系统的建立。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the risk of multimodal prompt injection attacks in agentic AI systems, which can lead to unintended behavior and policy breaches. It proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework that sanitizes prompts and verifies outputs before they are sent to downstream nodes. The framework includes text and visual sanitizers and an output validator coordinated by a provenance ledger. Experimental results show improved detection accuracy, reduced cross-agent trust leakage, and more stable agentic execution pathways. This framework enhances the security and reliability of agentic AI systems by expanding provenance tracking and validation to multi-agent orchestration.</div>
<div class="mono" style="margin-top:8px">论文针对多模态提示注入攻击在智能代理系统中的风险，提出了一种跨代理多模态溯源感知防御框架。该框架包括文本和视觉净化器以及输出验证器，并由溯源账本协调，以确保代理间通信的完整性。实验结果表明，注入检测准确性显著提高，信任泄露减少，代理执行路径更加稳定。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Laws for Energy Efficiency of Local LLMs</div>
<div class="meta-line">Authors: Ander Alvarez, Alessandro Genuardi, Nilotpal Sinha, Antonio Tiene, Mikail Okyay, Bakbergen Ryskulov, David Montero, Samuel Mugel, Román Orús</div>
<div class="meta-line">First: 2025-12-18T13:40:33+00:00 · Latest: 2025-12-29T15:54:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16531v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.16531v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven &quot;resolution knee&quot;, where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>本地LLM能效的标度律</div>
<div class="mono" style="margin-top:8px">在边缘设备上部署本地大型语言模型和视觉-语言模型需要在准确性与受限的计算和能源预算之间进行权衡。尽管图形处理器主导了现代人工智能部署，但大多数消费级硬件（包括笔记本电脑、台式机、工业控制器和嵌入式系统）仍依赖于中央处理器。尽管如此，仅中央处理器的推理计算法则对本地语言和视觉-语言工作负载的研究仍相对较少。我们系统地在两个广泛用于本地推理的中央处理器层级上对大型语言和视觉-语言模型进行了基准测试：一台搭载M2芯片的MacBook Pro，代表主流笔记本电脑级部署，以及一个Raspberry Pi 5，代表受限的、低功耗嵌入式设置。基于连续采样处理器和内存使用情况并结合面积-曲线积分的方法，我们描述了计算负载随输入文本长度对语言模型和随图像分辨率对视觉-语言模型的标度关系。我们发现了两条经验标度律：（1）语言模型推理的计算成本大约与标记长度成线性关系；（2）视觉-语言模型表现出预处理驱动的“分辨率拐点”，其中计算在内部分辨率限制以上保持恒定，在以下则急剧下降。除了这些标度律，我们还表明，基于量子启发的压缩可将处理器和内存使用量最多减少71.9%，能源消耗最多减少62%，同时保持或提高语义准确性。这些结果为本地语言和视觉-语言工作负载的单一中央处理器多模态标度提供了系统量化，并指出了模型压缩和输入分辨率预处理作为可持续边缘推理的有效、低成本杠杆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the energy efficiency of deploying large language models and vision-language models on edge devices, focusing on central processing units. By benchmarking these models on a MacBook Pro M2 and a Raspberry Pi 5, the researchers discovered two scaling laws: the computational cost for language models scales linearly with token length, and vision-language models have a resolution knee where compute remains constant above a certain resolution and decreases below it. Additionally, they found that quantum-inspired compression can reduce processor and memory usage by up to 71.9% and energy consumption by up to 62%, while maintaining or improving semantic accuracy.</div>
<div class="mono" style="margin-top:8px">该研究探讨了在边缘设备上部署大型语言模型和视觉-语言模型时的能效问题，重点关注中央处理单元。通过在MacBook Pro M2和Raspberry Pi 5上进行基准测试，研究人员发现两个缩放定律：语言模型的计算成本与标记长度成线性关系，而视觉-语言模型在某个分辨率以上保持计算不变，在以下则急剧下降。此外，他们发现量子启发式压缩可以将处理器和内存使用量最多减少71.9%，能量消耗最多减少62%，同时保持或提高语义准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs</div>
<div class="meta-line">Authors: Sahil Kale, Antonio Luca Alfeo</div>
<div class="meta-line">First: 2025-12-29T15:41:13+00:00 · Latest: 2025-12-29T15:41:13+00:00</div>
<div class="meta-line">Comments: Accepted to ICPRAM 2026 in Marbella, Spain</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23547v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23547v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>谎言之测：知识图谱在LLM幻觉自我检测中的稳健应用</div>
<div class="mono" style="margin-top:8px">幻觉，即生成看似真实但实际上虚假的陈述，仍然是安全部署LLM的主要障碍。基于自我检测方法的出色表现，我们探讨了使用结构化知识表示，即知识图谱，以提高幻觉自我检测的效果。具体而言，我们提出了一种简单而强大的方法，通过(i) 将LLM的响应转换为实体和关系的知识图谱，以及(ii) 利用这些图谱估计响应中包含幻觉的可能性，来丰富幻觉自我检测。我们使用两个广泛使用的LLM，GPT-4o和Gemini-2.5-Flash，在两个幻觉检测数据集上评估了所提出的方法。为了支持更可靠的未来基准测试，其中一个数据集已被手动整理和增强，并作为本工作的次要成果发布。与标准自我检测方法和SelfCheckGPT（一种最先进的方法）相比，我们的方法在准确性和F1分数上分别实现了高达16%和20%的相对改进。我们的结果表明，当原子事实以知识图谱形式呈现时，LLM能够更好地分析这些事实，即使初始输出包含不准确的信息。这种低成本、模型无关的方法为更安全和可信赖的语言模型铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to address the issue of hallucinations in large language models (LLMs) by leveraging knowledge graphs for robust self-detection. The method involves converting LLM responses into knowledge graphs and using these graphs to estimate the likelihood of hallucinations. Evaluations on GPT-4o and Gemini-2.5-Flash show up to 16% relative improvement in accuracy and 20% in F1-score compared to existing methods. The study also releases a curated dataset for future benchmarking. This approach enhances the analysis of atomic facts and offers a low-cost, model-agnostic solution for safer LLMs.</div>
<div class="mono" style="margin-top:8px">本文提出了一种将模型响应转换为知识图的方法，以提高幻觉检测的准确性。在GPT-4o和Gemini-2.5-Flash上的评估显示，与现有方法相比，该方法在准确率上提高了16%，在F1分数上提高了20%。该方法成本低且适用于多种模型，提升了语言模型的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion MRI with Machine Learning</div>
<div class="meta-line">Authors: Davood Karimi, Simon K. Warfield</div>
<div class="meta-line">First: 2024-01-01T13:03:35+00:00 · Latest: 2025-12-29T15:36:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.00019v6">Abs</a> · <a href="https://arxiv.org/pdf/2402.00019v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">\hspace{2mm} Diffusion-weighted magnetic resonance imaging (dMRI) of the brain offers unique capabilities including noninvasive probing of tissue microstructure and structural connectivity. It is widely used for clinical assessment of disease and injury, and for neuroscience research. Analyzing the dMRI data to extract useful information for medical and scientific purposes can be challenging. The dMRI measurements may suffer from strong noise and artifacts, and may exhibit high inter-session and inter-scanner variability in the data, as well as inter-subject heterogeneity in brain structure. Moreover, the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed data preprocessing and harmonization, microstructure mapping, tractography, and white matter tract analysis. We study the main findings, strengths, and weaknesses of the existing methods and suggest topics for future research. We find that machine learning may be exceptionally suited to tackle some of the difficult tasks in dMRI analysis. However, for this to happen, several shortcomings of existing methods and critical unresolved issues need to be addressed. There is a pressing need to improve evaluation practices, to increase the availability of rich training datasets and validation benchmarks, as well as model generalizability, reliability, and explainability concerns.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散MRI与机器学习</div>
<div class="mono" style="margin-top:8px">扩散加权磁共振成像（dMRI）的脑部成像提供了独特的功能，包括无创探查组织微观结构和结构连接。它广泛用于临床疾病和损伤评估，以及神经科学研究。分析dMRI数据以提取对医疗和科学研究有用的信息具有挑战性。dMRI测量可能受到强烈噪声和伪影的影响，并且数据在不同会话和不同扫描器之间表现出高变异性，同时不同个体的脑结构也存在异质性。此外，测量与感兴趣现象之间的关系可能非常复杂。近年来，dMRI分析中使用机器学习方法的数量不断增加。本文旨在评估这些努力，重点关注解决数据预处理和标准化、微观结构映射、纤维追踪和白质纤维分析的方法。我们研究了现有方法的主要发现、优势和劣势，并建议未来研究的主题。我们发现，机器学习可能特别适合解决dMRI分析中的某些困难任务。然而，为了实现这一点，现有方法的几个缺点和关键未解决的问题需要得到解决。迫切需要改进评估实践，增加丰富的训练数据集和验证基准的可用性，以及提高模型的一般性、可靠性和可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper examines the application of machine learning in diffusion-weighted magnetic resonance imaging (dMRI) analysis, focusing on data preprocessing, microstructure mapping, tractography, and white matter tract analysis. It highlights the challenges in dMRI data, such as noise, artifacts, and variability, and discusses the strengths and limitations of existing machine learning methods. The study suggests that while machine learning can effectively handle complex tasks in dMRI, improvements are needed in evaluation practices, dataset availability, and model reliability and explainability.</div>
<div class="mono" style="margin-top:8px">本文旨在评估机器学习在扩散加权磁共振成像（dMRI）分析中的应用，重点关注数据预处理、微结构映射、纤维追踪和白质纤维分析。文章指出了现有方法的优势和局限性，指出虽然机器学习能够有效处理dMRI中的复杂任务，但仍需改进评估方法、增加丰富数据集的可用性和提高模型的可靠性和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis</div>
<div class="meta-line">Authors: Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang</div>
<div class="meta-line">First: 2025-12-29T15:34:27+00:00 · Latest: 2025-12-29T15:34:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23545v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23545v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PathFound：一种促进证据搜索的病理性诊断代理多模态模型</div>
<div class="mono" style="margin-top:8px">近年来，病理性基础模型在视觉表示学习和多模态交互方面取得了显著进展。然而，大多数模型仍然依赖于静态推理范式，在这种范式中，全切片图像仅处理一次以生成预测，而不会在模糊诊断下进行重新评估或有针对性的证据获取。这与临床诊断工作流程形成对比，后者通过反复观察切片和进一步的检查请求来细化假设。我们提出PathFound，一种旨在支持病理性诊断中证据搜索推理的代理多模态模型。PathFound 结合了病理视觉基础模型、视觉-语言模型以及通过强化学习训练的推理模型的力量，通过初始诊断、证据搜索和最终决策阶段来主动获取信息并改进诊断。在多个大型多模态模型中，采用这种策略始终能提高诊断准确性，表明在计算病理学中采用证据搜索工作流程的有效性。在这些模型中，PathFound 在多种临床场景中实现了最先进的诊断性能，并展示了发现细微特征（如核特征和局部侵袭）的强大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PathFound is an agentic multimodal model that supports evidence-seeking inference in pathological diagnosis, addressing the limitations of static inference paradigms. It integrates visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to refine diagnoses through stages of initial diagnosis, evidence-seeking, and final decision. Experiments show that PathFound improves diagnostic accuracy across various models and clinical scenarios, highlighting the effectiveness of evidence-seeking workflows in computational pathology.</div>
<div class="mono" style="margin-top:8px">研究旨在通过开发一种名为PathFound的主动式多模态模型来提高病理诊断的准确性。PathFound结合了视觉基础模型、视觉-语言模型和通过强化学习训练的推理模型，主动寻求证据并细化诊断。该模型通过初始诊断、证据寻求和最终决策阶段的流程，相较于静态推理模型，在多种临床场景中表现出更高的诊断准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Act2Goal: From World Model To General Goal-conditioned Policy</div>
<div class="meta-line">Authors: Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo</div>
<div class="meta-line">First: 2025-12-29T15:28:42+00:00 · Latest: 2025-12-29T15:28:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23541v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://act2goal.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Act2Goal: 从世界模型到通用目标条件策略</div>
<div class="mono" style="margin-top:8px">以既表达能力强又精确的方式指定机器人操作任务仍然是一个核心挑战。虽然视觉目标提供了一种紧凑且无歧义的任务规范，但现有的目标条件策略往往难以处理长时序操作，因为它们依赖于单步动作预测，而没有明确建模任务进展。我们提出了Act2Goal，这是一种通用的目标条件操作策略，结合了目标条件视觉世界模型和多尺度时间控制。给定当前观察和目标视觉目标，世界模型生成一个可能的中间视觉状态序列，捕捉长时序结构。为了将这个视觉计划转化为稳健的执行，我们引入了多尺度时间哈希（MSTH），它将想象的轨迹分解为密集的近端帧和稀疏的远端帧，以实现细粒度的闭环控制并锚定全局任务一致性。策略通过端到端的交叉注意力将这些表示与运动控制耦合，从而实现连贯的长时序行为，同时对局部干扰保持反应。Act2Goal 在新对象、空间布局和环境上的零样本泛化表现优异。我们进一步通过基于LoRA的后见之明目标重新标记实现无奖励的在线适应，允许快速自主改进而无需外部监督。实验证明，Act2Goal 在几分钟的自主交互后，成功率达到从30%到90%，验证了多尺度时间控制的目标条件世界模型为稳健的长时序操作提供了必要的结构指导。项目页面：https://act2goal.github.io/</div>
</details>
</div>
<div class="card">
<div class="title">AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization</div>
<div class="meta-line">Authors: Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang</div>
<div class="meta-line">First: 2025-12-29T15:26:25+00:00 · Latest: 2025-12-29T15:26:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23537v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject&#x27;s attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyMS：基于布局引导和无需训练的多主题定制中自底向上注意力解耦</div>
<div class="mono" style="margin-top:8px">多主题定制旨在将多个用户指定的主题合成到一个连贯的图像中。为了解决主题缺失或冲突等问题，最近的工作引入了布局指导以提供明确的空间约束。然而，现有方法仍然难以平衡文本对齐、主题身份保留和布局控制这三个关键目标，而依赖额外训练进一步限制了其可扩展性和效率。在本文中，我们提出了一种名为AnyMS的新颖的无需训练框架，用于布局引导的多主题定制。AnyMS利用三种输入条件：文本提示、主题图像和布局约束，并引入了一种自底向上的双层注意力解耦机制，以在生成过程中协调它们的整合。具体而言，全局解耦将文本和视觉条件之间的跨注意力分离，以确保文本对齐。局部解耦将每个主题的注意力限制在其指定区域内，从而防止主题冲突，从而保证身份保留和布局控制。此外，AnyMS使用预训练的图像适配器来提取与扩散模型对齐的主题特定特征，从而去除主题学习或适配器调优的需要。大量实验表明，AnyMS达到了最先进的性能，支持复杂的组合，并可扩展到更多的主题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AnyMS is a training-free framework for layout-guided multi-subject customization that addresses the challenges of text alignment, subject identity preservation, and layout control. It uses a bottom-up dual-level attention decoupling mechanism to integrate text prompts, subject images, and layout constraints. Global decoupling ensures text alignment, while local decoupling prevents subject conflicts, preserving identity and layout. Pre-trained image adapters are used to extract subject-specific features, eliminating the need for subject learning or adapter tuning. Experiments show that AnyMS outperforms existing methods in handling complex compositions and scaling to multiple subjects.</div>
<div class="mono" style="margin-top:8px">AnyMS 是一个无需训练的多主题定制框架，通过引入布局指导来解决文本对齐、主题身份保留和布局控制的挑战。它使用自底向上的双层注意力解耦机制，分离文本和视觉条件之间的交叉注意力以实现文本对齐，并将每个主题的注意力限制在其指定区域内，以防止冲突并确保身份保留和布局控制。实验表明，AnyMS 在处理复杂组合和扩展到更多主题方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Timepoint-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI</div>
<div class="meta-line">Authors: Wenhao Guo, Golrokh Mirzaei</div>
<div class="meta-line">Venue: Cancers, 2026, 18(1), 36</div>
<div class="meta-line">First: 2025-11-23T19:38:03+00:00 · Latest: 2025-12-29T15:25:19+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18595v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18595v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset&#x27;s size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>胶质母细胞瘤随访MRI中深度学习模型的时间点特定基准测试</div>
<div class="mono" style="margin-top:8px">在胶质母细胞瘤中，从真正的肿瘤进展（TP）区分治疗相关的假进展（PsP）在早期随访中尤为具有挑战性。我们首次对胶质母细胞瘤进展队列（n = 180）的随访MRI使用深度学习模型进行了阶段特定、横断面基准测试。我们独立分析不同的放疗后扫描，以测试架构性能是否依赖于时间点。在统一的、经过质量控制驱动的管道中，使用患者水平交叉验证训练了11种代表性DL家族（CNNs、LSTMs、混合模型、变压器和选择性状态空间模型）。在两个阶段中，准确率相当（~0.70-0.74），但在第二次随访中，几种模型的F1和AUC有所提高，表明后期护理路径中区分能力更丰富。Mamba+CNN混合模型始终提供了最佳的准确率-效率权衡，而变压器变体在显著更高的计算成本下提供了竞争力的AUC，轻量级CNN在效率上表现出色但可靠性较低。性能还对批次大小敏感，强调了标准化训练协议的必要性。值得注意的是，总体绝对区分能力仍然有限，反映了TP与PsP之间的固有难度以及数据集的大小不平衡。这些结果建立了阶段意识基准，并激发了未来结合纵向建模、多序列MRI和更大规模多中心队列的工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study benchmarks deep learning models for distinguishing true tumor progression from treatment-related pseudoprogression in glioblastoma follow-up MRI scans. Eleven model architectures were evaluated across different post-radiation therapy scans, showing comparable accuracy but improved discrimination at the second follow-up. A Mamba+CNN hybrid provided the best accuracy-efficiency trade-off, while transformer models had higher computational costs. The results highlight the need for standardized training protocols and suggest that absolute discrimination remains challenging due to the intrinsic difficulty of the task and dataset imbalance.</div>
<div class="mono" style="margin-top:8px">研究旨在通过比较不同时间点的MRI图像，评估深度学习模型在区分胶质母细胞瘤真实进展与治疗相关假进展方面的性能。使用180名患者的队列训练了11种深度学习架构，并在不同放射治疗后的时间点上进行评估。总体而言，各阶段的准确率相似，但在第二次随访时，区分能力有所提高，表明后期的可区分性更强。Mamba+CNN混合模型在准确性和效率之间表现最佳，而变压器模型虽然AUC较高，但计算成本更高。研究强调了标准化训练协议和更大规模数据集的必要性，以提高区分能力。</div>
</details>
</div>
<div class="card">
<div class="title">When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework</div>
<div class="meta-line">Authors: Haoyu Liu, Chaoyu Gong, Mengke He, Jiate Li, Kai Han, Siqiang Luo</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-08-07T15:55:13+00:00 · Latest: 2025-12-29T15:24:22+00:00</div>
<div class="meta-line">Comments: Accepted to KDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05526v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.05526v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of generative video models has made detecting AI-generated and manipulated videos an urgent challenge. Existing detection approaches often fail to generalize across diverse manipulation types due to their reliance on isolated spatial, temporal, or spectral information, and typically require large models to perform well. This paper introduces SSTGNN, a lightweight Spatial-Spectral-Temporal Graph Neural Network framework that represents videos as structured graphs, enabling joint reasoning over spatial inconsistencies, temporal artifacts, and spectral distortions. SSTGNN incorporates learnable spectral filters and spatial-temporal differential modeling into a unified graph-based architecture, capturing subtle manipulation traces more effectively. Extensive experiments on diverse benchmark datasets demonstrate that SSTGNN not only achieves superior performance in both in-domain and cross-domain settings, but also offers strong efficiency and resource allocation. Remarkably, SSTGNN accomplishes these results with up to 42$\times$ fewer parameters than state-of-the-art models, making it highly lightweight and resource-friendly for real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当深度伪造检测遇到图神经网络：统一且轻量级的学习框架</div>
<div class="mono" style="margin-top:8px">生成视频模型的普及使得检测AI生成和篡改的视频成为迫切的挑战。现有检测方法往往由于依赖孤立的空间、时间和频谱信息而难以在多种篡改类型之间泛化，通常需要大型模型才能表现良好。本文介绍了一种轻量级的时空频谱图神经网络框架SSTGNN，该框架将视频表示为结构化的图，能够联合推理空间不一致、时间伪影和频谱失真。SSTGNN将可学习的频谱滤波器和时空差分建模统一到基于图的架构中，更有效地捕捉细微的篡改痕迹。在多种基准数据集上的广泛实验表明，SSTGNN不仅在领域内和跨领域设置中均表现出优越的性能，而且具有强大的效率和资源分配能力。令人惊讶的是，SSTGNN仅需比最先进的模型少42倍的参数，使其在实际部署中具有高度的轻量化和资源友好性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of detecting AI-generated and manipulated videos by introducing SSTGNN, a lightweight Spatial-Spectral-Temporal Graph Neural Network framework. SSTGNN represents videos as structured graphs to jointly reason over spatial inconsistencies, temporal artifacts, and spectral distortions. Experimental results show that SSTGNN outperforms existing methods in both in-domain and cross-domain settings while requiring significantly fewer parameters, making it highly efficient and resource-friendly for real-world deployment.</div>
<div class="mono" style="margin-top:8px">本文通过引入轻量级的时空频图神经网络框架SSTGNN，解决了检测AI生成和篡改视频的挑战。SSTGNN将视频表示为结构化的图，以联合推理空间不一致、时间伪影和频谱失真。该框架结合了可学习的频谱滤波器和时空差分建模，有效捕捉细微的篡改痕迹。实验结果表明，SSTGNN在领域内和跨领域设置中均优于最先进的模型，同时参数量最多减少42倍，使其在实际部署中具有高效和资源友好性。</div>
</details>
</div>
<div class="card">
<div class="title">Machine Unlearning using Forgetting Neural Networks</div>
<div class="meta-line">Authors: Amartya Hatua, Trung T. Nguyen, Filip Cano, Andrew H. Sung</div>
<div class="meta-line">First: 2024-10-29T02:52:26+00:00 · Latest: 2025-12-29T15:15:41+00:00</div>
<div class="meta-line">Comments: 12 Pages, Accepted at ICAART 2026 - 18th International Conference on Agents and Artificial Intelligence</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.22374v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.22374v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern computer systems store vast amounts of personal data, enabling advances in AI and ML but risking user privacy and trust. For privacy reasons, it is sometimes desired for an ML model to forget part of the data it was trained on. In this paper, we introduce a novel unlearning approach based on Forgetting Neural Networks (FNNs), a neuroscience-inspired architecture that explicitly encodes forgetting through multiplicative decay factors. While FNNs had previously been studied as a theoretical construct, we provide the first concrete implementation and demonstrate their effectiveness for targeted unlearning. We propose several variants with per-neuron forgetting factors, including rank-based assignments guided by activation levels, and evaluate them on MNIST and Fashion-MNIST benchmarks. Our method systematically removes information associated with forget sets while preserving performance on retained data. Membership inference attacks confirm the effectiveness of FNN-based unlearning in erasing information about the training data from the neural network. These results establish FNNs as a promising foundation for efficient and interpretable unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用遗忘神经网络的机器卸载</div>
<div class="mono" style="margin-top:8px">现代计算机系统存储了大量的个人数据，这虽然促进了人工智能和机器学习的发展，但也可能损害用户的隐私和信任。出于隐私原因，有时希望机器学习模型忘记其训练数据的一部分。在本文中，我们介绍了一种基于遗忘神经网络（FNNs）的新颖卸载方法，这是一种受神经科学启发的架构，通过乘法衰减因子明确编码遗忘。虽然FNNs之前曾被作为理论构架研究，但我们提供了第一个具体的实现，并证明了它们在目标卸载中的有效性。我们提出了几种变体，包括基于激活水平的排名分配的每神经元遗忘因子，并在MNIST和Fashion-MNIST基准上进行了评估。我们的方法系统地移除了与遗忘集相关的信息，同时保留了保留数据的性能。成员推理攻击证实了基于FNN的卸载在从神经网络中擦除训练数据信息方面的有效性。这些结果确立了FNNs作为高效和可解释卸载的基础的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of unlearning in machine learning models to protect user privacy. It introduces Forgetting Neural Networks (FNNs), which use multiplicative decay factors to encode forgetting. The authors evaluate several FNN variants on MNIST and Fashion-MNIST datasets and show that FNNs can effectively remove information associated with specific data points while maintaining model performance on other data. Membership inference attacks confirm the successful erasure of training data information from the neural network.</div>
<div class="mono" style="margin-top:8px">本文探讨了机器学习模型中遗忘的问题，以保护用户隐私。提出了遗忘神经网络（FNNs），通过乘法衰减因子来编码遗忘。作者在MNIST和Fashion-MNIST上评估了FNN的几种变体，展示了这些网络能够有效移除特定训练数据的相关信息，同时保持对其他数据的性能。成员推理攻击证实了FNN在神经网络中成功擦除信息的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Expressive Temporal Specifications for Reward Monitoring</div>
<div class="meta-line">Authors: Omar Adalat, Francesco Belardinelli</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-11-16T22:28:30+00:00 · Latest: 2025-12-29T15:04:16+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12808v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.12808v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of specifying dense reward functions in Reinforcement Learning to improve agent training efficiency. It uses quantitative Linear Temporal Logic on finite traces to synthesize reward monitors that provide nuanced feedback during training. Experiments demonstrate that these monitors consistently outperform Boolean monitors in terms of task completion and convergence time in various environments.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过提供密集的奖励函数来提高强化学习中代理训练的效率。它利用有限轨迹上的定量线性时序逻辑来合成奖励监控器，这些监控器在训练过程中提供详细的反馈。实验结果表明，这些监控器在任务完成和收敛时间方面优于布尔监控器，适用于各种环境。</div>
</details>
</div>
<div class="card">
<div class="title">Ordinal Adaptive Correction: A Data-Centric Approach to Ordinal Image Classification with Noisy Labels</div>
<div class="meta-line">Authors: Alireza Sedighi Moghaddam, Mohammad Reza Mohammadi</div>
<div class="meta-line">First: 2025-09-02T14:17:16+00:00 · Latest: 2025-12-29T15:03:06+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02351v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.02351v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Labeled data is a fundamental component in training supervised deep learning models for computer vision tasks. However, the labeling process, especially for ordinal image classification where class boundaries are often ambiguous, is prone to error and noise. Such label noise can significantly degrade the performance and reliability of machine learning models. This paper addresses the problem of detecting and correcting label noise in ordinal image classification tasks. To this end, a novel data-centric method called ORDinal Adaptive Correction (ORDAC) is proposed for adaptive correction of noisy labels. The proposed approach leverages the capabilities of Label Distribution Learning (LDL) to model the inherent ambiguity and uncertainty present in ordinal labels. During training, ORDAC dynamically adjusts the mean and standard deviation of the label distribution for each sample. Rather than discarding potentially noisy samples, this approach aims to correct them and make optimal use of the entire training dataset. The effectiveness of the proposed method is evaluated on benchmark datasets for age estimation (Adience) and disease severity detection (Diabetic Retinopathy) under various asymmetric Gaussian noise scenarios. Results show that ORDAC and its extended versions (ORDAC_C and ORDAC_R) lead to significant improvements in model performance. For instance, on the Adience dataset with 40% noise, ORDAC_R reduced the mean absolute error from 0.86 to 0.62 and increased the recall metric from 0.37 to 0.49. The method also demonstrated its effectiveness in correcting intrinsic noise present in the original datasets. This research indicates that adaptive label correction using label distributions is an effective strategy to enhance the robustness and accuracy of ordinal classification models in the presence of noisy data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>序数自适应校正：一种面向数据的序数图像分类噪声标签校正方法</div>
<div class="mono" style="margin-top:8px">标记数据是训练计算机视觉任务的监督深度学习模型的基本组成部分。然而，标记过程，尤其是在序数图像分类中，由于类边界往往模糊不清，容易出错和产生噪声。这种标签噪声会显著降低机器学习模型的性能和可靠性。本文针对序数图像分类任务中的标签噪声检测和校正问题进行了研究。为此，提出了一种新颖的数据为中心的方法，称为序数自适应校正（ORDAC），用于噪声标签的自适应校正。该方法利用标签分布学习（LDL）的能力来建模序数标签中存在的固有模糊性和不确定性。在训练过程中，ORDAC 动态调整每个样本的标签分布的均值和标准差。该方法的目标是校正这些潜在的噪声样本，并充分利用整个训练数据集。通过在年龄估计（Adience）和疾病严重程度检测（糖尿病视网膜病变）基准数据集上进行各种非对称高斯噪声场景下的评估，证明了所提出方法的有效性。例如，在Adience数据集上噪声比例为40%的情况下，ORDAC_R将平均绝对误差从0.86降低到0.62，召回率从0.37提高到0.49。该方法还证明了其在纠正原始数据集中固有的噪声方面的有效性。这项研究表明，在噪声数据存在的情况下，使用标签分布进行自适应标签校正是增强序数分类模型的鲁棒性和准确性的有效策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the issue of label noise in ordinal image classification, proposing a data-centric method called ORDinal Adaptive Correction (ORDAC). ORDAC uses Label Distribution Learning to model the ambiguity in ordinal labels and dynamically adjusts the mean and standard deviation of the label distribution during training. Experimental results on age estimation and disease severity detection datasets show significant improvements in model performance, with reductions in mean absolute error and increases in recall metrics under noisy conditions.</div>
<div class="mono" style="margin-top:8px">本文针对有序图像分类中的标签噪声问题，提出了一种数据为中心的方法——ORDinal Adaptive Correction (ORDAC)。ORDAC 使用 Label Distribution Learning 来建模有序标签中的模糊性，并在训练过程中动态调整每个样本的标签分布的均值和标准差。实验结果表明，ORDAC 及其变体在年龄估计和疾病严重程度检测数据集上的表现显著提升，减少了均绝对误差并提高了召回率指标，特别是在噪声条件下。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251230_0328.html">20251230_0328</a>
<a href="archive/20251229_0326.html">20251229_0326</a>
<a href="archive/20251228_0329.html">20251228_0329</a>
<a href="archive/20251227_0325.html">20251227_0325</a>
<a href="archive/20251226_0326.html">20251226_0326</a>
<a href="archive/20251225_0325.html">20251225_0325</a>
<a href="archive/20251224_0328.html">20251224_0328</a>
<a href="archive/20251223_0327.html">20251223_0327</a>
<a href="archive/20251222_0324.html">20251222_0324</a>
<a href="archive/20251221_0326.html">20251221_0326</a>
<a href="archive/20251220_0327.html">20251220_0327</a>
<a href="archive/20251219_0327.html">20251219_0327</a>
<a href="archive/20251218_0339.html">20251218_0339</a>
<a href="archive/20251217_0331.html">20251217_0331</a>
<a href="archive/20251216_0329.html">20251216_0329</a>
<a href="archive/20251215_0331.html">20251215_0331</a>
<a href="archive/20251214_0324.html">20251214_0324</a>
<a href="archive/20251213_0324.html">20251213_0324</a>
<a href="archive/20251212_0329.html">20251212_0329</a>
<a href="archive/20251211_0326.html">20251211_0326</a>
<a href="archive/20251210_0323.html">20251210_0323</a>
<a href="archive/20251209_0326.html">20251209_0326</a>
<a href="archive/20251208_0324.html">20251208_0324</a>
<a href="archive/20251207_0323.html">20251207_0323</a>
<a href="archive/20251206_0325.html">20251206_0325</a>
<a href="archive/20251205_0326.html">20251205_0326</a>
<a href="archive/20251204_0326.html">20251204_0326</a>
<a href="archive/20251203_0328.html">20251203_0328</a>
<a href="archive/20251202_0331.html">20251202_0331</a>
<a href="archive/20251201_0324.html">20251201_0324</a>
<a href="archive/20251130_0323.html">20251130_0323</a>
<a href="archive/20251129_0323.html">20251129_0323</a>
<a href="archive/20251128_0324.html">20251128_0324</a>
<a href="archive/20251127_0324.html">20251127_0324</a>
<a href="archive/20251126_0325.html">20251126_0325</a>
<a href="archive/20251125_0322.html">20251125_0322</a>
<a href="archive/20251124_0323.html">20251124_0323</a>
<a href="archive/20251123_0323.html">20251123_0323</a>
<a href="archive/20251122_0325.html">20251122_0325</a>
<a href="archive/20251121_0324.html">20251121_0324</a>
<a href="archive/20251120_0326.html">20251120_0326</a>
<a href="archive/20251119_0325.html">20251119_0325</a>
<a href="archive/20251118_0324.html">20251118_0324</a>
<a href="archive/20251117_0322.html">20251117_0322</a>
<a href="archive/20251116_0322.html">20251116_0322</a>
<a href="archive/20251115_0324.html">20251115_0324</a>
<a href="archive/20251114_0325.html">20251114_0325</a>
<a href="archive/20251113_0326.html">20251113_0326</a>
<a href="archive/20251112_0326.html">20251112_0326</a>
<a href="archive/20251111_0318.html">20251111_0318</a>
<a href="archive/20251110_0322.html">20251110_0322</a>
<a href="archive/20251109_0323.html">20251109_0323</a>
<a href="archive/20251108_0321.html">20251108_0321</a>
<a href="archive/20251107_0320.html">20251107_0320</a>
<a href="archive/20251106_0322.html">20251106_0322</a>
<a href="archive/20251105_0321.html">20251105_0321</a>
<a href="archive/20251104_0324.html">20251104_0324</a>
<a href="archive/20251103_0317.html">20251103_0317</a>
<a href="archive/20251102_0321.html">20251102_0321</a>
<a href="archive/20251101_0317.html">20251101_0317</a>
<a href="archive/20251031_0318.html">20251031_0318</a>
<a href="archive/20251030_0328.html">20251030_0328</a>
<a href="archive/20251029_0325.html">20251029_0325</a>
<a href="archive/20251028_0324.html">20251028_0324</a>
<a href="archive/20251027_0320.html">20251027_0320</a>
<a href="archive/20251026_0328.html">20251026_0328</a>
<a href="archive/20251025_0320.html">20251025_0320</a>
<a href="archive/20251024_0328.html">20251024_0328</a>
<a href="archive/20251023_1235.html">20251023_1235</a>
<a href="archive/20251023_0316.html">20251023_0316</a>
<a href="archive/20251022_0319.html">20251022_0319</a>
<a href="archive/20251021_1916.html">20251021_1916</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
