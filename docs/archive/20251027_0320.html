<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-10-27 03:20</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251027_0320</div>
    <div class="row"><div class="card">
<div class="title">HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video   Narratives</div>
<div class="meta-line">Authors: Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</div>
<div class="meta-line">First: 2025-10-23T17:59:59+00:00 · Latest: 2025-10-23T17:59:59+00:00</div>
<div class="meta-line">Comments: Project page and code: https://holo-cine.github.io/</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20822v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20822v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://holo-cine.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art text-to-video models excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling. We bridge this &quot;narrative gap&quot; with HoloCine, a model
that generates entire scenes holistically to ensure global consistency from the
first shot to the last. Our architecture achieves precise directorial control
through a Window Cross-Attention mechanism that localizes text prompts to
specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within
shots but sparse between them) ensures the efficiency required for minute-scale
generation. Beyond setting a new state-of-the-art in narrative coherence,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques. Our work
marks a pivotal shift from clip synthesis towards automated filmmaking, making
end-to-end cinematic creation a tangible future. Our code is available at:
https://holo-cine.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HoloCine：整体生成电影多镜头长视频叙事</div>
<div class="mono" style="margin-top:8px">最先进的文本到视频模型在生成孤立片段方面表现出色，但在创建连贯的多镜头叙事方面却力有未逮，而多镜头叙事正是叙事的核心。我们通过HoloCine模型填补了这一“叙事缺口”，该模型能够整体生成整个场景，以确保从第一镜头到最后一个镜头的全局一致性。我们的架构通过窗口交叉注意力机制实现精确的导演控制，将文本提示定位到特定的镜头上，同时稀疏跨镜头自注意力模式（镜头内部密集但镜头之间稀疏）确保了分钟级生成所需的效率。除了在叙事连贯性方面达到新的最先进的水平外，HoloCine还发展出非凡的涌现能力：持久的角色和场景记忆，以及对电影技术的直观理解。我们的工作标志着从片段合成向自动化电影制作的转折点，使端到端的电影创作成为可实现的未来。我们的代码可在：https://holo-cine.github.io/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">HoloCine addresses the limitation of current text-to-video models in generating coherent multi-shot narratives by introducing a holistic generation approach. It uses a Window Cross-Attention mechanism for precise control over text prompts and a Sparse Inter-Shot Self-Attention pattern to ensure efficiency. The model demonstrates remarkable emergent abilities, including persistent memory and understanding of cinematic techniques, setting a new standard in narrative coherence and automated filmmaking.</div>
<div class="mono" style="margin-top:8px">HoloCine通过引入整体生成方法解决了当前文本到视频模型在生成连贯多镜头叙事方面的局限性。它使用Window Cross-Attention机制实现对文本提示的精确控制，并使用Sparse Inter-Shot Self-Attention模式确保效率。该模型展示了持久记忆和对电影技术的理解等出色的 emergent 能力，为叙事连贯性和自动化电影制作设定了新标准。</div>
</details>
</div>
<div class="card">
<div class="title">One-Step Offline Distillation of Diffusion-based Models via Koopman   Modeling</div>
<div class="meta-line">Authors: Nimrod Berman, Ilan Naiman, Moshe Eliasof, Hedi Zisling, Omri Azencot</div>
<div class="meta-line">First: 2025-05-19T16:59:47+00:00 · Latest: 2025-10-23T17:59:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2505.13358v3">Abs</a> · <a href="http://arxiv.org/pdf/2505.13358v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based generative models have demonstrated exceptional performance,
yet their iterative sampling procedures remain computationally expensive. A
prominent strategy to mitigate this cost is distillation, with offline
distillation offering particular advantages in terms of efficiency, modularity,
and flexibility. In this work, we identify two key observations that motivate a
principled distillation framework: (1) while diffusion models have been viewed
through the lens of dynamical systems theory, powerful and underexplored tools
can be further leveraged; and (2) diffusion models inherently impose
structured, semantically coherent trajectories in latent space. Building on
these observations, we introduce the Koopman Distillation Model (KDM), a novel
offline distillation approach grounded in Koopman theory - a classical
framework for representing nonlinear dynamics linearly in a transformed space.
KDM encodes noisy inputs into an embedded space where a learned linear operator
propagates them forward, followed by a decoder that reconstructs clean samples.
This enables single-step generation while preserving semantic fidelity. We
provide theoretical justification for our approach: (1) under mild assumptions,
the learned diffusion dynamics admit a finite-dimensional Koopman
representation; and (2) proximity in the Koopman latent space correlates with
semantic similarity in the generated outputs, allowing for effective trajectory
alignment. KDM achieves highly competitive performance across standard offline
distillation benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Koopman建模的一步离线蒸馏扩散模型</div>
<div class="mono" style="margin-top:8px">基于扩散的生成模型已经展示了出色的表现，但其迭代采样过程仍然计算成本高昂。一种减轻这种成本的显著策略是蒸馏，离线蒸馏特别在效率、模块化和灵活性方面具有优势。在本文中，我们识别了两个关键观察结果，以指导一个有原则的蒸馏框架：(1) 尽管扩散模型被视作动力系统理论的视角，但还有强大的且未充分探索的工具可以进一步利用；(2) 扩散模型本质上会在潜在空间中施加结构化的、语义连贯的轨迹。基于这些观察结果，我们引入了Koopman蒸馏模型(KDM)，这是一种基于Koopman理论的新型离线蒸馏方法——Koopman理论是一种经典框架，用于在变换空间中将非线性动力学线性表示。KDM将嘈杂的输入编码到嵌入空间中，在该空间中，一个学习到的线性算子将它们向前传播，随后通过解码器重建干净的样本。这使得单步生成同时保持语义保真度。我们为我们的方法提供了理论上的证明：(1) 在温和的假设下，学习到的扩散动力学具有有限维的Koopman表示；(2) 在Koopman潜在空间中的邻近性与生成输出中的语义相似性相关，允许有效的轨迹对齐。KDM在标准的离线蒸馏基准测试中取得了高度竞争力的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the computational inefficiency of diffusion-based generative models by proposing the Koopman Distillation Model (KDM). Motivated by the potential of Koopman theory to linearize nonlinear dynamics and the inherent structured trajectories in latent space of diffusion models, KDM uses an offline distillation approach. It encodes noisy inputs into an embedded space where a learned linear operator propagates them, followed by a decoder to reconstruct clean samples. Theoretical justification supports the approach, showing that learned diffusion dynamics can be represented in a finite-dimensional Koopman space and that proximity in this space correlates with semantic similarity in generated outputs. KDM performs competitively in offline distillation benchmarks.</div>
<div class="mono" style="margin-top:8px">本文通过提出Koopman Distillation Model (KDM)，利用Koopman理论实现了一步离线蒸馏，以解决基于扩散的生成模型的计算效率问题。KDM将输入编码到一个潜在空间，在该空间中，一个线性算子进行传播，随后通过解码器重建干净样本，从而保持语义保真度。理论分析支持该方法，表明学习到的扩散动力学可以在有限维的Koopman空间中表示，并且该空间中的邻近性与生成输出的语义相似性相关。KDM在标准的离线蒸馏基准测试中表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered   Canvas</div>
<div class="meta-line">Authors: Guocheng Gordon Qian, Ruihang Zhang, Tsai-Shien Chen, Yusuf Dalva, Anujraaj Argo Goyal, Willi Menapace, Ivan Skorokhodov, Meng Dong, Arpit Sahni, Daniil Ostashev, Ju Hu, Sergey Tulyakov, Kuan-Chieh Jackson Wang</div>
<div class="meta-line">First: 2025-10-23T17:59:55+00:00 · Latest: 2025-10-23T17:59:55+00:00</div>
<div class="meta-line">Comments: 9 pages, preprint</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20820v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their impressive visual fidelity, existing personalized generative
models lack interactive control over spatial composition and scale poorly to
multiple subjects. To address these limitations, we present LayerComposer, an
interactive framework for personalized, multi-subject text-to-image generation.
Our approach introduces two main contributions: (1) a layered canvas, a novel
representation in which each subject is placed on a distinct layer, enabling
occlusion-free composition; and (2) a locking mechanism that preserves selected
layers with high fidelity while allowing the remaining layers to adapt flexibly
to the surrounding context. Similar to professional image-editing software, the
proposed layered canvas allows users to place, resize, or lock input subjects
through intuitive layer manipulation. Our versatile locking mechanism requires
no architectural changes, relying instead on inherent positional embeddings
combined with a new complementary data sampling strategy. Extensive experiments
demonstrate that LayerComposer achieves superior spatial control and identity
preservation compared to the state-of-the-art methods in multi-subject
personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LayerComposer：基于空间感知分层画布的交互式个性化T2I</div>
<div class="mono" style="margin-top:8px">尽管现有的个性化生成模型具有出色的视觉保真度，但它们缺乏对空间组成进行交互式控制的能力，并且在处理多个主体时表现不佳。为了解决这些限制，我们提出了LayerComposer，这是一种交互式的多主体文本到图像生成框架。我们的方法引入了两个主要贡献：(1) 分层画布，这是一种新颖的表示方法，其中每个主体位于不同的层上，从而实现无遮挡的组合；(2) 锁定机制，该机制保持选定层的高保真度，同时允许其余层灵活适应周围环境。类似于专业的图像编辑软件，所提出的分层画布允许用户通过直观的分层操作放置、调整大小或锁定输入主体。我们的多功能锁定机制不需要进行架构更改，而是依赖于固有的位置嵌入以及一种新的互补数据采样策略。广泛的实验表明，与多主体个性化图像生成的最新方法相比，LayerComposer在空间控制和身份保留方面具有优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LayerComposer is an interactive framework for generating personalized multi-subject text-to-image outputs. It introduces a layered canvas where each subject is placed on a distinct layer, allowing for occlusion-free composition and flexible adaptation of the remaining layers. The locking mechanism preserves selected layers while adapting others to the context. Experiments show that LayerComposer outperforms existing methods in terms of spatial control and identity preservation in multi-subject image generation.</div>
<div class="mono" style="margin-top:8px">LayerComposer 是一个交互式框架，用于生成多主体的文本到图像，并具有空间控制能力。它引入了一种分层画布，其中每个主体位于一个独立的层上，允许无遮挡的组合，并且具有一种锁定机制，可以保留选定的层同时使其余层适应周围环境。实验表明，LayerComposer 在多主体图像生成中的空间控制和身份保留方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards General Modality Translation with Contrastive and Predictive   Latent Diffusion Bridge</div>
<div class="meta-line">Authors: Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</div>
<div class="meta-line">First: 2025-10-23T17:59:54+00:00 · Latest: 2025-10-23T17:59:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20819v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/lddbm/home">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用模态翻译的对比预测潜在去噪扩散桥模型</div>
<div class="mono" style="margin-top:8px">生成建模的最新进展将去噪扩散模型定位为从复杂数据分布中采样的最先进的工具。尽管这些模型在图像和音频等单一模态领域取得了显著的成功，但将它们的能力扩展到模态翻译（MT），即在不同感官模态之间翻译信息，仍然是一个开放的挑战。现有方法通常依赖于共享维度、高斯先验和特定模态的架构等限制性假设，这限制了它们的通用性和理论基础。在本工作中，我们提出了潜在去噪扩散桥模型（LDDBM），这是一种基于潜在变量扩展的去噪扩散桥模型的通用框架。通过在共享的潜在空间中操作，我们的方法可以在不需要对齐维度的情况下学习任意模态之间的桥梁。我们引入了一种对比对齐损失来强制配对样本之间的语义一致性，并设计了一种通用的编码器-解码器架构，专门用于潜在空间中的噪声预测。此外，我们提出了预测损失来指导训练以实现准确的跨域翻译，并探索了几种训练策略以提高稳定性。我们的方法支持任意模态对，并在包括多视图到3D形状生成、图像超分辨率和多视图场景合成在内的多种MT任务中表现出色。全面的实验和消融实验验证了我们框架的有效性，建立了通用模态翻译的新强基线。如需更多信息，请参见我们的项目页面：https://sites.google.com/view/lddbm/home.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of modality translation by proposing the Latent Denoising Diffusion Bridge Model (LDDBM), which operates in a shared latent space to translate information across different sensory modalities without requiring aligned dimensions. The method uses a contrastive alignment loss to enforce semantic consistency and a predictive loss to guide accurate cross-domain translation. Experiments show strong performance on various tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis, establishing a new strong baseline in general modality translation.</div>
<div class="mono" style="margin-top:8px">本文通过提出Latent Denoising Diffusion Bridge Model (LDDBM)解决了不同感官模态间信息翻译的挑战。该方法在共享的潜在空间中操作，以实现不同模态间的翻译，并使用对比对齐损失来确保语义一致性，以及预测损失来引导准确的跨域翻译。广泛的实验表明，LDDBM在多视图到3D形状生成、图像超分辨率和多视图场景合成等多种任务上表现出色，建立了通用模态翻译的新基准。</div>
</details>
</div>
<div class="card">
<div class="title">VAMOS: A Hierarchical Vision-Language-Action Model for   Capability-Modulated and Steerable Navigation</div>
<div class="meta-line">Authors: Mateo Guaman Castro, Sidharth Rajagopal, Daniel Gorbatov, Matt Schmittle, Rohan Baijal, Octi Zhang, Rosario Scalise, Sidharth Talia, Emma Romig, Celso de Melo, Byron Boots, Abhishek Gupta</div>
<div class="meta-line">First: 2025-10-23T17:59:45+00:00 · Latest: 2025-10-23T17:59:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20818v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20818v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vamos-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A fundamental challenge in robot navigation lies in learning policies that
generalize across diverse environments while conforming to the unique physical
constraints and capabilities of a specific embodiment (e.g., quadrupeds can
walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that
decouples semantic planning from embodiment grounding: a generalist planner
learns from diverse, open-world data, while a specialist affordance model
learns the robot&#x27;s physical constraints and capabilities in safe, low-cost
simulation. We enabled this separation by carefully designing an interface that
lets a high-level planner propose candidate paths directly in image space that
the affordance model then evaluates and re-ranks. Our real-world experiments
show that VAMOS achieves higher success rates in both indoor and complex
outdoor navigation than state-of-the-art model-based and end-to-end learning
methods. We also show that our hierarchical design enables cross-embodied
navigation across legged and wheeled robots and is easily steerable using
natural language. Real-world ablations confirm that the specialist model is key
to embodiment grounding, enabling a single high-level planner to be deployed
across physically distinct wheeled and legged robots. Finally, this model
significantly enhances single-robot reliability, achieving 3X higher success
rates by rejecting physically infeasible plans. Website:
https://vamos-vla.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VAMOS：一种分层的视觉-语言-行动模型，用于能力调节和可调节导航</div>
<div class="mono" style="margin-top:8px">机器人导航中的一个基本挑战在于学习能够在多种环境中泛化的策略，同时符合特定实体的独特物理约束和能力（例如，四足机器人可以爬楼梯，但轮式机器人不能）。我们提出了VAMOS，一种分层的视觉-语言-行动模型，将语义规划与实体接地分离：一个通用的规划者从多样化的开放世界数据中学习，而一个专门的手段模型在安全、低成本的模拟中学习机器人的物理约束和能力。我们通过精心设计一个接口实现了这种分离，该接口允许高层规划者直接在图像空间中提出候选路径，然后由手段模型评估和重新排名。我们的实地实验表明，VAMOS在室内和复杂室外导航中都比最先进的基于模型和端到端学习方法具有更高的成功率。我们还展示了这种分层设计能够在腿式和轮式机器人之间实现跨实体导航，并且可以通过自然语言轻松调节。实地消融实验证实，专门模型对于实体接地至关重要，使单一高层规划者能够在物理上不同的轮式和腿式机器人上部署。最后，该模型显著提高了单个机器人的可靠性，通过拒绝物理上不可行的计划，成功率提高了3倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VAMOS is a hierarchical vision-language-action model designed to navigate diverse environments while respecting the physical capabilities of the robot. It separates semantic planning from embodiment grounding, with a generalist planner learning from diverse data and a specialist model learning the robot&#x27;s physical constraints. Experiments show VAMOS outperforms state-of-the-art methods in both indoor and outdoor navigation, and its hierarchical design allows for cross-embodied navigation and easy steering with natural language. The specialist model is crucial for embodiment grounding, enabling a single planner to work across different types of robots and significantly improving single-robot reliability by rejecting infeasible plans.</div>
<div class="mono" style="margin-top:8px">VAMOS是一种分层的视觉-语言-行动模型，旨在在多样化的环境中导航并尊重机器人的物理能力。该模型将语义规划与实体接地分离，通用规划器从多样化的数据中学习，而专家模型则学习机器人的物理限制。实验表明，VAMOS在室内和复杂室外导航中均优于最先进的方法，并且其分层设计允许跨不同类型的腿足和轮式机器人进行导航，并且可以通过自然语言轻松调整。专家模型对于实体接地至关重要，使单个规划器能够在不同类型的机器人上部署，并显著提高了单个机器人的可靠性，通过拒绝不可行的计划实现了成功率提高3倍。</div>
</details>
</div>
<div class="card">
<div class="title">KL-Regularized Reinforcement Learning is Designed to Mode Collapse</div>
<div class="meta-line">Authors: Anthony GX-Chen, Jatin Prakash, Jeff Guo, Rob Fergus, Rajesh Ranganath</div>
<div class="meta-line">First: 2025-10-23T17:59:40+00:00 · Latest: 2025-10-23T17:59:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20817v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It is commonly believed that optimizing the reverse KL divergence results in
&quot;mode seeking&quot;, while optimizing forward KL results in &quot;mass covering&quot;, with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KL-正则化强化学习旨在模式塌陷</div>
<div class="mono" style="margin-top:8px">通常认为，优化逆KL散度会导致“模式寻求”，而优化正向KL则会导致“质量覆盖”，后者如果目标是从多个多样模式中采样则更受欢迎。我们通过数学和实验证明，这种直觉并不一定适用于使用逆/正向KL正则化（例如，语言模型中常用的方式）进行强化学习。相反，逆/正向KL的选择决定了最优目标分布的家族，参数化为正则化系数。模式覆盖主要取决于其他因素，如正则化强度，以及奖励和参考概率之间的相对比例。此外，我们证明了常用的设置，如低正则化强度和相等的可验证奖励，往往会指定单模目标分布，这意味着优化目标，从本质上讲，是非多样性的。我们利用这些见解构建了一个简单、可扩展且具有理论依据的算法。它对奖励幅度的修改最小，但优化了一个将高概率集中在所有高质量采样模式上的目标分布。在实验中，这种简单的修改能够提高大型语言模型和化学语言模型的解决方案质量和多样性，无需任何多样性的外部信号，并且在使用逆/正向KL时，无论是哪种情况都有效。</div>
</details>
</div>
<div class="card">
<div class="title">GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic   Manipulation</div>
<div class="meta-line">Authors: Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</div>
<div class="meta-line">First: 2025-10-23T17:59:26+00:00 · Latest: 2025-10-23T17:59:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20813v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20813v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://3dgsworld.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents GSWorld, a robust, photo-realistic simulator for robotics
manipulation that combines 3D Gaussian Splatting with physics engines. Our
framework advocates &quot;closing the loop&quot; of developing manipulation policies with
reproducible evaluation of policies learned from real-robot data and sim2real
policy training without using real robots. To enable photo-realistic rendering
of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian
Scene Description File), that infuses Gaussian-on-Mesh representation with
robot URDF and other objects. With a streamlined reconstruction pipeline, we
curate a database of GSDF that contains 3 robot embodiments for single-arm and
bimanual manipulation, as well as more than 40 objects. Combining GSDF with
physics engines, we demonstrate several immediate interesting applications: (1)
learning zero-shot sim2real pixel-to-action manipulation policy with
photo-realistic rendering, (2) automated high-quality DAgger data collection
for adapting policies to deployment environments, (3) reproducible benchmarking
of real-robot manipulation policies in simulation, (4) simulation data
collection by virtual teleoperation, and (5) zero-shot sim2real visual
reinforcement learning. Website: https://3dgsworld.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GSWorld：闭环照片级真实感模拟套件用于机器人操作</div>
<div class="mono" style="margin-top:8px">本文介绍了GSWorld，这是一种结合了3D高斯点积和物理引擎的稳健的照片级真实感机器人操作模拟器。我们的框架提倡“闭环”开发操作策略，通过可重复的评估从真实机器人数据中学习的操作策略，并在无需使用真实机器人的情况下进行模拟到现实的策略训练。为了实现多样场景的照片级真实感渲染，我们提出了一种新的资产格式，我们称之为GSDF（高斯场景描述文件），它将高斯-网格表示与机器人URDF和其他对象结合在一起。通过简化重建流水线，我们整理了一个包含3个单臂和双臂操作的机器人实体以及超过40个物体的GSDF数据库。结合物理引擎，我们展示了几个有趣的即时应用：（1）使用照片级真实感渲染学习零样本模拟到现实的像素到动作操作策略，（2）自动高质量的DAgger数据收集以适应部署环境，（3）在模拟中重复评估真实机器人操作策略，（4）通过虚拟遥控收集模拟数据，以及（5）零样本模拟到现实的视觉强化学习。网站：https://3dgsworld.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">GSWorld is a robust photo-realistic simulator for robotic manipulation that integrates 3D Gaussian Splatting with physics engines. It aims to close the loop in developing manipulation policies by using reproducible evaluation of real-robot data and sim2real policy training. Key findings include learning zero-shot sim2real pixel-to-action policies, automated high-quality data collection for policy adaptation, reproducible benchmarking of real-robot policies, simulation data collection by virtual teleoperation, and zero-shot sim2real visual reinforcement learning.</div>
<div class="mono" style="margin-top:8px">GSWorld 是一个结合 3D 高斯散点图与物理引擎的鲁棒照片级真实感机器人操作模拟器。它旨在通过使用来自真实机器人数据的可重复评估和 sim2real 策略训练来闭合开发操作策略的循环。关键实验发现包括学习零样本 sim2real 像素到动作操作策略、高质量的自动 DAgger 数据收集、真实机器人操作策略的可重复基准测试、通过虚拟遥操作收集模拟数据以及零样本 sim2real 视觉强化学习。</div>
</details>
</div>
<div class="card">
<div class="title">SpectraMorph: Structured Latent Learning for Self-Supervised   Hyperspectral Super-Resolution</div>
<div class="meta-line">Authors: Ritik Shah, Marco F Duarte</div>
<div class="meta-line">First: 2025-10-23T17:59:26+00:00 · Latest: 2025-10-23T17:59:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20814v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hyperspectral sensors capture dense spectra per pixel but suffer from low
spatial resolution, causing blurred boundaries and mixed-pixel effects.
Co-registered companion sensors such as multispectral, RGB, or panchromatic
cameras provide high-resolution spatial detail, motivating hyperspectral
super-resolution through the fusion of hyperspectral and multispectral images
(HSI-MSI). Existing deep learning based methods achieve strong performance but
rely on opaque regressors that lack interpretability and often fail when the
MSI has very few bands. We propose SpectraMorph, a physics-guided
self-supervised fusion framework with a structured latent space. Instead of
direct regression, SpectraMorph enforces an unmixing bottleneck: endmember
signatures are extracted from the low-resolution HSI, and a compact multilayer
perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed
by linear mixing, with training performed in a self-supervised manner via the
MSI sensor&#x27;s spectral response function. SpectraMorph produces interpretable
intermediates, trains in under a minute, and remains robust even with a
single-band (pan-chromatic) MSI. Experiments on synthetic and real-world
datasets show SpectraMorph consistently outperforming state-of-the-art
unsupervised/self-supervised baselines while remaining very competitive against
supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpectraMorph：结构化潜在学习的自监督超光谱超分辨率</div>
<div class="mono" style="margin-top:8px">超光谱传感器每像素捕获密集光谱，但存在低空间分辨率的问题，导致边界模糊和混合像素效应。配准的伴生传感器，如多光谱、RGB或全色相机，提供高分辨率的空间细节，推动了通过融合超光谱和多光谱图像（HSI-MSI）实现超光谱超分辨率。现有的基于深度学习的方法表现强大，但依赖于不透明的回归器，缺乏可解释性，且当MSI带宽很少时往往失效。我们提出了一种基于物理的自监督融合框架SpectraMorph，具有结构化的潜在空间。SpectraMorph 不直接进行回归，而是施加一个解混瓶颈：从低分辨率HSI中提取端元签名，并通过MSI预测类似丰度的图。光谱通过线性混合重建，通过MSI传感器的光谱响应函数在自监督模式下进行训练。SpectraMorph 生成可解释的中间结果，训练时间不到一分钟，并且即使在单带（全色）MSI的情况下也能保持鲁棒性。在合成和真实数据集上的实验表明，SpectraMorph 在自监督/半监督基准上始终优于最先进的方法，同时在监督基准上保持非常有竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpectraMorph is a self-supervised fusion framework for hyperspectral super-resolution that uses a structured latent space to enforce an unmixing bottleneck. It extracts endmember signatures from low-resolution hyperspectral images and predicts abundance-like maps from high-resolution multispectral images using a compact multilayer perceptron. SpectraMorph outperforms state-of-the-art unsupervised and self-supervised methods and remains robust even with a single-band multispectral image, while producing interpretable intermediates and training quickly.</div>
<div class="mono" style="margin-top:8px">SpectraMorph 是一种自监督融合框架，用于高光谱超分辨率，它使用结构化的潜在空间来强制执行解混瓶颈。该方法从低分辨率高光谱图像中提取端元签名，并从高分辨率多光谱图像中预测丰度图。SpectraMorph 在合成和真实世界数据集上的表现优于现有的一些无监督和自监督方法，同时即使使用单波段多光谱图像也能保持鲁棒性，并且训练速度快。</div>
</details>
</div>
<div class="card">
<div class="title">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via   Speculation</div>
<div class="meta-line">Authors: Yuhan Liu, Lianhui Qin, Shengjie Wang</div>
<div class="meta-line">First: 2025-10-23T17:59:21+00:00 · Latest: 2025-10-23T17:59:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20812v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20812v1">PDF</a> · <a href="https://github.com/Tinaliu0123/speculative-verdict">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>小草图，大裁决：基于推测的密集信息视觉推理</div>
<div class="mono" style="margin-top:8px">大型多模态视觉语言模型（VLMs）在多模态理解方面取得了显著进展，但在处理密集交织了文本注释和细粒度图形元素的信息密集型图像时，它们面临挑战。主要挑战在于在密集布局中精确定位关键线索以及进行多跳推理以整合分散的证据。我们提出了推测裁决（SV），这是一种无需训练的框架，灵感来源于推测解码，结合了多个轻量级草图专家和一个大型裁决模型。在草图阶段，小型VLM作为草图专家生成提供多样化定位候选的推理路径；在裁决阶段，强大的VLM综合这些路径生成最终答案，同时降低计算成本并恢复正确答案。为了进一步提高效率和准确性，SV引入了一种共识专家选择机制，仅将高一致性的推理路径转发到裁决阶段。实验证明，SV在InfographicVQA、ChartMuseum、ChartQAPro和HR-Bench 4K等具有挑战性的信息密集型和高分辨率视觉问答基准测试中取得了持续的改进。通过综合多个部分准确推理路径中的正确见解，SV在错误纠正和成本效率方面优于大型专有模型或训练管道。代码可在https://github.com/Tinaliu0123/speculative-verdict 获取</div>
</details>
</div>
<div class="card">
<div class="title">On the Detectability of LLM-Generated Text: What Exactly Is   LLM-Generated Text?</div>
<div class="meta-line">Authors: Mingmeng Geng, Thierry Poibeau</div>
<div class="meta-line">First: 2025-10-23T17:59:06+00:00 · Latest: 2025-10-23T17:59:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="http://arxiv.org/abs/2510.20810v1">Abs</a> · <a href="http://arxiv.org/pdf/2510.20810v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely &quot;LLM-generated
text&quot;. Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型生成文本的可检测性：什么是大语言模型生成的文本？</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLMs）的广泛应用，许多研究人员开始关注如何检测由它们生成的文本。然而，对于“LLM生成的文本”这一目标并没有一致或精确的定义。不同的应用场景和LLM的多样性进一步增加了检测的难度。通常认为的检测目标通常只代表LLM能够生成的文本的一部分。人类对LLM输出的编辑以及LLM对用户微妙的影响正在模糊LLM生成的文本与人类撰写的文本之间的界限。现有的基准测试和评估方法未能充分解决实际检测应用中的各种条件。因此，检测器的数值结果往往被误解，其重要性也在减弱。因此，检测器在特定条件下仍然有用，但其结果应仅作为参考而非决定性指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of detecting text generated by large language models (LLMs) due to the lack of a precise definition of &#x27;LLM-generated text&#x27;. It highlights that human edits and subtle influences from LLMs make it difficult to distinguish between LLM-generated and human-written text. The study finds that existing benchmarks and evaluation methods do not fully capture the complexity of real-world detection scenarios, leading to misinterpretation of detector results. Detectors are thus useful under specific conditions but should be interpreted cautiously as references rather than definitive indicators.</div>
<div class="mono" style="margin-top:8px">论文探讨了由于缺乏对‘LLM生成文本’的精确定义而难以检测由大型语言模型（LLMs）生成的文本的问题。研究指出，人类编辑和LLM的微妙影响使得区分LLM生成文本和人类撰写的文本变得困难。研究发现，现有的基准和评估方法未能充分捕捉到实际检测场景的复杂性，导致检测器结果的误解。因此，检测器在特定条件下是有用的，但其结果应谨慎解释为参考而非决定性指标。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251026_0328.html">20251026_0328</a>
<a href="archive/20251025_0320.html">20251025_0320</a>
<a href="archive/20251024_0328.html">20251024_0328</a>
<a href="archive/20251023_1235.html">20251023_1235</a>
<a href="archive/20251023_0316.html">20251023_0316</a>
<a href="archive/20251022_0319.html">20251022_0319</a>
<a href="archive/20251021_1916.html">20251021_1916</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
