<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-16 03:34</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260216_0334</div>
    <div class="row"><div class="card">
<div class="title">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</div>
<div class="meta-line">Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</div>
<div class="meta-line">First: 2026-02-12T18:59:59+00:00 · Latest: 2026-02-12T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12281v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &quot;intention-action gap.&#x27;&#x27; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &quot;boot-time compute&quot; and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展验证比扩展策略学习更能有效实现视觉-语言-行动对齐</div>
<div class="mono" style="margin-top:8px">通用机器人长期愿景依赖于它们理解和执行自然语言指令的能力。视觉-语言-行动（VLA）模型在这一目标上取得了显著进展，但它们生成的动作仍然可能与给定的指令不一致。在本文中，我们研究测试时验证作为缩小“意图-行动差距”的手段。我们首先表征了基于指令的执行的测试时扩展定律，证明了同时扩展重述指令的数量和生成动作的数量大大增加了测试时样本多样性，通常比独立扩展每个维度更有效地恢复正确的动作。为了利用这些扩展定律，我们提出了CoVer，一种视觉-语言-行动对齐的对比验证器，并展示了我们的架构随着额外计算资源和数据的增加而平滑扩展。然后，我们介绍了“启动时计算”和一个分层验证推理流水线，用于VLAs。在部署时，我们的框架从视觉-语言模型（VLM）预计算一组多样化的重述指令，反复为每条指令生成动作候选，然后使用验证器选择最优的高层提示和低层动作片段。与在相同数据上扩展策略预训练相比，我们的验证方法在SIMPLER基准测试中获得了22%的同分布改进和13%的异分布改进，在实际实验中进一步提高了45%。在PolaRiS基准测试中，CoVer实现了14%的任务进展和9%的成功率改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper explores test-time verification as a method to improve alignment between actions and natural language instructions in vision-language-action models. It demonstrates that jointly scaling the number of rephrased instructions and generated actions increases test-time sample diversity, leading to more efficient recovery of correct actions. The proposed CoVer architecture scales gracefully with additional resources, and the framework precomputes a diverse set of rephrased instructions, generating action candidates and using a verifier to select the optimal actions. Compared to scaling policy pre-training, the verification approach shows significant improvements, with 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, and further improvements in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</div>
<div class="mono" style="margin-top:8px">本文探讨了测试时验证作为提高视觉-语言-动作模型中动作与自然语言指令之间对齐的方法。研究表明，同时扩展重述指令的数量和生成的动作数量可以提高测试时样本多样性，更有效地恢复正确的动作。提出的CoVer架构在额外资源下能够平滑扩展，并通过层次验证推理管道预先计算多样化的重述指令，生成动作候选并选择最优的高级提示和低级动作片段。与扩展策略预训练相比，在SIMPLER基准上显示出显著的改进，包括22%的分布内改进和13%的分布外改进，以及在现实世界实验和PolaRiS基准上的进一步改进。</div>
</details>
</div>
<div class="card">
<div class="title">Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</div>
<div class="meta-line">Authors: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu</div>
<div class="meta-line">First: 2026-02-12T18:59:54+00:00 · Latest: 2026-02-12T18:59:54+00:00</div>
<div class="meta-line">Comments: Project page: https://stroke-of-surprise.github.io/ Code: https://github.com/stroke-of-surprise/Stroke-Of-Surprise</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12280v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12280v1">PDF</a> · <a href="https://github.com/stroke-of-surprise/Stroke-Of-Surprise">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://stroke-of-surprise.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &quot;dual-constraint&quot;: initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a &quot;common structural subspace&quot; valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>惊喜一击：渐进语义幻觉在矢量素描中的应用</div>
<div class="mono" style="margin-top:8px">视觉幻觉传统上依赖于空间操作，如多视角一致性。在本研究中，我们引入了渐进语义幻觉，这是一种新颖的矢量素描任务，其中单个素描通过逐步添加线条经历剧烈的语义转变。我们提出了Stroke of Surprise，这是一种生成框架，优化矢量线条以在不同的绘画阶段满足不同的语义解释。核心挑战在于“双重约束”：初始前缀线条必须形成一个连贯的对象（例如，一只鸭子），同时作为添加增量线条后第二个概念（例如，一只绵羊）的结构基础。为了解决这一问题，我们提出了一种基于双重分支Score Distillation Sampling (SDS)机制的序列感知联合优化框架。与顺序方法冻结初始状态不同，我们的方法动态调整前缀线条以发现适用于两个目标的“共同结构子空间”。此外，我们引入了一种新的Overlay Loss，以确保空间互补性，而不是遮挡，从而实现结构整合。大量实验表明，我们的方法在可识别性和幻觉强度方面显著优于最先进的基线方法，成功地将视觉异位词从空间维度扩展到时间维度。项目页面：https://stroke-of-surprise.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces Progressive Semantic Illusions in vector sketching, where a single sketch transforms dramatically through sequential strokes. The Stroke of Surprise framework optimizes strokes to maintain coherence and serve as the foundation for a second concept. The method addresses the dual-constraint by dynamically adjusting initial strokes and using a novel Overlay Loss to ensure structural integration. Experiments show significant improvements over existing methods in recognizability and illusion strength, expanding visual anagrams temporally.</div>
<div class="mono" style="margin-top:8px">该研究引入了矢量素描中的渐进语义幻象，通过顺序添加线条使单个素描在不同阶段发生剧烈的语义转变。Stroke of Surprise框架优化线条以在不同阶段满足不同的语义解释。它通过动态调整初始线条来同时满足初始和后续的概念。实验表明，该方法在可识别性和幻象强度方面优于现有技术，并将视觉异文从空间扩展到时间维度。</div>
</details>
</div>
<div class="card">
<div class="title">UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</div>
<div class="meta-line">Authors: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu</div>
<div class="meta-line">First: 2026-02-12T18:59:49+00:00 · Latest: 2026-02-12T18:59:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12279v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniT：统一多模态链式思维测试时扩展</div>
<div class="mono" style="margin-top:8px">统一模型可以在单一架构中处理多模态理解和生成，但通常它们在单次通过过程中运行，而不进行迭代细化输出。许多多模态任务，尤其是涉及复杂空间组合、多个相互作用的对象或不断变化的指令的任务，需要分解指令、验证中间结果并进行迭代修正。虽然测试时扩展（TTS）已经证明，为迭代推理分配额外的推理计算可以显著提高语言模型的性能，但将这一范式扩展到统一的多模态模型仍然是一个开放的挑战。我们引入了UniT，这是一种多模态链式思维测试时扩展的框架，使单一统一模型能够在多轮次中进行推理、验证和细化。UniT 结合了代理数据合成、统一模型训练和灵活的测试时推理，以引发包括验证、子目标分解和内容记忆在内的认知行为。我们的主要发现是：(1) 统一模型在短推理轨迹上的训练在测试时能够泛化到更长的推理链；(2) 顺序链式思维推理比并行采样提供了一种更具扩展性和计算效率的TTS策略；(3) 在生成和编辑轨迹上的训练提高了分布外视觉推理能力。这些结果确立了多模态测试时扩展作为一种有效范式，以推进统一模型中的生成和理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve the performance of unified models in handling complex multimodal tasks by enabling iterative reasoning. UniT, a framework for multimodal chain-of-thought test-time scaling, allows a single unified model to iteratively reason, verify, and refine its outputs. Key findings include the generalization of unified models to longer inference chains, the scalability and efficiency of sequential chain-of-thought reasoning, and the improvement in out-of-distribution visual reasoning through training on generation and editing trajectories.</div>
<div class="mono" style="margin-top:8px">论文提出了UniT框架，该框架使单一统一模型能够在多轮中迭代优化其输出。关键发现包括：统一模型在短推理轨迹上的训练能够在较长推理链中泛化良好，顺序链式思考推理比并行采样更具可扩展性和计算效率，以及在生成和编辑轨迹上的训练能够提高视觉推理的泛化能力。这项工作促进了统一模型中的生成和理解能力。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Test-Time Scaling for WebAgents</div>
<div class="meta-line">Authors: Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai, Michael W. Mahoney, Kurt Keutzer, Amir Gholami</div>
<div class="meta-line">First: 2026-02-12T18:58:30+00:00 · Latest: 2026-02-12T18:58:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12276v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent&#x27;s own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理测试时缩放以适应网络代理</div>
<div class="mono" style="margin-top:8px">测试时缩放已成为提高神经网络模型性能和增强其可靠性的标准方法。然而，它在代理型、多步骤任务上的行为尚不完全清楚：小的每步误差会在长时间范围内累积；我们发现，简单地均匀增加采样显示了递减的回报。在本文中，我们提出了CATTS，一种简单的技术，用于动态为多步骤代理分配计算资源。我们首先对网络代理的推理时缩放进行了实证研究。我们发现，均匀增加每步计算资源在长时间环境中很快达到饱和。然后我们研究了更强的聚合策略，包括基于LLM的仲裁者，它可以超越简单的投票，但可以推翻高一致性的决策。我们展示了代理自身投票分布得出的不确定性统计（熵和top-1/top-2差距）与后续成功相关，并提供了一种动态计算分配的实用信号。基于这些发现，我们引入了基于信心的测试时缩放(CATTS)，它仅在决策真正有争议时才使用投票得出的不确定性来分配计算资源。CATTS在WebArena-Lite和GoBrowse上的性能比React提高了最多9.1%，同时使用的令牌数量最多减少了2.3倍，提供了效率提升和可解释的决策规则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of test-time scaling for agentic, multi-step tasks, where small errors can accumulate over time. The authors introduce CATTS, a technique that dynamically allocates compute based on decision uncertainty. They find that uniformly increasing per-step compute quickly saturates, and propose an LLM-based Arbiter that can outperform naive voting but may overrule high-consensus decisions. CATTS uses vote-derived uncertainty to allocate compute only when decisions are contentious, improving performance on WebArena-Lite and GoBrowse by up to 9.1% with fewer tokens compared to uniform scaling.</div>
<div class="mono" style="margin-top:8px">该研究针对多步任务中累积误差的问题，提出了CATTS，一种基于决策不确定性的动态计算分配技术。实验证明，均匀增加每步计算很快会饱和，而CATTS通过利用投票得出的不确定性，能够在WebArena-Lite和GoBrowse上将性能提高最多9.1%，同时使用更少的令牌，相比均匀分配计算更有效率。</div>
</details>
</div>
<div class="card">
<div class="title">Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage</div>
<div class="meta-line">Authors: Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen</div>
<div class="meta-line">First: 2026-02-12T18:58:12+00:00 · Latest: 2026-02-12T18:58:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12274v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12274v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>功能空间解耦扩散在碳捕获与封存正向和逆向建模中的应用</div>
<div class="mono" style="margin-top:8px">准确表征地下流场对于碳捕获与封存（CCS）至关重要，但稀疏观测导致的逆问题病态性仍是一个挑战。我们提出了一种生成框架Fun-DDPS，该框架结合了功能空间扩散模型和可微神经算子代理，用于正向和逆向建模。我们的方法使用单通道扩散模型学习地质参数（地质模型）的先验分布，然后利用局部神经算子（LNO）代理为跨场条件提供符合物理的指导。这种解耦允许扩散先验在参数空间中稳健地恢复缺失信息，而代理则提供基于梯度的数据同化高效指导。我们在合成的CCS建模数据集上展示了Fun-DDPS，取得了两个关键结果：（1）在只有25%观测的情况下进行正向建模，Fun-DDPS的相对误差为7.7%，而标准代理的相对误差为86.9%（提高了11倍），证明了其在极端数据稀疏性条件下处理问题的能力，此时确定性方法失效。（2）我们首次对基于扩散的逆解算器进行了与渐近精确拒绝采样（RS）后验的严格验证。Fun-DDPS和联合状态基线（Fun-DPS）的 Jensen-Shannon 散度均小于0.06，与真实值一致。关键的是，Fun-DDPS 生成了物理上一致的实现，没有联合状态基线中观察到的高频伪影，且样本效率提高了4倍，优于拒绝采样。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Fun-DDPS is a generative framework combining function-space diffusion models and differentiable neural operators for forward and inverse modeling in CCS. It learns a prior distribution over geological parameters and uses a Local Neural Operator surrogate for efficient gradient-based guidance. Fun-DDPS demonstrates significant improvements in handling extreme data sparsity, achieving 7.7% relative error compared to 86.9% for standard surrogates, and provides physically consistent realizations with 4x improved sample efficiency.</div>
<div class="mono" style="margin-top:8px">研究旨在通过前向和反向建模解决碳捕获与封存（CCS）中对地下流动准确表征的挑战。Fun-DDPS 生成框架结合了函数空间扩散模型和可微神经算子来处理稀疏观测。关键实验结果包括在只有25%观测数据的情况下，前向建模的相对误差为7.7%，而标准替代方案为86.9%，并且反向建模的詹森-沙恩 divergence 小于0.06，展示了更高的样本效率和物理一致性。</div>
</details>
</div>
<div class="card">
<div class="title">MonarchRT: Efficient Attention for Real-Time Video Generation</div>
<div class="meta-line">Authors: Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen</div>
<div class="meta-line">First: 2026-02-12T18:56:53+00:00 · Latest: 2026-02-12T18:56:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12271v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12271v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MonarchRT：实时视频生成的高效注意力机制</div>
<div class="mono" style="margin-top:8px">实时视频生成中的扩散变换器受到三维自注意力的二次成本限制，尤其是在既少步又自回归的实时环境中，错误会随时间累积，每个去噪步骤必须携带更多的信息。在这种环境中，我们发现先前的稀疏注意力近似失效，尽管在双向、多步扩散中表现出色。具体来说，我们观察到视频注意力不是可靠的稀疏结构，而是由时空位置驱动的显著周期性结构与动态稀疏语义对应关系和密集混合相结合，超过了甚至先验top-k注意力的表示能力。基于这一洞察，我们提出了Monarch-RT，这是一种用于视频扩散模型的结构化注意力参数化方法，通过Monarch矩阵分解注意力。通过适当的块对齐结构和我们扩展的Tiled Monarch参数化，我们实现了高表达性同时保持计算效率。我们进一步通过微调克服了参数化的开销，使用了自定义的Triton内核。我们首先验证了Monarch-RT在现有仅针对双向模型设计的稀疏基线中的高有效性。我们还观察到，当应用于最先进的模型Self-Forcing时，Monarch-RT可以达到95%的注意力稀疏度，且不损失质量，使Monarch-RT成为实时视频生成中高能力稀疏注意力参数化的开创性工作。我们的优化实现分别在Nvidia RTX 5090、H100和B200 GPU上优于FlashAttention-2、FlashAttention-3和FlashAttention-4内核，提供了1.4-11.8倍的内核加速。这使我们首次能够在单个RTX 5090上以16 FPS实现真正的实时视频生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of real-time video generation with Diffusion Transformers, which are hindered by the quadratic cost of 3D self-attention. It proposes Monarch-RT, a structured attention parameterization that uses Monarch matrices to achieve high expressivity while maintaining computational efficiency. Monarch-RT outperforms existing sparse baselines and achieves up to 95% attention sparsity with no quality loss when applied to Self-Forcing, enabling true real-time video generation at 16 FPS on a single RTX 5090 GPU, with speedups ranging from 1.4X to 11.8X compared to FlashAttention kernels.</div>
<div class="mono" style="margin-top:8px">论文针对Diffusion Transformers在实时视频生成中的瓶颈问题，即3D自注意力的二次成本，提出了一种结构化注意力参数化方法Monarch-RT，利用Monarch矩阵实现高效表达同时保持计算效率。Monarch-RT在应用于Self-Forcing时可实现高达95%的注意力稀疏性且不损失质量，首次在单个RTX 5090 GPU上实现了真正的实时视频生成，每秒16帧，相比FlashAttention内核速度提升1.4到11.8倍。</div>
</details>
</div>
<div class="card">
<div class="title">CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use</div>
<div class="meta-line">Authors: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang</div>
<div class="meta-line">First: 2026-02-12T18:55:09+00:00 · Latest: 2026-02-12T18:55:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12268v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12268v1">PDF</a> · <a href="https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn&#x27;s intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CM2：使用检查表奖励的多轮多步代理工具使用强化学习</div>
<div class="mono" style="margin-top:8px">AI代理越来越多地通过推理多轮用户交互和调用外部工具来解决实际任务。然而，在这种设置中应用强化学习仍然很困难：现实目标往往缺乏可验证的奖励，而是强调开放性行为；此外，多轮、多步代理工具使用仍然未被充分探索；而且构建和维护可执行的工具环境成本高昂，限制了规模和覆盖面。我们提出了CM2，一种使用检查表奖励代替可验证结果奖励的强化学习框架。CM2将每轮预期行为细分为具有明确证据基础和结构化元数据的细粒度二元标准，将开放性判断转化为更稳定的分类决策。为了平衡稳定性和信息量，我们的方法采用稀疏奖励分配但密集评估标准的策略。训练在可扩展的大语言模型模拟工具环境中进行，避免了为大量工具集进行繁重的工程工作。实验表明，CM2在tau^-Bench上比监督微调提高了8个点，在BFCL-V4上提高了10个点，在ToolSandbox上提高了12个点。结果与或甚至超过了同样规模的开源基线，包括判断模型。因此，CM2提供了一种无需依赖可验证奖励来优化多轮多步工具使用代理的可扩展方法。开源社区提供的代码：https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes CM2, an RL framework that uses checklist rewards to address the challenges of applying reinforcement learning to multi-turn and multi-step agentic tool use. By decomposing each turn&#x27;s behavior into binary criteria with explicit evidence, CM2 transforms open-ended judgments into more stable classification decisions. Experiments show that CM2 outperforms supervised fine-tuning on multiple benchmarks, improving by 8 points on tau^-Bench, 10 points on BFCL-V4, and 12 points on ToolSandbox, demonstrating its effectiveness in optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards.</div>
<div class="mono" style="margin-top:8px">论文提出了CM2，一种使用清单奖励的RL框架，以解决将强化学习应用于多轮和多步骤代理工具使用时的挑战。通过将每轮行为分解为具有明确证据的二元标准，CM2将开放性判断转换为更稳定的分类决策。实验表明，CM2在多个基准测试上优于监督微调，分别在tau^-Bench上提高8分，在BFCL-V4上提高10分，在ToolSandbox上提高12分，展示了其在优化多轮多步骤工具使用代理时的有效性，无需依赖可验证的奖励。</div>
</details>
</div>
<div class="card">
<div class="title">T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</div>
<div class="meta-line">Authors: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas</div>
<div class="meta-line">First: 2026-02-12T18:52:35+00:00 · Latest: 2026-02-12T18:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12262v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12262v1">PDF</a> · <a href="https://github.com/Tyrion58/T3D">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model&#x27;s own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T3D：通过轨迹自蒸馏与直接判别优化的少量步骤扩散语言模型</div>
<div class="mono" style="margin-top:8px">扩散大型语言模型（DLLMs）有可能通过并行解码多个标记来实现快速文本生成。然而，在实践中，它们的推理效率受到需要许多细化步骤的限制，而大幅减少步骤会导致生成质量显著下降。为了解决这个问题，我们提出了一种轨迹自蒸馏框架，通过蒸馏模型自身的生成轨迹来改进少量步骤解码。我们结合了直接判别优化（DDO），这是一种反向KL目标，促进模式寻求蒸馏，并鼓励学生集中于高概率教师模式。在各种基准测试中，我们的方法在紧缩步骤预算下始终优于强大的少量步骤基线和标准训练。尽管全步骤解码仍然更优，但我们显著缩小了差距，为实用的少量步骤DLLMs奠定了坚实的基础。源代码可在https://github.com/Tyrion58/T3D获取。</div>
</details>
</div>
<div class="card">
<div class="title">Think like a Scientist: Physics-guided LLM Agent for Equation Discovery</div>
<div class="meta-line">Authors: Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama, Rose Yu</div>
<div class="meta-line">First: 2026-02-12T18:49:27+00:00 · Latest: 2026-02-12T18:49:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像科学家一样思考：基于物理的LLM代理进行方程发现</div>
<div class="mono" style="margin-top:8px">通过符号、可解释的公式来解释观察到的现象是科学的基本目标。近年来，大型语言模型（LLMs）因其广泛的领域知识和强大的推理能力，已成为符号方程发现的有前途的工具。然而，现有的大多数基于LLM的系统试图直接从数据中猜测方程，而不建模科学家通常遵循的多步推理过程：首先推断诸如对称性等物理属性，然后使用这些属性作为先验来限制候选方程的空间。我们引入了开普勒代理，这是一种明确遵循这一科学推理过程的代理框架。该代理协调基于物理的工具来提取中间结构，并使用这些结果来配置符号回归引擎，如PySINDy和PySR，包括它们的功能库和结构约束。在一系列物理方程基准测试中，开普勒代理在符号准确性上显著高于LLM和传统基线，并且对噪声数据具有更高的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the ability of large language models (LLMs) to discover symbolic equations by mimicking the scientific reasoning process. KeplerAgent, an agent-based framework, explicitly models this process by first inferring physical properties and then using these as priors to restrict the space of candidate equations. This approach leads to higher symbolic accuracy and better robustness to noisy data compared to both LLMs and traditional methods across various physical equation benchmarks.</div>
<div class="mono" style="margin-top:8px">研究旨在通过模拟科学家的推理过程来提升符号方程的发现。KeplerAgent框架首先推断物理属性如对称性，并将其作为先验知识来限制候选方程的空间。这种方法在各种物理方程基准测试中比现有的LLM和传统方法具有更高的符号准确性和更好的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Privacy Risks in Time Series Forecasting: User- and Record-Level Membership Inference</div>
<div class="meta-line">Authors: Nicolas Johansson, Tobias Olsson, Daniel Nilsson, Johan Östman, Fazeleh Hoseini</div>
<div class="meta-line">First: 2025-09-04T12:43:45+00:00 · Latest: 2026-02-12T18:46:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04169v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04169v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Membership inference attacks (MIAs) aim to determine whether specific data were used to train a model. While extensively studied on classification models, their impact on time series forecasting remains largely unexplored. We address this gap by introducing two new attacks: (i) an adaptation of multivariate LiRA, a state-of-the-art MIA originally developed for classification models, to the time-series forecasting setting, and (ii) a novel end-to-end learning approach called Deep Time Series (DTS) attack. We benchmark these methods against adapted versions of other leading attacks from the classification setting.
  We evaluate all attacks in realistic settings on the TUH-EEG and ELD datasets, targeting two strong forecasting architectures, LSTM and the state-of-the-art N-HiTS, under both record- and user-level threat models. Our results show that forecasting models are vulnerable, with user-level attacks often achieving perfect detection. The proposed methods achieve the strongest performance in several settings, establishing new baselines for privacy risk assessment in time series forecasting. Furthermore, vulnerability increases with longer prediction horizons and smaller training populations, echoing trends observed in large language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间序列预测中的隐私风险：用户级和记录级成员推断</div>
<div class="mono" style="margin-top:8px">成员推断攻击（MIAs）旨在确定特定数据是否被用于训练模型。尽管在分类模型上进行了广泛研究，但它们对时间序列预测的影响仍很大程度上未被探索。我们通过引入两种新的攻击方法来填补这一空白：(i) 将当前最先进的分类模型MIA方法多变量LiRA进行适应，应用于时间序列预测设置；(ii) 提出一种新的端到端学习方法，称为深度时间序列（DTS）攻击。我们使用来自分类设置的其他领先攻击的适应版本对这些方法进行了基准测试。
我们在现实环境中对TUH-EEG和ELD数据集上的所有攻击进行了评估，针对两种强大的预测架构LSTM和最先进的N-HiTS，在用户级和记录级威胁模型下进行。我们的结果表明，预测模型存在漏洞，用户级攻击往往能够实现完美的检测。所提出的方法在多种情况下表现出最强的性能，为时间序列预测中的隐私风险评估建立了新的基准。此外，预测时间越长，训练样本越少，漏洞越严重，这与大型语言模型中观察到的趋势一致。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates privacy risks in time series forecasting through membership inference attacks (MIAs). It introduces two new attacks: an adapted multivariate LiRA method and a novel Deep Time Series (DTS) attack. Evaluations on the TUH-EEG and ELD datasets show that forecasting models are vulnerable to these attacks, with user-level attacks often achieving perfect detection. The study highlights that vulnerability increases with longer prediction horizons and smaller training populations.</div>
<div class="mono" style="margin-top:8px">该研究通过成员推理攻击探讨时间序列预测模型中的隐私风险，引入了两种新攻击方法：多变量LiRA方法的适应版本和新型Deep Time Series (DTS) 攻击。在TUH-EEG和ELD数据集上的评估显示，这些攻击方法能够有效检测出模型是否使用了特定数据，尤其是在用户级别，检测率可达完美。研究还指出，预测时间越长和训练数据越少时，模型的脆弱性越高。</div>
</details>
</div>
<div class="card">
<div class="title">On the implicit regularization of Langevin dynamics with projected noise</div>
<div class="meta-line">Authors: Govind Menon, Austin J. Stromme, Adrien Vacher</div>
<div class="meta-line">First: 2026-02-12T18:45:42+00:00 · Latest: 2026-02-12T18:45:42+00:00</div>
<div class="meta-line">Comments: 30 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12257v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12257v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于投影噪声下朗之万动力学的隐式正则化</div>
<div class="mono" style="margin-top:8px">我们研究了噪声投影到等距群作用方向正交方向上的朗之万动力学。这种数学模型被引入以揭示对称性对过参数化模型的随机梯度下降法的影响。我们的主要结果识别了一种新的隐式正则化形式：当初始密度和目标密度都对群作用不变时，投影噪声下的朗之万动力学在分布上等价于具有额外与群轨道负对数体积成正比的附加漂移项的各向同性扩散下的朗之万动力学。我们通过构造两个过程在群上的第三个过程的耦合来证明这一结果，并将附加漂移识别为轨道的平均曲率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates Langevin dynamics with projected noise to understand the impact of symmetry on stochastic gradient descent in over-parametrized models. The key finding is that when both the initial and target densities are invariant under a group action, the dynamics with projected noise are equivalent in law to isotropic diffusion dynamics with an additional drift term. This drift is proportional to the negative log volume of the group orbit, indicating a form of implicit regularization. The proof involves constructing a coupling of the two processes through a third process on the group itself, and identifying the drift as the mean curvature of the orbits.</div>
<div class="mono" style="margin-top:8px">研究探讨了投影噪声下的 Langevin 动力学，以理解对称性对过参数化模型中随机梯度下降的影响。主要发现是，当初始和目标密度都对群作用不变时，投影噪声下的动力学与具有额外线性漂移项的等向扩散动力学等价，该漂移项与群轨道的负体积对数成正比，并可解释为轨道的平均曲率。</div>
</details>
</div>
<div class="card">
<div class="title">EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph</div>
<div class="meta-line">Authors: Nan Jiang, Ziyi Wang, Yexiang Xue</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-11-08T04:39:11+00:00 · Latest: 2026-02-12T18:38:11+00:00</div>
<div class="meta-line">Comments: Camera-ready version accepted for ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05849v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05849v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nan-jiang-group.github.io/egg-sr">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the search space and accelerating training lies in *symbolic equivalence*: many expressions, although syntactically different, define the same function -- for example, $\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, and $2\log(x_1)+3\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates symbolic equivalence into a class of modern symbolic regression methods, including Monte Carlo Tree Search (MCTS), Deep Reinforcement Learning (DRL), and Large Language Models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module (via equality graphs), accelerating learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalent generated sequences in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Theoretically, we show the benefit of embedding EGG into learning: it tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances a class of symbolic regression models across several benchmarks, discovering more accurate expressions within the same time limit. Project page is at: https://nan-jiang-group.github.io/egg-sr.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">EGG-SR is a framework that integrates symbolic equivalence into symbolic regression methods such as Monte Carlo Tree Search, Deep Reinforcement Learning, and Large Language Models to reduce redundant exploration and accelerate learning. Theoretical analysis shows that embedding EGG into learning tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR enhances the performance of symbolic regression models across various benchmarks, discovering more accurate expressions within the same time limit.</div>
<div class="mono" style="margin-top:8px">EGG-SR 是一个框架，将符号等价性整合到符号回归方法中，如 MCTS、DRL 和 LLM，以减少冗余探索并加速学习。它通过等价图模块表示等价表达式，帮助在 MCTS 中修剪冗余子树，在 DRL 中聚合奖励，并在 LLM 中丰富反馈提示。实验表明，EGG-SR 在各种基准测试中提高了在相同时间限制内发现的表达式的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Community Concealment from Unsupervised Graph Learning-Based Clustering</div>
<div class="meta-line">Authors: Dalyapraz Manatova, Pablo Moriano, L. Jean Camp</div>
<div class="meta-line">First: 2026-02-12T18:36:19+00:00 · Latest: 2026-02-12T18:36:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12250v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于无监督图学习聚类的社区隐藏</div>
<div class="mono" style="margin-top:8px">图神经网络（GNNs）旨在使用带属性的图来学习表示。这些表示在无监督聚类和社区检测中非常有益。然而，这种推断可能会揭示敏感群体、聚类系统或集体行为，从而引发关于群体级隐私的担忧。例如，在社会和关键基础设施网络中的社区归属可能会暴露协调的资产组、操作层次结构和系统依赖性，这些信息可用于进行画像或情报收集。我们研究了一个防御场景，在这种场景中，数据发布者（防御者）试图隐藏一个感兴趣的社区，同时在有限的、具有实用性的网络更改中做出改变。我们的分析表明，社区隐藏受到两个可量化因素的强烈影响：社区边界的连接性和保护社区与相邻社区之间的特征相似性。根据这些发现，我们提出了一种重新布线选定边和修改节点特征的扰动策略，以减少GNN消息传递中利用的独特性。在我们的实验中，该方法在合成基准和真实网络图下的扰动预算相同的情况下，优于DICE。总体而言，它在评估的设置中实现了约20-45%的中位相对隐藏改进。这些发现表明了一种针对基于GNN的社区学习的缓解策略，并突显了图学习中固有的群体级隐私风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of protecting community privacy in attributed graphs using graph neural networks (GNNs). It identifies two key factors affecting community concealment: boundary connectivity and feature similarity. The study proposes a perturbation strategy that restructures selected edges and modifies node features to reduce GNN-based community distinctiveness. Experiments show that this method outperforms DICE, achieving median relative concealment improvements of 20-45% across various settings.</div>
<div class="mono" style="margin-top:8px">研究探讨了使用图神经网络（GNN）进行无监督聚类时如何隐藏社区的问题。研究发现，边界连接性和特征相似性是影响社区隐藏的关键因素。研究人员提出了一种重新配置选定边和修改节点特征的策略，以减少社区的独特性，实验结果表明，该方法在合成和真实网络基准上的隐藏效果比DICE提高了约20-45%。</div>
</details>
</div>
<div class="card">
<div class="title">&quot;Sorry, I Didn&#x27;t Catch That&quot;: How Speech Models Miss What Matters Most</div>
<div class="meta-line">Authors: Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou</div>
<div class="meta-line">First: 2026-02-12T18:36:09+00:00 · Latest: 2026-02-12T18:36:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12249v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>&quot;对不起，我没有听清楚那句话&quot;: 为何语音模型忽视了最重要的内容</div>
<div class="mono" style="margin-top:8px">尽管语音识别系统在标准基准上的单词错误率很低，但在实际部署中，它们往往在短且高风险的口头表达上失败。在这里，我们研究了这种失败模式在一项高风险任务中的表现：美国参与者说出的美国街道名称的转录。我们评估了来自OpenAI、Deepgram、Google和Microsoft的15个模型在来自语言多样化的美国发言者的录音上的表现，并发现平均转录错误率为44%。我们通过地理区域量化了转录失败的下游影响，并表明错误转录系统性地影响了所有发言者，但非英语母语发言者的路由距离错误是英语母语发言者的两倍。为了减轻这种伤害，我们引入了一种合成数据生成方法，使用开源文本转语音模型生成命名实体的多样化发音。使用不到1000个合成样本进行微调后，非英语母语发言者的街道名称转录准确性提高了近60%（相对于基线模型）。我们的结果突显了语音系统基准性能与实际可靠性之间的关键差距，并展示了减少高风险转录错误的一个简单且可扩展的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the failure of speech recognition systems on short, high-stakes utterances, focusing on the transcription of U.S. street names. Evaluating 15 models from various providers, the research found an average error rate of 44%. It also identified that mis-transcriptions significantly impact routing distances, with non-English primary speakers experiencing twice the error rate compared to English primary speakers. By generating synthetic data with diverse pronunciations and fine-tuning models, the study achieved a 60% improvement in transcription accuracy for non-English primary speakers, highlighting the need for better real-world reliability in speech systems.</div>
<div class="mono" style="margin-top:8px">研究探讨了语音识别系统在处理短时高风险语音片段时的失败情况，重点关注美国街道名称的转录。评估了来自不同提供商的15个模型，发现平均错误率为44%。研究还发现，错误转录显著影响路线距离，非英语母语说话者的错误率是英语母语说话者的两倍。通过生成具有多样发音的合成数据并微调模型，研究实现了非英语母语说话者转录准确性近60%的提升，突显了语音系统在实际应用中需要更好的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction</div>
<div class="meta-line">Authors: Nick Ferguson, Josh Pennington, Narek Beghian, Aravind Mohan, Douwe Kiela, Sheshansh Agrawal, Thien Hang Nguyen</div>
<div class="meta-line">First: 2026-02-12T18:31:37+00:00 · Latest: 2026-02-12T18:31:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12247v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12247v1">PDF</a> · <a href="https://github.com/ContextualAI/extract-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExtractBench：复杂结构化提取的基准和评估方法</div>
<div class="mono" style="margin-top:8px">非结构化文档如PDF包含有价值的结构化信息，但下游系统需要这些数据以可靠且标准化的形式存在。随着LLM被越来越多地部署以自动化此提取过程，准确性和可靠性变得至关重要。然而，进展受到两个瓶颈的阻碍。首先，没有端到端的基准可以评估大规模企业级模式下的PDF到JSON提取。其次，没有系统的方法来捕捉嵌套提取的语义，其中字段需要不同的正确性概念（标识符的精确匹配，数量的容忍度，名称的语义等价），数组需要对齐，遗漏必须与幻觉区分开。我们通过ExtractBench解决了这两个问题，这是一个开源的PDF到JSON结构化提取基准和评估框架。基准测试包括35份PDF文档与JSON模式和人工标注的金标准标签，覆盖了经济上重要的领域，产生了12,867个可评估的字段，涵盖了从几十到几百个字段的模式复杂性。评估框架将模式视为可执行的规范：每个字段声明其评分标准。基线评估显示，前沿模型（GPT-5/5.2，Gemini-3 Flash/Pro，Claude 4.5 Opus/Sonnet）在现实模式下仍然不可靠。随着模式复杂性的增加，性能急剧下降，所有测试模型在包含369个字段的财务报告模式下均无有效输出。我们已在https://github.com/ContextualAI/extract-bench/发布了ExtractBench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ExtractBench is a benchmark and evaluation framework for PDF-to-JSON structured extraction, addressing the lack of an end-to-end benchmark and principled methodology for nested extraction. It includes 35 PDF documents with human-annotated gold labels and JSON schemas, covering 12,867 fields across various domains. Evaluations show that leading models like GPT-5/5.2, Gemini-3 Flash/Pro, and Claude 4.5 Opus/Sonnet struggle with realistic schemas, with performance dropping sharply as schema complexity increases.</div>
<div class="mono" style="margin-top:8px">ExtractBench 是一个开源基准和评估框架，用于 PDF 到 JSON 的结构化提取，解决了缺乏端到端基准和规范方法的问题。它包含 35 个 PDF 文档、JSON 架构和人工标注的黄金标签，覆盖了 12,867 个字段，涉及多个领域。评估结果显示，最先进的模型在现实架构上表现不佳，随着架构复杂性的增加，性能急剧下降，最终在包含 369 个字段的财务报告架构上没有任何有效的输出。</div>
</details>
</div>
<div class="card">
<div class="title">Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces</div>
<div class="meta-line">Authors: Anthony Kobanda, Waris Radji</div>
<div class="meta-line">First: 2026-02-12T18:30:27+00:00 · Latest: 2026-02-12T18:30:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12245v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内在能量联合嵌入预测架构诱导拟度量空间</div>
<div class="mono" style="margin-top:8px">联合嵌入预测架构（JEPAs）旨在通过从上下文嵌入预测目标嵌入来学习表示，在潜在空间中诱导标量兼容能量。相比之下，拟度量强化学习（QRL）研究通过支持在非对称动力学下达到目标的方向距离值来进行目标导向控制。在本文中，我们通过限制关注JEPA能量函数的规范类：定义为两个状态之间可接受轨迹上累积局部努力的下确界的形式内在（最小作用）能量，来连接这些观点。在目标到达控制中，最优的成本到去函数恰好具有这种内在形式；相反，训练JEPAs来模拟内在能量的模型位于QRL所针对的拟度量价值类中。此外，我们观察到为什么对称的有限能量在方向性问题上结构上不匹配，当方向性很重要时，应使用非对称（拟度量）能量。</div>
</details>
</div>
<div class="card">
<div class="title">Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</div>
<div class="meta-line">Authors: Manjunath Kudlur, Evan King, James Wang, Pete Warden</div>
<div class="meta-line">First: 2026-02-12T18:20:45+00:00 · Latest: 2026-02-12T18:20:45+00:00</div>
<div class="meta-line">Comments: 7 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12241v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, which resolves otherwise locally ambiguous acoustics using distant lexical context. However, this global dependency incurs quadratic complexity in sequence length, inducing an inherent &quot;encode-the-whole-utterance&quot; latency profile. For streaming use cases, this causes TTFT to grow linearly with utterance length as the encoder must process the entire prefix before any decoder token can be emitted. To better meet the needs of on-device, streaming ASR use cases we introduce Moonshine v2, an ergodic streaming-encoder ASR model that employs sliding-window self-attention to achieve bounded, low-latency inference while preserving strong local context. Our models achieve state of the art word error rates across standard benchmarks, attaining accuracy on-par with models 6x their size while running significantly faster. These results demonstrate that carefully designed local attention is competitive with the accuracy of full attention at a fraction of the size and latency cost, opening new possibilities for interactive speech interfaces on edge devices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>月光shine v2：遍历流式编码器ASR及其在临界延迟语音应用中的应用</div>
<div class="mono" style="margin-top:8px">临界延迟语音应用（例如，实时转录、语音命令和实时翻译）需要低首个词时间（TTFT）和高转录准确性，特别是在资源受限的边缘设备上。全注意Transformer编码器仍然是自动语音识别（ASR）的强准确性基准，因为每一帧可以直接关注每一帧，这可以使用远程词汇上下文解决原本局部模糊的声学。然而，这种全局依赖性导致序列长度上的二次复杂性，产生固有的“编码整个语音片段”的延迟模式。对于流式使用场景，这会导致TTFT随着语音片段长度线性增长，因为编码器必须处理整个前缀，才能发出任何解码器词元。为了更好地满足边缘设备上流式ASR使用场景的需求，我们引入了月光shine v2，这是一种遍历流式编码器ASR模型，采用滑动窗口自注意力机制，以实现有界、低延迟推理，同时保留强大的局部上下文。我们的模型在标准基准测试中达到了最先进的字错误率，其准确性与大小为其6倍的模型相当，但运行速度显著更快。这些结果表明，精心设计的局部注意力在大小和延迟成本仅为全注意力的一小部分的情况下，与全注意力的准确性相当，为边缘设备上的交互式语音界面开辟了新的可能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Moonshine v2 is designed to address the latency challenges in streaming ASR for resource-constrained devices by using sliding-window self-attention, which allows for bounded inference time while maintaining strong local context. The model achieves state-of-the-art word error rates on standard benchmarks, with accuracy comparable to models six times larger but running much faster, demonstrating the effectiveness of local attention in reducing latency without significant loss of accuracy.</div>
<div class="mono" style="margin-top:8px">Moonshine v2 通过引入一种新的流式编码器 ASR 模型来解决实时语音应用中的延迟问题，该模型使用滑动窗口自注意力机制以保持低且可控制的延迟同时保留强大的局部上下文。该模型在标准基准测试中达到了最先进的字错误率，其准确度与大得多的模型相当，但运行速度要快得多。</div>
</details>
</div>
<div class="card">
<div class="title">Hyperparameter Transfer with Mixture-of-Expert Layers</div>
<div class="meta-line">Authors: Tianze Jiang, Blake Bordelon, Cengiz Pehlevan, Boris Hanin</div>
<div class="meta-line">First: 2026-01-28T03:02:30+00:00 · Latest: 2026-02-12T18:19:47+00:00</div>
<div class="meta-line">Comments: 25 Pages, 18 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20205v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20205v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合专家层中的超参数转移</div>
<div class="mono" style="margin-top:8px">混合专家（MoE）层已成为通过解耦总可训练参数与每个令牌前向传递中激活参数来扩大现代神经网络的重要工具。然而，稀疏MoE由于(i) 新的可训练参数（路由器权重），这些参数与其他所有参数组一样需要超参数（HP）调优；(ii) 新的架构规模维度（专家的数量和大小）必须选择并可能变得很大，增加了训练的复杂性。为了使HP选择既便宜又可靠，我们提出了一种新的参数化方法，用于具有MoE层的变压器模型，以扩展模型宽度、深度、专家数量和专家（隐藏）大小。我们的参数化方法通过新颖的动力学均场理论（DMFT）分析得到了证明。当我们以固定令牌预算变化不同的模型维度时，我们发现经验上，我们的参数化方法能够在从51M到超过2B总参数的模型之间实现可靠的HP转移。我们进一步将从小模型上短令牌窗口扫掠中识别出的HP应用到大模型上长令牌窗口的训练中，并报告了表现良好的模型行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of hyperparameter tuning for Mixture-of-Experts (MoE) layers in transformer models, which introduce new trainable parameters and architectural dimensions. A new parameterization is proposed based on a dynamical mean-field theory analysis to facilitate reliable hyperparameter transfer across different model sizes. Experiments show that this parameterization enables consistent performance when scaling from 51M to over 2B parameters, allowing hyperparameters optimized on smaller models to be effectively applied to larger models.</div>
<div class="mono" style="margin-top:8px">论文针对Mixture-of-Experts (MoE)层在变压器模型中的超参数调优挑战，提出了一个新的基于动态均场理论（DMFT）分析的参数化方法，以促进不同模型维度下的超参数可靠转移。实验结果显示，该参数化方法在从51M到超过2B参数的模型扩展过程中，能够实现一致的超参数性能，从而实现高效的模型训练。</div>
</details>
</div>
<div class="card">
<div class="title">Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation</div>
<div class="meta-line">Authors: Zhen Han, Mattias Teye, Derek Yadgaroff, Judith Bütepage</div>
<div class="meta-line">Venue: SIGGRAPH</div>
<div class="meta-line">First: 2025-07-24T12:25:12+00:00 · Latest: 2026-02-12T18:17:00+00:00</div>
<div class="meta-line">Comments: Accepted to ACM TOG 2025 (SIGGRAPH journal track); Project page: https://electronicarts.github.io/tiny-voice2face/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.18352v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.18352v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://electronicarts.github.io/tiny-voice2face/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微型模型仍不够小：通过混合知识蒸馏实现高质量低资源面部动画模型</div>
<div class="mono" style="margin-top:8px">用于语音驱动3D面部动画的高质量、鲁棒机器学习模型的训练需要一个包含高质量音频-动画对的大型多样化数据集。为了解决缺乏此类数据集的问题，最近的工作引入了大型预训练语音编码器，这些编码器对输入音频的变体具有鲁棒性，因此使面部动画模型能够在不同说话人、音频质量和语言之间泛化。然而，由此产生的面部动画模型过于庞大，只能在专用机器上进行离线推理。在本文中，我们探索了游戏开发中的设备上实时面部动画模型。我们通过使用混合知识蒸馏和伪标签来克服大型数据集的缺乏。给定一个大型音频数据集，我们使用高性能的教师模型来训练非常小的学生模型。与预训练的语音编码器不同，我们的学生模型仅由卷积层和全连接层组成，消除了注意力上下文或递归更新的需要。在我们的实验中，我们证明了可以将内存占用减少到最多3.4 MB，并将所需的未来音频上下文减少到最多81 ms，同时保持高质量的动画。这为设备上推理铺平了道路，这是实现真实、模型驱动的数字角色的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of creating high-quality facial animation models for real-time use in game development, where large datasets are scarce. By employing hybrid knowledge distillation with pseudo-labeling, the authors train very small student models using a large audio dataset and a high-performing teacher model. The resulting models, which consist only of convolutional and fully-connected layers, have a memory footprint of up to 3.4 MB and require only 81 ms of future audio context, while still producing high-quality animations. This approach enables on-device inference, a crucial step towards realistic, model-driven digital characters in real-time applications.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在游戏开发中创建高质量且资源消耗低的面部动画模型的挑战。通过使用伪标签和混合知识蒸馏，利用大量音频数据和高性能教师模型来训练非常小的学生模型。这些模型仅由卷积和全连接层组成，将内存占用减少到最多3.4 MB，并且只需要81 ms的未来音频上下文，同时保持高质量的动画效果。</div>
</details>
</div>
<div class="card">
<div class="title">Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision</div>
<div class="meta-line">Authors: Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia</div>
<div class="meta-line">First: 2026-02-12T18:15:32+00:00 · Latest: 2026-02-12T18:15:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12236v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47\%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于脉冲神经网络的神经形态视觉连续学习的能效感知脉冲预算</div>
<div class="mono" style="margin-top:8px">基于脉冲神经网络（SNN）的神经形态视觉系统为事件驱动和帧驱动的相机提供超低功耗感知，但灾难性遗忘仍然是在不断变化的环境中部署的关键障碍。现有的连续学习方法主要针对人工神经网络开发，很少同时优化准确性和能效，特别是在事件驱动数据集上的探索非常有限。我们提出了一种能效感知的脉冲预算框架，用于连续SNN学习，该框架结合了经验回放、可学习的漏型积分-放电神经元参数以及自适应脉冲调度器，在训练过程中强制执行特定数据集的能效约束。我们的方法表现出模态依赖性：在帧驱动数据集（MNIST、CIFAR-10）上，脉冲预算作为稀疏性诱导的正则化器，提高准确率的同时将脉冲率降低高达47%；在事件驱动数据集（DVS-Gesture、N-MNIST、CIFAR-10-DVS）上，可控的预算放松可实现高达17.45个百分点的准确率提升，同时计算开销最小。在五个横跨不同模态的基准测试中，我们的方法在提高性能的同时最大限度地减少了动态功耗，推动了神经形态视觉系统中连续学习的实际可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of catastrophic forgetting in continual learning for spiking neural networks (SNNs) used in neuromorphic vision systems. It introduces an energy-aware spike budgeting framework that combines experience replay, learnable neuron parameters, and an adaptive scheduler to optimize both accuracy and energy efficiency. The method shows significant improvements in accuracy and energy efficiency, with up to 47% reduction in spike rates on frame-based datasets and up to 17.45 percentage point gains on event-based datasets, while maintaining low computational overhead.</div>
<div class="mono" style="margin-top:8px">研究旨在通过使用脉冲神经网络（SNN）解决神经形态视觉系统中持续学习中的灾难性遗忘问题。提出的能量感知脉冲预算框架结合了经验回放、可学习的神经元参数和自适应调度器，以优化准确性和能效。在基于帧的数据集上，脉冲预算提高了准确率并减少了高达47%的脉冲率；而在基于事件的数据集上，它实现了高达17.45个百分点的准确率提升，同时具有最小的计算开销，展示了在各种基准测试中的持续性能改进。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating LLM Reasoning Beyond Correctness and CoT</div>
<div class="meta-line">Authors: Soheil Abbasloo</div>
<div class="meta-line">First: 2025-10-20T22:08:59+00:00 · Latest: 2026-02-12T18:07:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.18134v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.18134v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">What does it truly mean for a language model to &quot;reason&quot;? Current evaluations reward models&#x27; correct standalone answers-but correctness alone reveals little about the process that produced them. We argue that reasoning should be understood not as a static chain of steps but as a dynamic trajectory in which ideas interact, clash, and evolve into integrated insights. Building on the philosophical tradition of dialectics, we introduce SIEV, a structured evaluation framework that assesses reasoning through explicit thesis-antithesis-synthesis interactions. SIEV produces interpretable trajectories that highlight key properties of reasoning-robustness to challenge, adaptability under conflict, and synthesis across competing viewpoints-dimensions that conventional correctness-based metrics cannot capture. Empirical results on GSM and MMLU demonstrate substantial gaps in the reasoning abilities of state-of-the-art models: for example, GPT-5-chat loses more than 40 points (out of 100) on GSM when evaluated through SIEV&#x27;s process-oriented lens. By shifting focus from what answer a model gives to how it arrives there, SIEV enables a more transparent and principled distinction between structured reasoning and surface-level pattern generation offering a clearer foundation for assessing and understanding the reasoning capabilities of LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越正确性和思维过程评估大语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">语言模型究竟如何“推理”意味着什么？当前的评估奖励模型的正确独立答案，但仅凭正确性并不能揭示其背后的推理过程。我们认为，推理不应被视为静态的步骤链，而应被视为一种动态轨迹，在其中思想相互作用、碰撞并发展成综合见解。基于辩证法的哲学传统，我们引入了SIEV结构化评估框架，通过明确的论题-反题-合题互动来评估推理。SIEV生成可解释的轨迹，突出推理的关键属性——面对挑战的稳健性、冲突下的适应性以及观点竞争中的综合能力——这些维度是传统基于正确性的度量无法捕捉到的。在GSM和MMLU上的实证结果表明，最先进的模型在推理能力上存在显著差距：例如，GPT-5-chat在通过SIEV的过程导向视角评估时，得分下降超过40分（满分100分）。通过将焦点从模型给出的答案转向其如何得出答案，SIEV使结构化推理与表面模式生成之间的区分更加透明和原则化，为评估和理解大语言模型的推理能力提供了更清晰的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper aims to redefine the concept of reasoning in language models by moving beyond mere correctness to evaluate the dynamic process of reasoning. It introduces SIEV, a structured evaluation framework based on dialectics, which assesses reasoning through explicit interactions of thesis, antithesis, and synthesis. The empirical results show significant gaps in reasoning abilities of state-of-the-art models like GPT-5-chat when evaluated through SIEV, highlighting robustness, adaptability, and synthesis as key dimensions not captured by conventional metrics.</div>
<div class="mono" style="margin-top:8px">论文通过引入SIEV框架，评估语言模型的推理能力超越了单纯的正确性，该框架通过动态交互来评估推理。SIEV显示，如GPT-5-chat等最先进的模型在鲁棒性和适应性方面存在明显不足，在其过程导向的评估中表现出显著的性能下降，突显了它们推理能力的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training</div>
<div class="meta-line">Authors: Miaosen Zhang, Yishan Liu, Shuxia Lin, Xu Yang, Qi Dai, Chong Luo, Weihao Jiang, Peng Hou, Anxiang Zeng, Xin Geng, Baining Guo</div>
<div class="meta-line">First: 2026-02-12T17:59:58+00:00 · Latest: 2026-02-12T17:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12222v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12222v1">PDF</a> · <a href="https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL&#x27;s use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model&#x27;s distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向基于策略的微调：分布判别理论及其在大模型训练中的应用</div>
<div class="mono" style="margin-top:8px">监督微调（SFT）计算效率高，但通常在泛化能力上不如强化学习（RL）。这一差距主要由RL使用基于策略数据引起。我们提出了一种框架来弥合这一差距，使其能够实现基于策略的微调。我们首先提出了\textbf{\textit{分布判别理论（DDT）}}，解释并量化了数据与模型诱导分布之间的对齐程度。利用DDT，我们引入了两种互补的技术：（i）\textbf{\textit{在分布内微调（IDFT）}}，一种在损失层面增强SFT泛化能力的方法；（ii）\textbf{\textit{提示解码}}，一种在数据层面的技术，可以重新对齐训练语料库以匹配模型的分布。大量实验表明，我们的框架在泛化性能上与著名的离线RL算法（包括DPO和SimPO）相当，同时保持了SFT管道的效率。因此，该提出的框架为在RL不可行的领域提供了一种实用的替代方案。我们在此开源代码：https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the gap between supervised fine-tuning (SFT) and reinforcement learning (RL) by proposing a framework for on-policy SFT. It introduces Distribution Discriminant Theory (DDT) to quantify the alignment between data and model-induced distribution, and two techniques: In-Distribution Finetuning (IDFT) and Hinted Decoding. Experiments show that this framework matches the generalization performance of offline RL algorithms like DPO and SimPO while maintaining the computational efficiency of SFT, making it a practical alternative for RL-infeasible domains.</div>
<div class="mono" style="margin-top:8px">论文旨在通过提出面向策略的SFT框架来弥合SFT与RL之间的差距。它引入了分布判别理论（DDT）来解释数据与模型诱导分布之间的对齐，并提出了两种技术：In-Distribution Finetuning（IDFT）和提示解码。实验表明，所提出的框架在泛化性能上与DPO和SimPO等离线RL算法相当，同时保持了SFT的效率，使其成为RL不可行领域的实用替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching</div>
<div class="meta-line">Authors: Onkar Susladkar, Tushar Prakash, Gayatri Deshmukh, Kiet A. Nguyen, Jiaxun Zhang, Adheesh Juvekar, Tianshu Bao, Lin Chai, Sparsh Mittal, Inderjit S Dhillon, Ismini Lourentzou</div>
<div class="meta-line">First: 2026-02-12T17:59:08+00:00 · Latest: 2026-02-12T17:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12221v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12221v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>兼而有之：通过统一离散流匹配实现多模态推理与生成</div>
<div class="mono" style="margin-top:8px">我们提出了一种统一的离散流匹配框架UniDFlow，用于多模态理解、生成和编辑。该框架通过特定任务的低秩适配器解耦理解与生成，避免了目标干扰和表示纠缠，同时一种新颖的基于参考的多模态偏好对齐优化了在相同条件下的相对结果，提高了忠实度和可控性，而无需大规模重新训练。UniDFlpw在八个基准测试中取得了SOTA性能，并且在包括 inpainting、上下文图像生成、基于参考的编辑和组合生成等任务中表现出强大的零样本泛化能力，尽管没有进行明确的特定任务训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">UniDFlow is a unified discrete flow-matching framework designed for multimodal understanding, generation, and editing. It separates understanding and generation using task-specific low-rank adapters to prevent objective interference and representation entanglement. The model also includes a reference-based multimodal preference alignment to enhance faithfulness and controllability. UniDFlow achieves state-of-the-art performance across eight benchmarks and demonstrates strong zero-shot generalization to various tasks without extensive retraining.</div>
<div class="mono" style="margin-top:8px">UniDFlow 是一个统一的离散流匹配框架，用于多模态理解、生成和编辑。该模型通过任务特定的低秩适配器将理解和生成分离，以防止目标干扰和表示纠缠。此外，模型还包含基于参考的多模态偏好对齐，以提高准确性和可控性。UniDFlow 在八个基准测试中达到了最先进的性能，并且在各种任务上展示了强大的零样本泛化能力，而无需大量重新训练。</div>
</details>
</div>
<div class="card">
<div class="title">The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics</div>
<div class="meta-line">Authors: Christian Internò, Jumpei Yamaguchi, Loren Amdahl-Culleton, Markus Olhofer, David Klindt, Barbara Hammer</div>
<div class="meta-line">First: 2026-02-12T17:56:07+00:00 · Latest: 2026-02-12T17:56:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12218v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12218v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confound what was learned during self-supervised learning (SSL). We propose a non-invasive evaluation protocol, PhyIP. We test whether physical quantities are linearly decodable from frozen representations, motivated by the linear representation hypothesis. Across fluid dynamics and orbital mechanics, we find that when SSL achieves low error, latent structure becomes linearly accessible. PhyIP recovers internal energy and Newtonian inverse-square scaling on OOD tests (e.g., $ρ&gt; 0.90$). In contrast, adaptation-based evaluations can collapse this structure ($ρ\approx 0.05$). These findings suggest that adaptation-based evaluation can obscure latent structures and that low-capacity probes offer a more accurate evaluation of physical world models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of evaluating whether neural models internalize physical laws or rely on statistical shortcuts, especially under out-of-distribution shifts. It introduces PhyIP, a non-invasive evaluation protocol, to test the linear decodability of physical quantities from frozen representations. The study finds that when self-supervised learning achieves low error, physical structures become linearly accessible, recovering internal energy and Newtonian scaling on OOD tests with high correlation ($ρ&gt;0.90$). In contrast, adaptation-based evaluations obscure these structures ($ρ\approx0.05$).</div>
<div class="mono" style="margin-top:8px">研究旨在评估神经模型是否将物理定律作为世界模型内部化，而不是依赖统计捷径，特别是在分布外转移时。研究引入了一种非侵入性评估协议PhyIP，测试从冻结表示中线性解码物理量的能力。研究发现，当自我监督学习达到低误差时，潜在的物理结构变得线性可访问，能够在分布外测试中恢复内部能量和牛顿反平方定律，并且具有高相关系数。相比之下，适应性评估会掩盖这些结构。这表明低容量探针可以更准确地评估物理世界模型。</div>
</details>
</div>
<div class="card">
<div class="title">VIRENA: Virtual Arena for Research, Education, and Democratic Innovation</div>
<div class="meta-line">Authors: Emma Hoes, K. Jonathan Klueser, Fabrizio Gilardi</div>
<div class="meta-line">First: 2026-02-12T17:46:52+00:00 · Latest: 2026-02-12T17:46:52+00:00</div>
<div class="meta-line">Comments: VIRENA is under active development and currently in use at the University of Zurich, supported by the DIZH Innovation Program: 2nd Founder-Call. This preprint will be updated as new features are released. For the latest version and to inquire about demos or pilot collaborations, contact the authors</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12207v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA&#x27;s no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRENA：虚拟竞技场，用于研究、教育和民主创新</div>
<div class="mono" style="margin-top:8px">数字平台塑造了人们的交流、讨论和形成观点的方式。由于数据访问受限、现实世界实验的伦理限制以及现有研究工具的局限性，研究这些动态变得越来越困难。VIRENA（虚拟竞技场）是一个平台，它能够在现实社交媒体环境中进行受控实验。多个参与者可以同时在基于信息流的平台（Instagram、Facebook、Reddit）和即时通讯应用（WhatsApp、Messenger）的现实复制品中互动。由大型语言模型驱动的AI代理可以与人类一起参与，具有可配置的人设和现实行为。研究人员可以通过无需编程技能的可视化界面操控内容审核方法、预排定刺激内容，并在不同条件下运行实验。VIRENA 使得以前不切实际的研究设计成为可能：研究人类与AI的互动、实验性地比较干预措施的效果以及观察群体讨论的展开。VIRENA 建立在开源技术之上，确保数据保留在机构控制之下并符合数据保护要求，目前在苏黎世大学使用，并可供试点合作。VIRENA 的无代码界面使其跨学科和跨领域的受控社交媒体模拟变得可行。本文记录了其设计、架构和功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VIRENA is a platform designed to enable controlled experimentation in realistic social media environments, addressing the challenges of restricted data access and ethical constraints. It allows multiple participants to interact in replicas of popular social media platforms and messaging apps, with large language model-powered AI agents participating alongside humans. Researchers can manipulate content moderation approaches and run experiments through a no-code visual interface. Key findings include the ability to study human-AI interaction, experimentally compare moderation interventions, and observe group deliberation in realistic settings, making previously impractical research designs possible.</div>
<div class="mono" style="margin-top:8px">VIRENA 是一个平台，旨在通过无代码可视化界面在现实的社交媒体环境中进行受控实验，解决数据访问受限和伦理约束的问题。其主要发现包括能够复制真实的社交媒体动态并模拟具有可配置人设的 AI 代理，使以前不切实际的研究设计变得可行。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting</div>
<div class="meta-line">Authors: Chutian Ma, Grigorii Pomazkin, Giacinto Paolo Saggese, Paul Smith</div>
<div class="meta-line">First: 2026-01-15T21:26:57+00:00 · Latest: 2026-02-12T17:45:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10863v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally allows user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal auto-regressive integrated models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Regarding stability, the AC-optimized model generated out-of-sample forecasts with 91.1\% reduced vertical variance relative to the MLE-fitted model. In terms of accuracy, the AC-optimized model achieved considerable improvements for medium-to-long-horizon forecasts. While one-step-ahead forecasts exhibited a 7.5\% increase in MAPE, all subsequent horizons experienced an improved accuracy as measured by MAPE of up to 26\%. These results indicate that our metric successfully trains models to produce more stable and accurate multi-step forecasts in exchange for some degradation in one-step-ahead performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越准确性：面向多时距预测的稳定性度量</div>
<div class="mono" style="margin-top:8px">传统的时序预测方法仅优化准确性。这一目标忽略了时间一致性，即模型如何在预测起始时间变化时一致地预测同一未来事件。我们引入了预测准确性和连贯性评分（简称预测AC评分），以衡量概率多时距预测的质量，该评分同时考虑多时距准确性和稳定性。该评分还允许用户指定权重以平衡准确性和一致性要求。作为示例应用，我们将该评分实现为训练季节性自回归整定模型的可微目标函数，并在M4小时基准数据集上对其进行评估。结果表明，与最大似然估计相比，该评分在稳定性方面取得了显著改进。具体而言，AC优化模型生成的离样本外预测的垂直方差减少了91.1%。在准确性方面，AC优化模型在中长期预测中取得了显著改进。尽管一步预测的MAPE提高了7.5%，但所有后续时距的准确性均有所提高，MAPE最高提高了26%。这些结果表明，我们的度量标准成功地训练模型以产生更稳定和准确的多步预测，尽管这在一步预测性能上有所下降。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitation of traditional time series forecasting methods that focus solely on accuracy, neglecting temporal consistency. It introduces the forecast accuracy and coherence score (forecast AC score) to evaluate multi-horizon forecasts, considering both accuracy and stability. The AC score is used as a differentiable objective function for training seasonal auto-regressive integrated models and is evaluated on the M4 Hourly benchmark dataset. The results show that the AC-optimized model significantly reduces vertical variance by 91.1% and improves accuracy for medium-to-long horizons, with up to 26% reduction in MAPE, albeit with a slight increase in one-step-ahead forecasts.</div>
<div class="mono" style="margin-top:8px">论文针对传统时间序列预测方法仅关注准确性的不足，忽视了时间一致性，提出了预测准确性和连贯性评分（预测AC评分）来评估多步预测的质量，同时考虑准确性和稳定性。AC评分被用作训练季节性自回归整定模型的可微分目标函数。在M4小时基准数据集上的实验表明，AC优化模型显著减少了91.1%的垂直方差，并且提高了中长期预测的准确性，MAPE最多降低了26%，尽管一阶预测的MAPE提高了7.5%。</div>
</details>
</div>
<div class="card">
<div class="title">DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing</div>
<div class="meta-line">Authors: Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song, Siyuan Wang, Yibin Wang, Yi Xin, Hongjian Liu, Zhixiong Zhang, Shengyuan Ding, Tianhang Wang, Zhenglin Cheng, Tao Lin, Cheng Jin, Kaicheng Yu, Jingjing Chen, Wenjie Wang, Zhongyu Wei, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-12T17:44:24+00:00 · Latest: 2026-02-12T17:44:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12205v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., &gt;10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable &#x27;think tokens&#x27; to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepGen 1.0：一种轻量级统一多模态模型，用于推进图像生成和编辑</div>
<div class="mono" style="margin-top:8px">当前用于图像生成和编辑的统一多模态模型通常依赖于庞大的参数规模（例如，&gt;10B），导致高昂的训练成本和部署足迹。在本工作中，我们提出了DeepGen 1.0，这是一种轻量级的5B统一模型，能够实现与更大模型相当或超越的全面能力。为了克服紧凑模型在语义理解和精细控制方面的局限性，我们引入了堆叠通道桥接（SCB），这是一种深度对齐框架，从多个VLM层中提取层次特征，并与可学习的“思考标记”融合，为生成骨干提供结构化、富含推理的指导。我们进一步设计了一种以数据为中心的训练策略，分为三个渐进阶段：（1）在大规模图像-文本对和编辑三元组上进行对齐预训练，以同步VLM和DiT表示；（2）在高质量的生成、编辑和推理任务混合集上进行联合监督微调，以培养全方位的能力；（3）使用MR-GRPO的强化学习，利用混合奖励函数和监督信号，显著提高了生成质量和与人类偏好的一致性，同时保持稳定的训练进展并避免视觉伪影。尽管仅在约5000万样本上进行训练，DeepGen 1.0在多种基准测试中均表现出领先性能，在WISE上超越了80B的HunyuanImage 28%，在UniREditBench上超越了27B的Qwen-Image-Edit 37%。通过开源我们的训练代码、权重和数据集，我们提供了一种高效、高性能的替代方案，以促进统一多模态研究的民主化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DeepGen 1.0 is a lightweight 5B unified model for image generation and editing, addressing the high training and deployment costs of large models. It introduces Stacked Channel Bridging (SCB) to enhance semantic understanding and fine-grained control. The model is trained in three stages: alignment pre-training, joint supervised fine-tuning, and reinforcement learning. DeepGen 1.0 outperforms larger models on various benchmarks, achieving 28% and 37% better performance on WISE and UniREditBench, respectively. It provides a more efficient alternative for unified multimodal research.</div>
<div class="mono" style="margin-top:8px">DeepGen 1.0 是一个轻量级的 5B 统一模型，用于图像生成和编辑，解决了大型模型高昂的训练和部署成本问题。它引入了堆叠通道桥接 (SCB) 来增强语义理解和精细控制。该模型经过三个阶段的训练：对齐预训练、联合监督微调和强化学习。DeepGen 1.0 在各种基准测试中表现出色，分别在 WISE 和 UniREditBench 上取得了 28% 和 37% 的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors</div>
<div class="meta-line">Authors: Arian Khorasani, Nathaniel Chen, Yug D Oswal, Akshat Santhana Gopalan, Egemen Kolemen, Ravid Shwartz-Ziv</div>
<div class="meta-line">First: 2026-01-30T21:08:55+00:00 · Latest: 2026-02-12T17:44:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00315v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00315v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越损失曲线：缩放定律、主动学习与从精确后验学习的极限</div>
<div class="mono" style="margin-top:8px">神经网络距离它们可能达到的最佳性能有多接近？标准基准无法回答这个问题，因为它们缺乏访问真正的后验 p(y|x) 的能力。我们使用条件归一化流作为或acles，使其在现实图像（AFHQ、ImageNet）上计算精确后验变得可行。这使我们能够进行五项调查。缩放定律：预测误差分解为不可约的偶然不确定性与可约的先验误差；先验部分遵循数据集大小的幂律，在总损失平台期后仍然继续缩小。学习的极限：偶然的下限是可以精确测量的，而不同架构在接近它的方式上存在显著差异：ResNets 展现出清晰的幂律缩放，而视觉变换器在低数据域中停滞不前。软标签：Oracle 后验包含超越类别标签的可学习结构：使用精确后验进行训练优于使用硬标签，并且能够实现近乎完美的校准。分布偏移：Oracle 计算受控扰动的精确KL散度，揭示了扰动类型比扰动幅度更重要：类别不平衡在KL散度值较高的情况下，输入噪声导致灾难性退化，几乎不影响准确性。主动学习：精确的先验不确定性区分了真正有信息量的样本与本质上模棱两可的样本，提高了样本效率。我们的框架揭示了标准度量隐藏了持续学习，掩盖了架构差异，并且无法诊断分布偏移的性质。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the limits of neural network performance by using class-conditional normalizing flows as oracles to access exact posteriors. The research finds that prediction error can be decomposed into irreducible aleatoric uncertainty and reducible epistemic error, with the latter following a power law in dataset size. The study also reveals that architectures like ResNets and Vision Transformers behave differently in terms of approaching the aleatoric floor, and that training with exact posteriors leads to better calibration. Additionally, the research shows that the type of distribution shift is more critical than its magnitude, and that exact epistemic uncertainty can improve sample efficiency in active learning.</div>
<div class="mono" style="margin-top:8px">研究使用类条件归一化流作为先知来访问精确的后验，以探索神经网络性能的极限。研究发现预测误差可以分解为不可约的不确定性与可减少的误差，后者随数据集大小增加遵循幂律。研究还揭示不同架构在接近不可约地板时表现不同，ResNets 展现出清晰的幂律缩放，而 Vision Transformers 在低数据环境下表现不佳。此外，使用精确的后验进行训练比使用硬标签提供更好的校准，先知还可以测量扰动的确切 KL 散度，强调分布类型比分布幅度更重要。该框架还表明，精确的先验不确定性可以提高主动学习中的样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction</div>
<div class="meta-line">Authors: Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</div>
<div class="meta-line">First: 2026-02-12T17:40:15+00:00 · Latest: 2026-02-12T17:40:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12204v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12204v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid architectures combining state-space models with attention have achieved strong efficiency-quality tradeoffs, yet existing approaches either apply attention uniformly or learn static sparse patterns. This misses a key opportunity: \emph{attention demand should decrease over time as recurring patterns become familiar}. We present a surprising finding from analyzing GPT-2 models: \textbf{88\%} of attention operations retrieve information already predictable from the model&#x27;s hidden state, and this redundancy does \emph{not} decrease during training. Motivated by this observation, we introduce \textbf{\ours{}} (\textbf{C}onsolidation-based \textbf{R}outing for \textbf{A}daptive \textbf{M}emory), a biologically inspired memory consolidation mechanism that gradually distills episodic retrievals into parametric semantic memory. Unlike prior sparse attention methods, \ours{} exhibits \emph{decreasing attention utilization} over training, achieving a \textbf{37.8$\times$} reduction through a sharp phase transition at approximately 3K steps. We prove that this capability is \emph{impossible} without consolidation: any static routing scheme requires $Ω(f \cdot n)$ attention for tasks with recurring patterns of frequency $f$. On our proposed SRCD benchmark, \ours{} achieves \textbf{100\% retrieval accuracy} at 1.6\% attention compute (vs.\ 68\% for baselines), and consolidated patterns transfer to unseen tasks with \textbf{48--52\%} attention reduction without retraining. Remarkably, the learned consolidation dynamics quantitatively match human episodic-to-semantic memory transition curves from cognitive psychology ($γ= 0.43$ vs.\ $γ_{\text{human}} \approx 0.4$--$0.5$). Code and benchmarks are available at [anonymized].</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习忘记注意力：记忆巩固以适应计算减少</div>
<div class="mono" style="margin-top:8px">结合状态空间模型与注意力的混合架构已实现了强大的效率-质量权衡，但现有方法要么均匀应用注意力，要么学习静态稀疏模式。这错过了一个关键机会：\emph{随着时间的推移，重复模式变得熟悉后，注意力需求应该减少}。我们通过对GPT-2模型的分析发现：\textbf{88\%}的注意力操作检索的信息已经可以从模型的隐藏状态预测，而且这种冗余在训练过程中并未减少。受此观察的启发，我们引入了\textbf{\ours{}}（基于巩固的自适应记忆路由），这是一种生物启发的记忆巩固机制，逐步将事件检索提炼为参数语义记忆。与之前的稀疏注意力方法不同，\ours{}在训练过程中表现出\emph{注意力利用率下降}，通过大约3K步的尖锐相变实现了\textbf{37.8$\times$}的计算减少。我们证明了这种能力在没有巩固的情况下是\emph{不可能}的：任何静态路由方案对于具有重复模式频率$f$的任务都需要$Ω(f \cdot n)$的注意力。在我们提出的SRCD基准上，\ours{}在1.6\%注意力计算下实现了\textbf{100\%}的检索准确率（基线为68%），并且巩固后的模式在无需重新训练的情况下将未见过的任务的注意力减少\textbf{48--52\%}。令人惊讶的是，学习到的巩固动态与认知心理学中的人类事件到语义记忆过渡曲线定量匹配（$γ= 0.43$ vs.\ $γ_{\text{human}} \approx 0.4$--$0.5$）。代码和基准可在[匿名链接]获取。</div>
</details>
</div>
<div class="card">
<div class="title">LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs</div>
<div class="meta-line">Authors: Yujun Zhou, Jingdong Yang, Yue Huang, Kehan Guo, Zoe Emory, Bikram Ghosh, Amita Bedar, Sujay Shekar, Zhenwen Liang, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang</div>
<div class="meta-line">Venue: Nat Mach Intell 8, 20-31 (2026)</div>
<div class="meta-line">First: 2024-10-18T05:21:05+00:00 · Latest: 2026-02-12T17:29:23+00:00</div>
<div class="meta-line">Comments: Published at Nature Machine Intelligence</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.14182v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.14182v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Artificial Intelligence (AI) is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. Large language models (LLMs) and vision language models (VLMs) now assist in experiment design and procedural guidance, yet their &quot;illusion of understanding&quot; may lead researchers to overtrust unsafe outputs. Here we show that current models remain far from meeting the reliability needed for safe laboratory operation. We introduce LabSafety Bench, a comprehensive benchmark that evaluates models on hazard identification, risk assessment, and consequence prediction across 765 multiple-choice questions and 404 realistic lab scenarios, encompassing 3,128 open-ended tasks. Evaluations on 19 advanced LLMs and VLMs show that no model evaluated on hazard identification surpasses 70% accuracy. While proprietary models perform well on structured assessments, they do not show a clear advantage in open-ended reasoning. These results underscore the urgent need for specialized safety evaluation frameworks before deploying AI systems in real laboratory settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实验室安全台：评估大型语言模型在科学实验室安全问题上的基准</div>
<div class="mono" style="margin-top:8px">人工智能（AI）正在革新科学研究，但其日益融入实验室环境带来了关键的安全挑战。大型语言模型（LLMs）和视觉语言模型（VLMs）现在协助实验设计和程序指导，但它们的“理解错觉”可能导致研究人员过度信任不安全的输出。我们展示当前模型远未达到安全实验室操作所需的可靠性。我们引入了LabSafety Bench，这是一个全面的基准，评估模型在危害识别、风险评估和后果预测方面的表现，涵盖765个多项选择题和404个现实实验室场景，共计3,128个开放式任务。对19个先进LLM和VLM的评估显示，没有模型在危害识别上的准确率超过70%。虽然专有模型在结构化评估中表现良好，但在开放式推理方面并没有明显优势。这些结果强调了在实际实验室环境中部署AI系统之前，迫切需要专门的安全评估框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study aims to address the critical safety challenges posed by the integration of AI in scientific laboratories. It introduces LabSafety Bench, a benchmark that tests models on hazard identification, risk assessment, and consequence prediction. Evaluations on 19 advanced LLMs and VLMs reveal that no model achieves over 70% accuracy in hazard identification, highlighting the need for specialized safety evaluation frameworks before deploying AI in real laboratory settings.</div>
<div class="mono" style="margin-top:8px">研究旨在解决AI在科学实验室中集成带来的安全挑战。引入了LabSafety Bench基准，测试模型在危害识别、风险评估和后果预测方面的表现。对19种先进LLM和VLM的评估显示，没有模型在危害识别上的准确率超过70%，强调了在实际实验室环境中部署AI系统之前需要专门的安全评估框架的紧迫性。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education</div>
<div class="meta-line">Authors: Mohamed Huti, Alasdair Mackintosh, Amy Waldock, Dominic Andrews, Maxime Lelièvre, Moritz Boos, Tobias Murray, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod</div>
<div class="meta-line">First: 2026-02-12T17:29:03+00:00 · Latest: 2026-02-12T17:29:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier&#x27;&#x27; of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling&#x27;&#x27; when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉推理基准：评估多模态大语言模型在小学课堂真实视觉问题上的推理能力</div>
<div class="mono" style="margin-top:8px">AI模型在文本推理方面已达到最先进的水平；然而，它们在处理空间和关系结构方面的推理能力仍然是一个关键瓶颈——特别是在依赖视觉的早期数学中。本文介绍了视觉推理基准（VRB），这是一个新型数据集，旨在评估多模态大语言模型（MLLMs）在解决课堂真实视觉问题方面的能力。该基准基于来自赞比亚和印度小学考试的701个问题，涵盖了诸如类比推理、模式填充和空间匹配等一系列任务。我们概述了基准的方法和开发过程，故意使用未经编辑、文字最少的图像来测试模型是否能满足小学教育的实际需求。我们的研究结果揭示了一个“能力不均衡”的领域，模型在静态技能如计数和缩放方面表现出更好的熟练度，但在面对动态操作如折叠、反射和旋转时则达到了一个明显的“空间天花板”。这些弱点对课堂使用视觉推理问题构成风险，可能导致错误评分、虚假支持和强化学生的错误观念。因此，像VRB这样的面向教育的基准对于确定用于课堂的多模态工具的功能边界至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces the Visual Reasoning Benchmark (VRB) to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from primary education. The benchmark consists of 701 questions from Zambia and India, covering tasks like reasoning by analogy and spatial matching. The study finds that models perform well in static skills such as counting and scaling but struggle with dynamic operations like folding and reflection, indicating a &#x27;spatial ceiling&#x27; that limits their effectiveness in classrooms.</div>
<div class="mono" style="margin-top:8px">本文介绍了视觉推理基准（VRB），用于评估多模态大型语言模型（MLLMs）解决小学课堂中真实视觉问题的能力。基准包括来自赞比亚和印度的701个问题，涵盖了类比推理和空间匹配等任务。研究发现，模型在计数和缩放等静态技能上表现良好，但在折叠、反射等动态操作上却遇到困难，表明存在一个‘空间天花板’，限制了它们在课堂中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Rewards in Reinforcement Learning for Cyber Defence</div>
<div class="meta-line">Authors: Elizabeth Bates, Chris Hicks, Vasilios Mavroudis</div>
<div class="meta-line">First: 2026-02-04T17:55:23+00:00 · Latest: 2026-02-12T17:29:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04809v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越强化学习中网络防御中的奖励</div>
<div class="mono" style="margin-top:8px">近年来，使用深度强化学习训练自主网络防御代理以防御计算机网络的兴趣激增。这些代理通常在精心设计的奖励函数下，在网络健身房环境中进行训练，这些奖励函数结合了多种惩罚和激励，以应对各种（不）希望的状态和昂贵的操作。密集奖励有助于缓解探索复杂环境的挑战，但也可能使代理偏向次优甚至更危险的解决方案，这是复杂网络环境中一个关键问题。我们使用稀疏和密集奖励函数、两种成熟的网络健身房、不同规模的网络以及策略梯度和值基强化学习算法，全面评估了奖励函数结构对学习和策略行为特征的影响。我们的评估得益于一种新颖的基准评估方法，该方法允许直接比较不同奖励函数之间的差异，揭示了奖励、动作空间和次优策略风险之间的复杂关系在网络环境中。我们的结果表明，只要稀疏奖励与目标对齐并且可以频繁遇到，稀疏奖励能够提供增强的训练可靠性和具有较低风险的更有效的网络防御代理。令人惊讶的是，稀疏奖励还可以产生与网络防御者目标更好的对齐策略，并在没有显式基于奖励的数值惩罚的情况下节省使用昂贵的防御操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of reward function structure on the learning and policy behavior of autonomous cyber defense agents using reinforcement learning. The research evaluates both sparse and dense reward functions in various cyber gym environments and network sizes, employing policy gradient and value-based RL algorithms. Key findings indicate that sparse rewards, when aligned with goals and frequently encountered, enhance training reliability and produce more effective, lower-risk cyber defense policies. Surprisingly, sparse rewards also lead to policies that are better aligned with cyber defender goals and use costly defensive actions sparingly without explicit penalties.</div>
<div class="mono" style="margin-top:8px">该研究探讨了奖励函数结构对自主网络防御代理在强化学习中的学习和策略特征的影响。通过在各种网络规模和网络防御环境中的密集和稀疏奖励函数之间进行比较，研究发现，当稀疏奖励与目标对齐且频繁出现时，它们可以提高训练可靠性并生成更有效的、风险更低的网络防御策略。令人惊讶的是，稀疏奖励还会导致更符合网络防御者目标的策略，并减少对昂贵防御行动的使用，而无需显式的基于奖励的数值惩罚。</div>
</details>
</div>
<div class="card">
<div class="title">AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</div>
<div class="meta-line">Authors: Amine Lbath, Massih-Reza Amini, Aurelien Delaitre, Vadim Okun</div>
<div class="meta-line">First: 2025-08-28T14:59:39+00:00 · Latest: 2026-02-12T17:24:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20866v5">Abs</a> · <a href="https://arxiv.org/pdf/2508.20866v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing complexity of software systems and the sophistication of cyber-attacks have underscored the need for reliable automated software vulnerability detection. Data-driven approaches using deep learning models show promise but critically depend on the availability of large, accurately labeled datasets. Yet existing datasets either suffer from noisy labels, limited vulnerability coverage, or fail to reflect vulnerabilities as they occur in real-world software. This also limits large-scale benchmarking of such solutions. Automated vulnerability injection provides a way to address these limitations, but existing techniques remain limited in coverage, contextual fidelity, or injection success. In this paper, we present AVIATOR, the first AI-agentic vulnerability injection framework. AVIATOR decomposes vulnerability injection into a coordinated workflow of specialized AI agents, tool-based analysis, and iterative self-correction, explicitly mirroring expert reasoning. It integrates RAG and lightweight LoRA-based fine-tuning to produce realistic, category-specific vulnerabilities without relying on handcrafted patterns. Across three benchmarks, AVIATOR achieves high injection fidelity (91-95%) surpassing existing injection techniques in both accuracy and vulnerability coverage. When used for data augmentation to train deep learning-based vulnerability detection (DLVD) models, AVIATOR provides the strongest downstream gains in vulnerability detection. Across models and base datasets, AVIATOR improves average F1 scores by +22% over no augmentation, +25% over VGX, holding the prior best injection success rate, and +3% over VulScribeR, the prior state-of-the-art LLM-based injection model, with +7% higher recall and no precision loss. Its augmented data exhibits the lowest distributional distortion and scales efficiently with &lt;2% syntax rejection at 4.3x lower cost than VulScribeR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI自主漏洞注入与转换及优化推理</div>
<div class="mono" style="margin-top:8px">软件系统的复杂性和网络攻击的复杂性突显了可靠自动化软件漏洞检测的必要性。基于数据的方法使用深度学习模型显示出潜力，但关键依赖于大规模、准确标注的数据集。然而，现有数据集要么标签不准确，要么漏洞覆盖范围有限，或者无法反映真实软件中出现的漏洞。这也限制了此类解决方案的大规模基准测试。自动漏洞注入提供了一种解决这些限制的方法，但现有技术在覆盖范围、上下文保真度或注入成功率方面仍然有限。在本文中，我们提出了AVIATOR，这是第一个AI自主漏洞注入框架。AVIATOR将漏洞注入分解为专门AI代理、工具分析和迭代自我纠正的协调工作流，明确地模仿专家推理。它结合了RAG和轻量级LoRA微调，生成现实且类别特定的漏洞，而不依赖于手工设计的模式。在三个基准测试中，AVIATOR实现了高注入保真度（91-95%），在准确性和漏洞覆盖方面超过了现有注入技术。当用于训练基于深度学习的漏洞检测（DLVD）模型的数据增强时，AVIATOR在漏洞检测方面提供了最强的下游增益。在所有模型和基础数据集上，AVIATOR的平均F1分数比无增强提高了+22%，比VGX提高了+25%，保持了先前的最佳注入成功率，比VulScribeR，即先前最先进的基于LLM的注入模型，提高了+3%，召回率提高了+7%，且无精度损失。其增强数据的分布失真最低，且在成本降低4.3倍的情况下，语法拒绝率低于2%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces AVIATOR, an AI-agentic vulnerability injection framework that decomposes vulnerability injection into a workflow of specialized AI agents, tool-based analysis, and iterative self-correction. It uses RAG and lightweight LoRA-based fine-tuning to generate realistic, category-specific vulnerabilities without relying on handcrafted patterns. AVIATOR achieves high injection fidelity (91-95%) and provides the strongest downstream gains in vulnerability detection, improving average F1 scores by +22% over no augmentation, +25% over VGX, and +3% over VulScribeR, the prior state-of-the-art LLM-based injection model.</div>
<div class="mono" style="margin-top:8px">论文介绍了AVIATOR，这是一种AI代理漏洞注入框架，将漏洞注入分解为由专门的AI代理、工具分析和迭代自我纠正组成的工作流。AVIATOR 使用 RAG 和轻量级 LoRA 基础微调来生成无需依赖手工编写的模式的现实、类别特定的漏洞。它实现了高注入保真度（91-95%），并提供了在漏洞检测中最强的下游增益，平均 F1 分数提高了 +22%，召回率提高了 +7%，超过了之前的最佳 LLM 基础注入模型 VulScribeR。</div>
</details>
</div>
<div class="card">
<div class="title">WaveFormer: Wavelet Embedding Transformer for Biomedical Signals</div>
<div class="meta-line">Authors: Habib Irani, Bikram De, Vangelis Metsis</div>
<div class="meta-line">First: 2026-02-12T17:20:43+00:00 · Latest: 2026-02-12T17:20:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12189v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12189v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Biomedical signal classification presents unique challenges due to long sequences, complex temporal dynamics, and multi-scale frequency patterns that are poorly captured by standard transformer architectures. We propose WaveFormer, a transformer architecture that integrates wavelet decomposition at two critical stages: embedding construction, where multi-channel Discrete Wavelet Transform (DWT) extracts frequency features to create tokens containing both time-domain and frequency-domain information, and positional encoding, where Dynamic Wavelet Positional Encoding (DyWPE) adapts position embeddings to signal-specific temporal structure through mono-channel DWT analysis. We evaluate WaveFormer on eight diverse datasets spanning human activity recognition and brain signal analysis, with sequence lengths ranging from 50 to 3000 timesteps and channel counts from 1 to 144. Experimental results demonstrate that WaveFormer achieves competitive performance through comprehensive frequency-aware processing. Our approach provides a principled framework for incorporating frequency-domain knowledge into transformer-based time series classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WaveFormer：小波嵌入变换器在生物医学信号中的应用</div>
<div class="mono" style="margin-top:8px">生物医学信号分类由于长序列、复杂的时序动态和多尺度频率模式，标准的变换器架构难以充分捕捉。我们提出WaveFormer，这是一种在两个关键阶段整合小波分解的变换器架构：在嵌入构建阶段，多通道离散小波变换(DWT)提取频率特征，创建包含时域和频域信息的令牌；在位置编码阶段，动态小波位置编码(DyWPE)通过单通道DWT分析适应信号特定的时序结构。我们在八个涵盖人类活动识别和脑信号分析的多样数据集上评估了WaveFormer，序列长度从50到3000个时间步，通道数从1到144。实验结果表明，WaveFormer通过全面的频率感知处理实现了竞争力的性能。我们的方法为将频域知识整合到基于变换器的时间序列分类中提供了原则性的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of classifying biomedical signals with long sequences and complex temporal dynamics by proposing WaveFormer, a transformer architecture that integrates wavelet decomposition. It uses multi-channel Discrete Wavelet Transform for embedding construction and Dynamic Wavelet Positional Encoding for adaptive positional encoding. Evaluations on eight diverse datasets show that WaveFormer performs competitively through comprehensive frequency-aware processing.</div>
<div class="mono" style="margin-top:8px">研究针对生物医学信号分类中的长序列和复杂时序动态挑战，提出了WaveFormer，一种结合小波分解的变压器架构。在嵌入构建阶段，使用多通道离散小波变换（DWT）提取频率特征；在位置编码阶段，通过单通道DWT分析自适应位置嵌入，以适应信号特定的时序结构。WaveFormer在八个不同的数据集上进行了评估，结果显示它通过全面的频率感知处理实现了竞争力的性能。</div>
</details>
</div>
<div class="card">
<div class="title">OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data</div>
<div class="meta-line">Authors: Patrick Langer, Thomas Kaar, Max Rosenblattl, Maxwell A. Xu, Winnie Chow, Martin Maritsch, Robert Jakob, Ning Wang, Aradhana Verma, Brian Han, Daniel Seung Kim, Henry Chubb, Scott Ceresnak, Aydin Zahedivash, Alexander Tarlochan Singh Sandhu, Fatima Rodriguez, Daniel McDuff, Elgar Fleisch, Oliver Aalami, Filipe Barata, Paul Schmiedmayer</div>
<div class="meta-line">First: 2025-10-02T09:58:23+00:00 · Latest: 2026-02-12T17:19:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02410v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.02410v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs have emerged as powerful tools for interpreting multimodal data. In medicine, they hold particular promise for synthesizing large volumes of clinical information into actionable insights and digital health applications. Yet, a major limitation remains their inability to handle time series. To overcome this gap, we present OpenTSLM, a family of Time Series Language Models (TSLMs) created by integrating time series as a native modality to pretrained LLMs, enabling reasoning over multiple time series of any length. We investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt, models time series implicitly by concatenating learnable time series tokens with text tokens via soft prompting. Although parameter-efficient, we hypothesize that explicit time series modeling scales better and outperforms implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time series with text via cross-attention. We benchmark both variants against baselines that treat time series as text tokens or plots, across a suite of text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR, compared to 9.05 and 52.2 for finetuned text-only models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences, while maintaining stable memory requirements. By contrast, SoftPrompt grows exponentially in memory with sequence length, requiring around 110 GB compared to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA. To facilitate further research, we provide all code, datasets, and models open-source.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenTSLM：用于处理多变量医疗文本和时间序列数据的语言模型</div>
<div class="mono" style="margin-top:8px">大规模语言模型（LLMs）已成为解释多模态数据的强大工具。在医学领域，它们特别有潜力将大量临床信息综合成可操作的见解和数字健康应用。然而，一个主要限制是它们无法处理时间序列数据。为克服这一差距，我们提出了OpenTSLM，这是一种通过将时间序列作为预训练LLMs的原生模态来集成的时间序列语言模型（TSLMs），从而能够在任意长度的时间序列上进行推理。我们研究了OpenTSLM的两种架构。第一种，OpenTSLM-SoftPrompt，通过软提示将可学习的时间序列标记与文本标记连接起来，隐式建模时间序列。尽管参数效率较高，但我们假设显式的时间序列建模更具扩展性并优于隐式方法。因此，我们引入了OpenTSLM-Flamingo，它通过交叉注意力将时间序列与文本集成。我们使用一系列文本-时间序列链式推理任务（CoT）对两种变体与将时间序列视为文本标记或图表的基线进行了基准测试。我们引入了三个数据集：HAR-CoT、Sleep-CoT和ECG-QA-CoT。在所有任务中，OpenTSLM模型均优于基线，睡眠分期任务的F1值达到69.9，HAR任务的F1值达到65.4，而微调的纯文本模型分别为9.05和52.2。值得注意的是，即使参数量为10亿的OpenTSLM模型也超过了GPT-4o（15.47和2.95）。OpenTSLM-Flamingo在性能上与OpenTSLM-SoftPrompt相当，并在更长的序列上表现更好，同时保持稳定的内存需求。相比之下，SoftPrompt的内存需求随序列长度呈指数增长，当在LLaMA-3B上训练ECG-QA时，需要约110 GB，而训练时仅需40 GB VRAM。临床专家评审发现OpenTSLMs在ECG-QA上表现出强大的推理能力。为了促进进一步研究，我们提供了所有代码、数据集和模型的开源版本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance language models (LMs) for handling time series data in medical applications. OpenTSLM, a family of Time Series Language Models, integrates time series as a native modality into pretrained LLMs. Two architectures, OpenTSLM-SoftPrompt and OpenTSLM-Flamingo, were developed. OpenTSLM-Flamingo, which uses cross-attention to integrate time series with text, outperforms OpenTSLM-SoftPrompt, especially on longer sequences. Across various text-time-series reasoning tasks, OpenTSLM models significantly outperform baselines, achieving F1 scores of 69.9 and 65.4 in sleep staging and heart activity recognition, respectively, compared to 9.05 and 52.2 for fine-tuned text-only models. OpenTSLM-Flamingo also maintains stable memory requirements, unlike SoftPrompt, which requires significantly more memory for longer sequences.</div>
<div class="mono" style="margin-top:8px">研究旨在增强语言模型处理医疗应用中时间序列数据的能力。OpenTSLM 是一种将时间序列作为原生模态集成到预训练 LLM 中的时序语言模型系列。开发了两种架构：OpenTSLM-SoftPrompt 和 OpenTSLM-Flamingo。OpenTSLM-Flamingo 使用交叉注意力将时间序列与文本集成，尤其在长序列上表现更优。在各种文本-时间序列推理任务中，OpenTSLM 模型显著优于基线模型，睡眠分期和心活动识别任务的 F1 分数分别为 69.9 和 65.4，而仅微调文本模型的分数分别为 9.05 和 52.2。OpenTSLM-Flamingo 还保持了稳定的内存需求，而 SoftPrompt 在长序列上需要的内存呈指数增长，训练 ECG-QA 时需要约 110 GB，相比之下，LLaMA-3B 训练时只需 40 GB 显存。</div>
</details>
</div>
<div class="card">
<div class="title">SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization</div>
<div class="meta-line">Authors: Sunghwan Kim, Wooseok Jeong, Serin Kim, Sangam Lee, Dongha Lee</div>
<div class="meta-line">First: 2026-02-12T17:18:00+00:00 · Latest: 2026-02-12T17:18:00+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12187v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search-Augmented Generative Engines (SAGE) have emerged as a new paradigm for information access, bridging web-scale retrieval with generative capabilities to deliver synthesized answers. This shift has fundamentally reshaped how web content gains exposure online, giving rise to Search-Augmented Generative Engine Optimization (SAGEO), the practice of optimizing web documents to improve their visibility in AI-generated responses. Despite growing interest, no evaluation environment currently supports comprehensive investigation of SAGEO. Specifically, existing benchmarks lack end-to-end visibility evaluation of optimization strategies, operating on pre-determined candidate documents that abstract away retrieval and reranking preceding generation. Moreover, existing benchmarks discard structural information (e.g., schema markup) present in real web documents, overlooking the rich signals that search systems actively leverage in practice. Motivated by these gaps, we introduce SAGEO Arena, a realistic and reproducible environment for stage-level SAGEO analysis. Our objective is to jointly target search-oriented optimization (SEO) and generation-centric optimization (GEO). To achieve this, we integrate a full generative search pipeline over a large-scale corpus of web documents with rich structural information. Our findings reveal that existing approaches remain largely impractical under realistic conditions and often degrade performance in retrieval and reranking. We also find that structural information helps mitigate these limitations, and that effective SAGEO requires tailoring optimization to each pipeline stage. Overall, our benchmark paves the way for realistic SAGEO evaluation and optimization beyond simplified settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGEO竞技场：评估生成增强检索优化的现实环境</div>
<div class="mono" style="margin-top:8px">生成增强检索引擎（SAGE）作为一种新的信息访问范式，将大规模检索与生成能力相结合，提供合成答案。这一转变从根本上重塑了网络内容在线曝光的方式，催生了生成增强检索优化（SAGEO），即优化网页文档以提高其在AI生成响应中的可见性。尽管兴趣日益浓厚，但目前尚无评估环境支持全面研究SAGEO。具体而言，现有基准缺乏端到端的优化策略可见性评估，仅基于预先确定的候选文档，这些文档忽略了检索和重新排序之前的结构信息。此外，现有基准忽略了真实网页文档中存在的结构信息（例如，模式标记），忽视了搜索系统在实践中积极利用的丰富信号。鉴于这些差距，我们引入了SAGEO竞技场，这是一种现实且可重复的环境，用于阶段级SAGEO分析。我们的目标是同时针对搜索引擎优化（SEO）和生成中心优化（GEO）。为此，我们整合了一个大规模网页文档语料库的完整生成检索流水线，这些文档具有丰富的结构信息。我们的研究发现，现有方法在现实条件下仍然不切实际，并且往往在检索和重新排序中降低性能。我们还发现，结构信息有助于缓解这些限制，有效的SAGEO需要针对每个流水线阶段进行优化。总体而言，我们的基准为现实的SAGEO评估和优化铺平了道路，超越了简化设置。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces SAGEO Arena, an environment designed to evaluate Search-Augmented Generative Engine Optimization (SAGEO) by integrating a full generative search pipeline with rich structural information from web documents. The motivation is to address the lack of comprehensive evaluation tools for SAGEO, which typically ignore structural information and retrieval processes. Key findings show that existing approaches often degrade performance under realistic conditions, and that structural information is crucial for effective SAGEO optimization, which needs to be tailored to each pipeline stage.</div>
<div class="mono" style="margin-top:8px">论文介绍了SAGEO Arena，一个用于评估搜索增强生成引擎优化(SAGEO)的现实环境。受缺乏全面评估工具的驱动，作者开发了一个包含丰富网页结构信息的完整生成搜索管道。主要发现包括现有方法在现实条件下的不实用性，以及优化需要针对每个管道阶段进行定制，特别是利用结构信息来提高检索和重排序的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning Algorithms for Nash Equilibria</div>
<div class="meta-line">Authors: Anas Barakat, Ioannis Panageas, Antonios Varvitsiotis</div>
<div class="meta-line">First: 2026-02-12T17:11:20+00:00 · Latest: 2026-02-12T17:11:20+00:00</div>
<div class="meta-line">Comments: AISTATS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Convex Markov Games (cMGs) were recently introduced as a broad class of multi-agent learning problems that generalize Markov games to settings where strategic agents optimize general utilities beyond additive rewards. While cMGs expand the modeling frontier, their theoretical foundations, particularly the structure of Nash equilibria (NE) and guarantees for learning algorithms, are not yet well understood. In this work, we address these gaps for an extension of cMGs, which we term General Utility Markov Games (GUMGs), capturing new applications requiring coupling between agents&#x27; occupancy measures. We prove that in GUMGs, Nash equilibria coincide with the fixed points of projected pseudo-gradient dynamics (i.e., first-order stationary points), enabled by a novel agent-wise gradient domination property. This insight also yields a simple proof of NE existence using Brouwer&#x27;s fixed-point theorem. We further show the existence of Markov perfect equilibria. Building on this characterization, we establish a policy gradient theorem for GUMGs and design a model-free policy gradient algorithm. For potential GUMGs, we establish iteration complexity guarantees for computing approximate-NE under exact gradients and provide sample complexity bounds in both the generative model and on-policy settings. Our results extend beyond prior work restricted to zero-sum cMGs, providing the first theoretical analysis of common-interest cMGs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>凸马尔可夫博弈及其扩展：新的纳什均衡存在性、特征化和学习算法证明</div>
<div class="mono" style="margin-top:8px">凸马尔可夫博弈(cMGs)最近被引入为一类多智能体学习问题，它将马尔可夫博弈推广到智能体优化超越加性奖励的一般效用的设置中。虽然cMGs扩展了建模的边界，但它们的理论基础，特别是纳什均衡(NE)的结构和学习算法的保证，尚未得到充分理解。在本文中，我们针对cMGs的一个扩展进行了研究，将其称为通用效用马尔可夫博弈(GUMGs)，以捕捉需要智能体占用度量耦合的新应用。我们证明在GUMGs中，纳什均衡与投影伪梯度动力学的不动点（即一阶稳定点）重合，这得益于一种新的智能体梯度支配性质。这一洞察还使用布劳威尔不动点定理提供了一个简单的NE存在性证明。我们进一步证明了马尔可夫完美均衡的存在性。基于这一特征化，我们为GUMGs建立了策略梯度定理，并设计了一种无模型策略梯度算法。对于潜在的GUMGs，我们建立了在精确梯度下计算近似NE的迭代复杂度保证，并在生成模型和在线策略设置中提供了样本复杂度界。我们的结果超越了仅限于零和cMGs的先前工作，提供了对共同利益cMGs的第一个理论分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the theoretical foundations of General Utility Markov Games (GUMGs), a class of multi-agent learning problems that generalize Markov games. It proves that Nash equilibria in GUMGs are equivalent to the fixed points of projected pseudo-gradient dynamics, leveraging a novel gradient domination property. The work also establishes the existence of Markov perfect equilibria and provides a policy gradient theorem, leading to a model-free algorithm. Additionally, it offers iteration and sample complexity guarantees for computing approximate Nash equilibria in both exact and approximate gradient settings, extending previous analyses to common-interest games beyond zero-sum scenarios.</div>
<div class="mono" style="margin-top:8px">本文研究了广义效用马尔可夫博弈（GUMGs），将马尔可夫博弈推广到具有耦合效用的设置。证明了GUMGs中的纳什均衡等同于投影伪梯度动力学的固定点，利用了一个新的梯度支配性质。工作还建立了马尔可夫完美均衡的存在性，并提供了一个策略梯度定理，从而导致了一个无模型的策略梯度算法。此外，它还为在精确梯度和近似梯度设置下计算近似纳什均衡提供了迭代和样本复杂度保证，超越了之前的零和博弈分析，扩展到了共同利益场景。</div>
</details>
</div>
<div class="card">
<div class="title">How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics</div>
<div class="meta-line">Authors: Yurong Chen, Yu He, Michael I. Jordan, Fan Yao</div>
<div class="meta-line">First: 2026-02-12T17:11:08+00:00 · Latest: 2026-02-12T17:11:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12180v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12180v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard methods for aligning large language models with human preferences learn from pairwise comparisons among sampled candidate responses and regularize toward a reference policy. Despite their effectiveness, the effects of sampling and reference choices are poorly understood theoretically. We investigate these effects through Identity Preference Optimization, a widely used preference alignment framework, and show that proper instance-dependent sampling can yield stronger ranking guarantees, while skewed on-policy sampling can induce excessive concentration under structured preferences. We then analyze iterative alignment dynamics in which the learned policy feeds back into future sampling and reference policies, reflecting a common practice of model-generated preference data. We prove that these dynamics can exhibit persistent oscillations or entropy collapse for certain parameter choices, and characterize regimes that guarantee stability. Our theoretical insights extend to Direct Preference Optimization, indicating the phenomena we captured are common to a broader class of preference-alignment methods. Experiments on real-world preference data validate our findings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>采样如何塑造LLM对齐：从单次优化到迭代动力学</div>
<div class="mono" style="margin-top:8px">用于使大型语言模型与人类偏好对齐的标准方法通过采样候选响应的成对比较进行学习，并向参考策略进行正则化。尽管这些方法非常有效，但采样和参考选择的影响在理论上知之甚少。我们通过广泛使用的偏好对齐框架Identity Preference Optimization来研究这些影响，并表明适当的实例相关采样可以提供更强的排名保证，而偏斜的在策略采样在结构化偏好下会导致过度集中。然后我们分析了学习策略反馈到未来采样和参考策略中的迭代对齐动力学，这反映了模型生成偏好数据的常见做法。我们证明了在某些参数选择下，这些动力学可以表现出持久的振荡或熵崩溃，并确定了保证稳定性的区域。我们的理论见解扩展到直接偏好优化，表明我们捕捉到的现象适用于更广泛的偏好对齐方法类别。在真实偏好数据上的实验验证了我们的发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper investigates how sampling affects the alignment of large language models with human preferences, using Identity Preference Optimization as a framework. It shows that proper sampling can improve ranking guarantees, while skewed sampling can lead to excessive concentration. The study also analyzes iterative alignment dynamics, proving that these can result in persistent oscillations or entropy collapse under certain conditions, and characterizes stable regimes. Experiments confirm these theoretical findings.</div>
<div class="mono" style="margin-top:8px">研究探讨了采样和参考选择如何影响大型语言模型与人类偏好的一致性。通过分析Identity Preference Optimization，作者表明适当的采样可以提高排名保证，而偏差的采样可能导致过度集中。研究还考察了迭代对齐动态，证明在某些条件下，这些动态会导致持续振荡或熵崩溃。实验结果表明，这些发现适用于更广泛的偏好对齐方法类别。</div>
</details>
</div>
<div class="card">
<div class="title">EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data</div>
<div class="meta-line">Authors: Nils Lehmann, Yi Wang, Zhitong Xiong, Xiaoxiang Zhu</div>
<div class="meta-line">First: 2026-02-12T17:09:14+00:00 · Latest: 2026-02-12T17:09:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12177v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12177v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art generative image and video models rely heavily on tokenizers that compress high-dimensional inputs into more efficient latent representations. While this paradigm has revolutionized RGB generation, Earth observation (EO) data presents unique challenges due to diverse sensor specifications and variable spectral channels. We propose EO-VAE, a multi-sensor variational autoencoder designed to serve as a foundational tokenizer for the EO domain. Unlike prior approaches that train separate tokenizers for each modality, EO-VAE utilizes a single model to encode and reconstruct flexible channel combinations via dynamic hypernetworks. Our experiments on the TerraMesh dataset demonstrate that EO-VAE achieves superior reconstruction fidelity compared to the TerraMind tokenizers, establishing a robust baseline for latent generative modeling in remote sensing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EO-VAE：向地球观测数据多传感器分词器迈进</div>
<div class="mono" style="margin-top:8px">当前最先进的生成图像和视频模型严重依赖于将高维输入压缩为更高效的潜在表示的分词器。虽然这种范式已经彻底改变了RGB生成，但由于多样化的传感器规格和变化的光谱通道，地球观测（EO）数据提出了独特的挑战。我们提出了EO-VAE，这是一种多传感器变分自编码器，旨在作为EO领域的基础分词器。与先前的方法不同，EO-VAE 使用单一模型通过动态超网络来编码和重构灵活的通道组合。我们在TerraMesh数据集上的实验表明，EO-VAE 在重构保真度方面优于TerraMind分词器，为遥感领域的潜在生成建模奠定了稳健的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the unique challenges of Earth observation (EO) data by proposing EO-VAE, a multi-sensor variational autoencoder designed as a tokenizer for EO data. Unlike previous methods that train separate tokenizers for different modalities, EO-VAE uses a single model with dynamic hypernetworks to encode and reconstruct various channel combinations. Experiments on the TerraMesh dataset show that EO-VAE outperforms TerraMind tokenizers in terms of reconstruction fidelity, setting a strong foundation for latent generative modeling in remote sensing.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出EO-VAE，一种多传感器变分自编码器，解决地球观测（EO）数据的独特挑战，作为EO数据的分词器。与之前的方法不同，EO-VAE 使用一个模型和动态超网络来编码和重建各种通道组合。实验结果表明，EO-VAE 在 TerraMesh 数据集上的重构保真度优于 TerraMind 分词器，为遥感领域的潜在生成建模奠定了坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Deep learning Based Correction Algorithms for 3D Medical Reconstruction in Computed Tomography and Macroscopic Imaging</div>
<div class="meta-line">Authors: Tomasz Les, Tomasz Markiewicz, Malgorzata Lorent, Miroslaw Dziekiewicz, Krzysztof Siwek</div>
<div class="meta-line">First: 2026-01-30T17:16:17+00:00 · Latest: 2026-02-12T17:06:54+00:00</div>
<div class="meta-line">Comments: 23 pages, 9 figures, submitted to Applied Sciences (MDPI)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00220v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00220v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a hybrid two-stage registration framework for reconstructing three-dimensional (3D) kidney anatomy from macroscopic slices, using CT-derived models as the geometric reference standard. The approach addresses the data-scarcity and high-distortion challenges typical of macroscopic imaging, where fully learning-based registration (e.g., VoxelMorph) often fails to generalize due to limited training diversity and large nonrigid deformations that exceed the capture range of unconstrained convolutional filters. In the proposed pipeline, the Optimal Cross-section Matching (OCM) algorithm first performs constrained global alignment: translation, rotation, and uniform scaling to establish anatomically consistent slice initialization. Next, a lightweight deep-learning refinement network, inspired by VoxelMorph, predicts residual local deformations between consecutive slices. The core novelty of this architecture lies in its hierarchical decomposition of the registration manifold. This hybrid OCM+DL design integrates explicit geometric priors with the flexible learning capacity of neural networks, ensuring stable optimization and plausible deformation fields even with few training examples. Experiments on an original dataset of 40 kidneys demonstrated better results compared to single-stage baselines. The pipeline maintains physical calibration via Hough-based grid detection and employs Bezier-based contour smoothing for robust meshing and volume estimation. Although validated on kidney data, the proposed framework generalizes to other soft-tissue organs reconstructed from optical or photographic cross-sections. By decoupling interpretable global optimization from data-efficient deep refinement, the method advances the precision, reproducibility, and anatomical realism of multimodal 3D reconstructions for surgical planning, morphological assessment, and medical education.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度学习的3D医学重建矫正算法在计算机断层扫描和宏观成像中的应用</div>
<div class="mono" style="margin-top:8px">本文介绍了一种用于从宏观切片重建三维（3D）肾脏解剖结构的混合两阶段注册框架，使用CT衍生模型作为几何参考标准。该方法解决了宏观成像中常见的数据稀缺性和高失真挑战，其中完全基于学习的注册（例如VoxelMorph）由于训练多样性有限和非刚性变形超出未约束卷积滤波器的捕捉范围而难以泛化。在所提出的流水线中，Optimal Cross-section Matching (OCM) 算法首先执行约束全局对齐：平移、旋转和均匀缩放，以建立解剖上一致的切片初始化。接下来，一个轻量级的深度学习精炼网络，受到VoxelMorph的启发，预测连续切片之间的残余局部变形。该架构的核心新颖性在于其分层分解注册流形。这种混合OCM+DL设计结合了显式的几何先验与神经网络的灵活学习能力，即使在少量训练示例的情况下也能确保稳定的优化和合理的变形场。在40个肾脏的原始数据集上进行的实验表明，与单阶段基线相比，结果更好。该流水线通过基于Hough的网格检测保持物理校准，并使用Bezier基的轮廓平滑进行稳健的网格化和体积估计。尽管在肾脏数据上进行了验证，但所提出的框架可以推广到从光学或摄影切片重建的其他软组织器官。通过将可解释的全局优化与数据高效的深度精炼解耦，该方法推进了多模态3D重建的精度、可重复性和解剖真实性，用于手术规划、形态评估和医学教育。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a hybrid two-stage registration framework for reconstructing 3D kidney anatomy from macroscopic slices using CT-derived models as a reference. The method addresses the challenges of data scarcity and high distortion in macroscopic imaging by combining an Optimal Cross-section Matching (OCM) algorithm for constrained global alignment with a lightweight deep-learning refinement network for local deformations. Experiments on 40 kidneys showed improved results compared to single-stage baselines, demonstrating the method&#x27;s effectiveness in maintaining physical calibration and ensuring plausible deformation fields with limited training data.</div>
<div class="mono" style="margin-top:8px">本文提出了一种用于从宏观切片重建3D肾脏解剖结构的混合两阶段注册框架，使用CT衍生模型作为参考。该方法通过结合Optimal Cross-section Matching (OCM)算法进行约束全局对齐和轻量级深度学习精修网络，解决了数据稀缺性和高失真问题。实验结果表明，该方法在40个肾脏上的表现优于单阶段基线，同时保持了物理校准和稳健的网格生成。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260215_0332.html">20260215_0332</a>
<a href="archive/20260213_0402.html">20260213_0402</a>
<a href="archive/20260212_0404.html">20260212_0404</a>
<a href="archive/20260211_0409.html">20260211_0409</a>
<a href="archive/20260210_0409.html">20260210_0409</a>
<a href="archive/20260208_0334.html">20260208_0334</a>
<a href="archive/20260207_0349.html">20260207_0349</a>
<a href="archive/20260206_0347.html">20260206_0347</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0352.html">20260204_0352</a>
<a href="archive/20260202_0332.html">20260202_0332</a>
<a href="archive/20260201_0328.html">20260201_0328</a>
<a href="archive/20260131_0341.html">20260131_0341</a>
<a href="archive/20260130_0339.html">20260130_0339</a>
<a href="archive/20260129_0337.html">20260129_0337</a>
<a href="archive/20260128_0335.html">20260128_0335</a>
<a href="archive/20260127_0332.html">20260127_0332</a>
<a href="archive/20260126_0325.html">20260126_0325</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0333.html">20260124_0333</a>
<a href="archive/20260123_0333.html">20260123_0333</a>
<a href="archive/20260122_0339.html">20260122_0339</a>
<a href="archive/20260121_0422.html">20260121_0422</a>
<a href="archive/20260120_0328.html">20260120_0328</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_0324.html">20260118_0324</a>
<a href="archive/20260117_0329.html">20260117_0329</a>
<a href="archive/20260116_0332.html">20260116_0332</a>
<a href="archive/20260115_0330.html">20260115_0330</a>
<a href="archive/20260114_0329.html">20260114_0329</a>
<a href="archive/20260113_0330.html">20260113_0330</a>
<a href="archive/20260112_0330.html">20260112_0330</a>
<a href="archive/20260111_0327.html">20260111_0327</a>
<a href="archive/20260110_0328.html">20260110_0328</a>
<a href="archive/20260109_0331.html">20260109_0331</a>
<a href="archive/20260108_0330.html">20260108_0330</a>
<a href="archive/20260107_0325.html">20260107_0325</a>
<a href="archive/20260106_0331.html">20260106_0331</a>
<a href="archive/20260105_0324.html">20260105_0324</a>
<a href="archive/20260104_0324.html">20260104_0324</a>
<a href="archive/20260103_0322.html">20260103_0322</a>
<a href="archive/20260102_0335.html">20260102_0335</a>
<a href="archive/20260101_0325.html">20260101_0325</a>
<a href="archive/20251231_0331.html">20251231_0331</a>
<a href="archive/20251230_0328.html">20251230_0328</a>
<a href="archive/20251229_0326.html">20251229_0326</a>
<a href="archive/20251228_0329.html">20251228_0329</a>
<a href="archive/20251227_0325.html">20251227_0325</a>
<a href="archive/20251226_0326.html">20251226_0326</a>
<a href="archive/20251225_0325.html">20251225_0325</a>
<a href="archive/20251224_0328.html">20251224_0328</a>
<a href="archive/20251223_0327.html">20251223_0327</a>
<a href="archive/20251222_0324.html">20251222_0324</a>
<a href="archive/20251221_0326.html">20251221_0326</a>
<a href="archive/20251220_0327.html">20251220_0327</a>
<a href="archive/20251219_0327.html">20251219_0327</a>
<a href="archive/20251218_0339.html">20251218_0339</a>
<a href="archive/20251217_0331.html">20251217_0331</a>
<a href="archive/20251216_0329.html">20251216_0329</a>
<a href="archive/20251215_0331.html">20251215_0331</a>
<a href="archive/20251214_0324.html">20251214_0324</a>
<a href="archive/20251213_0324.html">20251213_0324</a>
<a href="archive/20251212_0329.html">20251212_0329</a>
<a href="archive/20251211_0326.html">20251211_0326</a>
<a href="archive/20251210_0323.html">20251210_0323</a>
<a href="archive/20251209_0326.html">20251209_0326</a>
<a href="archive/20251208_0324.html">20251208_0324</a>
<a href="archive/20251207_0323.html">20251207_0323</a>
<a href="archive/20251206_0325.html">20251206_0325</a>
<a href="archive/20251205_0326.html">20251205_0326</a>
<a href="archive/20251204_0326.html">20251204_0326</a>
<a href="archive/20251203_0328.html">20251203_0328</a>
<a href="archive/20251202_0331.html">20251202_0331</a>
<a href="archive/20251201_0324.html">20251201_0324</a>
<a href="archive/20251130_0323.html">20251130_0323</a>
<a href="archive/20251129_0323.html">20251129_0323</a>
<a href="archive/20251128_0324.html">20251128_0324</a>
<a href="archive/20251127_0324.html">20251127_0324</a>
<a href="archive/20251126_0325.html">20251126_0325</a>
<a href="archive/20251125_0322.html">20251125_0322</a>
<a href="archive/20251124_0323.html">20251124_0323</a>
<a href="archive/20251123_0323.html">20251123_0323</a>
<a href="archive/20251122_0325.html">20251122_0325</a>
<a href="archive/20251121_0324.html">20251121_0324</a>
<a href="archive/20251120_0326.html">20251120_0326</a>
<a href="archive/20251119_0325.html">20251119_0325</a>
<a href="archive/20251118_0324.html">20251118_0324</a>
<a href="archive/20251117_0322.html">20251117_0322</a>
<a href="archive/20251116_0322.html">20251116_0322</a>
<a href="archive/20251115_0324.html">20251115_0324</a>
<a href="archive/20251114_0325.html">20251114_0325</a>
<a href="archive/20251113_0326.html">20251113_0326</a>
<a href="archive/20251112_0326.html">20251112_0326</a>
<a href="archive/20251111_0318.html">20251111_0318</a>
<a href="archive/20251110_0322.html">20251110_0322</a>
<a href="archive/20251109_0323.html">20251109_0323</a>
<a href="archive/20251108_0321.html">20251108_0321</a>
<a href="archive/20251107_0320.html">20251107_0320</a>
<a href="archive/20251106_0322.html">20251106_0322</a>
<a href="archive/20251105_0321.html">20251105_0321</a>
<a href="archive/20251104_0324.html">20251104_0324</a>
<a href="archive/20251103_0317.html">20251103_0317</a>
<a href="archive/20251102_0321.html">20251102_0321</a>
<a href="archive/20251101_0317.html">20251101_0317</a>
<a href="archive/20251031_0318.html">20251031_0318</a>
<a href="archive/20251030_0328.html">20251030_0328</a>
<a href="archive/20251029_0325.html">20251029_0325</a>
<a href="archive/20251028_0324.html">20251028_0324</a>
<a href="archive/20251027_0320.html">20251027_0320</a>
<a href="archive/20251026_0328.html">20251026_0328</a>
<a href="archive/20251025_0320.html">20251025_0320</a>
<a href="archive/20251024_0328.html">20251024_0328</a>
<a href="archive/20251023_1235.html">20251023_1235</a>
<a href="archive/20251023_0316.html">20251023_0316</a>
<a href="archive/20251022_0319.html">20251022_0319</a>
<a href="archive/20251021_1916.html">20251021_1916</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
