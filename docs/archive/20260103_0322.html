<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-03 03:22</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260103_0322</div>
    <div class="row"><div class="card">
<div class="title">SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</div>
<div class="meta-line">Authors: Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang</div>
<div class="meta-line">First: 2025-12-31T18:59:57+00:00 · Latest: 2025-12-31T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25075v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25075v1">PDF</a> · <a href="https://github.com/ZheningHuang/spacetimepilot">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zheninghuang.github.io/Space-Time-Pilot/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video&#x27;s motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpaceTimePilot：时空分离的动态场景生成渲染</div>
<div class="mono" style="margin-top:8px">我们提出了SpaceTimePilot，一种视频扩散模型，能够分离空间和时间以实现可控的生成渲染。给定单目视频，SpaceTimePilot可以在生成过程中独立改变摄像机视角和运动序列，重新渲染场景，实现空间和时间上的连续和任意探索。为此，我们在扩散过程中引入了有效的动画时间嵌入机制，允许对输出视频的运动序列进行显式控制，相对于源视频。由于现有数据集中没有提供同一动态场景的配对视频以连续的时间变化，我们提出了一种简单而有效的时空扭曲训练方案，利用现有的多视角数据集模拟时间差异。该策略有效地监督模型学习时间控制并实现稳健的时空分离。为了进一步提高双重控制的精度，我们引入了两个额外组件：改进的摄像机条件机制，允许从第一帧开始改变摄像机，以及CamxTime，第一个合成时空全覆盖渲染数据集，提供了场景内的完全自由时空视频轨迹。在时空扭曲方案和CamxTime数据集上的联合训练产生了更精确的时间控制。我们在真实世界和合成数据上评估了SpaceTimePilot，展示了清晰的时空分离和与先前工作相比的强劲结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SpaceTimePilot is a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, it can independently alter the camera viewpoint and motion sequence, enabling continuous exploration across space and time. The model uses an animation time-embedding mechanism and a temporal-warping training scheme to achieve temporal control and robust space-time disentanglement. Additional components improve the precision of dual control, and joint training on the temporal-warping scheme and CamxTime dataset enhances temporal control. Evaluations on real-world and synthetic data show clear space-time disentanglement and strong results compared to previous work.</div>
<div class="mono" style="margin-top:8px">SpaceTimePilot 是一种视频扩散模型，能够将空间和时间分离，实现可控的生成渲染。给定一个单目视频，它可以独立改变摄像机视角和运动序列，从而在空间和时间上进行连续探索。该模型使用动画时间嵌入机制和时间扭曲训练方案来实现时间控制和空间-时间分离。此外，还引入了改进的摄像机条件机制和 CamxTime 数据集，进一步提高了双控的精度。在真实世界和合成数据上的实验结果表明，该模型具有清晰的空间-时间分离和比之前方法更强的性能。</div>
</details>
</div>
<div class="card">
<div class="title">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</div>
<div class="meta-line">Authors: Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu</div>
<div class="meta-line">First: 2025-12-31T18:59:55+00:00 · Latest: 2025-12-31T18:59:55+00:00</div>
<div class="meta-line">Comments: Project page: https://yichuanh.github.io/GaMO/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25073v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yichuanh.github.io/GaMO/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GaMO：基于几何的多视图扩散外延法用于稀疏视图三维重建</div>
<div class="mono" style="margin-top:8px">近年来，三维重建在从密集多视角图像中高质量捕捉场景方面取得了显著进展，但在输入视角有限时却面临挑战。各种方法，包括正则化技术、语义先验和几何约束，已被实施以应对这一挑战。最新的基于扩散的方法通过生成新的视角来增强训练数据，从而产生了显著的改进，超越了早期的正则化和基于先验的方法。尽管取得了这些进展，我们仍识别出这些最先进的方法存在三个关键限制：超出已知视图边缘的覆盖不足、生成视图之间的几何不一致以及计算成本高昂的管道。我们提出了GaMO（几何感知多视图外延器），一种通过多视图外延重新构建稀疏视图的框架。与生成新视角不同，GaMO 从现有相机姿态扩展视场，这本身就能保持几何一致性并提供更广泛的场景覆盖。我们的方法以零样本方式采用多视图条件和几何感知去噪策略，无需训练。在Replica和ScanNet++上的广泛实验表明，GaMO 在3、6和9个输入视图下的重建质量达到最先进的水平，PSNR和LPIPS均优于先前方法，同时比最先进的基于扩散的方法快25倍，处理时间少于10分钟。项目页面：https://yichuanh.github.io/GaMO/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve 3D reconstruction from limited input views by addressing the limitations of existing methods such as inadequate coverage, geometric inconsistencies, and computational inefficiency. GaMO, a geometry-aware multi-view outpainting framework, expands the field of view from existing camera poses, preserving geometric consistency and enhancing scene coverage. Experiments on Replica and ScanNet++ show that GaMO outperforms prior methods in PSNR and LPIPS with a $25\times$ speedup over state-of-the-art diffusion-based methods, achieving reconstruction quality across 3, 6, and 9 input views.</div>
<div class="mono" style="margin-top:8px">研究通过引入GaMO，解决了从有限输入视角进行3D重建的挑战，该方法从现有相机姿态扩展视野，增强几何一致性和覆盖范围。GaMO未经过训练就使用多视角条件和几何感知去噪策略，其在Replica和ScanNet++上的重建质量优于先前方法，处理时间快了25倍。</div>
</details>
</div>
<div class="card">
<div class="title">Edit3r: Instant 3D Scene Editing from Sparse Unposed Images</div>
<div class="meta-line">Authors: Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2025-12-31T18:59:53+00:00 · Latest: 2025-12-31T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page: https://edit3r.github.io/edit3r/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25071v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25071v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://edit3r.github.io/edit3r/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Edit3r：从稀疏未对齐图像即时编辑3D场景</div>
<div class="mono" style="margin-top:8px">我们提出了Edit3r，这是一种单次通过框架，可以从未对齐、视角不一致、指令编辑过的图像中重建和编辑3D场景。与需要逐场景优化的先前方法不同，Edit3r可以直接预测指令对齐的3D编辑，从而实现快速且逼真的渲染，无需优化或姿态估计。训练此类模型的关键挑战在于缺乏多视角一致的编辑图像作为监督。我们通过(i)基于SAM2的重新着色策略生成可靠的、跨视角一致的监督，以及(ii)不对称输入策略，将重新着色的参考视图与原始辅助视图配对，鼓励网络融合和对齐不同的观察结果来解决这一问题。在推理时，我们的模型能够有效处理由2D方法（如InstructPix2Pix）编辑的图像，尽管在训练过程中并未接触到此类编辑。为了进行大规模的定量评估，我们引入了DL3DV-Edit-Bench基准，该基准基于DL3DV测试集构建，包含20个多样化的场景、4种编辑类型和总共100次编辑。全面的定量和定性结果表明，Edit3r在语义对齐和3D一致性方面优于最近的基线方法，同时具有显著更高的推理速度，使其在实时3D编辑应用中具有前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Edit3r is a feed-forward framework that reconstructs and edits 3D scenes from unposed, view-inconsistent images in a single pass. It directly predicts instruction-aligned 3D edits without requiring per-scene optimization, enabling fast and photorealistic rendering. Key to its training is a SAM2-based recoloring strategy for generating cross-view-consistent supervision and an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views. Quantitative and qualitative results demonstrate that Edit3r outperforms recent baselines in semantic alignment and 3D consistency, while operating at higher inference speed, making it suitable for real-time 3D editing applications.</div>
<div class="mono" style="margin-top:8px">Edit3r 是一个无需场景优化即可从不一致视角的未摆拍图像中重建和编辑 3D 场景的前馈框架。它直接预测指令对齐的 3D 编辑，无需逐场景优化，从而实现快速且逼真的渲染。训练的关键在于使用基于 SAM2 的重新着色策略生成跨视角一致的监督信息，以及将重新着色的参考视图与原始辅助视图配对的不对称输入策略。定量和定性结果表明，Edit3r 在语义对齐和 3D 一致性方面优于最近的基线模型，同时具有更高的推理速度，适用于实时 3D 编辑应用。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2025-12-31T18:59:51+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将开放性推理扩展以预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及对未来不确定性的推理。在本研究中，我们训练语言模型对开放性预测问题进行预测。为了扩大训练数据，我们从每日新闻中报道的全球事件中合成新型预测问题，使用完全自动化的仔细编纂配方。我们在OpenForesight数据集上训练Qwen3思考模型。为了防止训练和评估期间出现未来信息泄露，我们在数据生成和检索中使用离线新闻语料库。在一小部分验证集的指导下，我们展示了检索的好处以及强化学习（RL）中改进的奖励函数。一旦我们获得最终的预测系统，我们将在2025年5月至8月之间进行保留测试。我们的专门模型OpenForecaster 8B与更大规模的专有模型相当，我们的训练提高了预测的准确性、校准性和一致性。我们发现预测训练带来的校准改进在流行基准上具有普遍性。我们开源了所有模型、代码和数据，以使语言模型预测研究广泛可访问。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work aims to enhance language models for open-ended forecasting by synthesizing questions from daily news. The Qwen3 models are trained on the OpenForesight dataset, using an offline news corpus to avoid future information leakage. The study shows that retrieval and an improved reward function enhance the model&#x27;s performance. The OpenForecaster 8B model demonstrates competitive accuracy, calibration, and consistency, outperforming larger proprietary models. Calibration improvements generalize across benchmarks, and all resources are open-sourced for broader research access.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过从每日新闻中合成问题来增强语言模型的开放性预测能力。Qwen3模型在OpenForesight数据集上进行训练，使用离线新闻语料库以避免未来信息泄露。研究显示检索和改进的奖励函数提升了模型性能。OpenForecaster 8B模型展示了竞争力的准确度、校准性和一致性，超越了更大规模的专有模型。校准改进在基准测试中泛化，所有资源均已开源，以促进更广泛的科研访问。</div>
</details>
</div>
<div class="card">
<div class="title">FineTec: Fine-Grained Action Recognition Under Temporal Corruption via Skeleton Decomposition and Sequence Completion</div>
<div class="meta-line">Authors: Dian Shao, Mingfei Shi, Like Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-31T18:59:12+00:00 · Latest: 2025-12-31T18:59:12+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25067v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://smartdianlab.github.io/projects-FineTec/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recognizing fine-grained actions from temporally corrupted skeleton sequences remains a significant challenge, particularly in real-world scenarios where online pose estimation often yields substantial missing data. Existing methods often struggle to accurately recover temporal dynamics and fine-grained spatial structures, resulting in the loss of subtle motion cues crucial for distinguishing similar actions. To address this, we propose FineTec, a unified framework for Fine-grained action recognition under Temporal Corruption. FineTec first restores a base skeleton sequence from corrupted input using context-aware completion with diverse temporal masking. Next, a skeleton-based spatial decomposition module partitions the skeleton into five semantic regions, further divides them into dynamic and static subgroups based on motion variance, and generates two augmented skeleton sequences via targeted perturbation. These, along with the base sequence, are then processed by a physics-driven estimation module, which utilizes Lagrangian dynamics to estimate joint accelerations. Finally, both the fused skeleton position sequence and the fused acceleration sequence are jointly fed into a GCN-based action recognition head. Extensive experiments on both coarse-grained (NTU-60, NTU-120) and fine-grained (Gym99, Gym288) benchmarks show that FineTec significantly outperforms previous methods under various levels of temporal corruption. Specifically, FineTec achieves top-1 accuracies of 89.1% and 78.1% on the challenging Gym99-severe and Gym288-severe settings, respectively, demonstrating its robustness and generalizability. Code and datasets could be found at https://smartdianlab.github.io/projects-FineTec/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FineTec：通过骨架分解和序列完成在时间腐蚀下的细粒度动作识别</div>
<div class="mono" style="margin-top:8px">从时间腐蚀的骨架序列中识别细粒度动作仍然是一个重大挑战，尤其是在现实场景中，实时姿态估计经常产生大量缺失数据。现有方法往往难以准确恢复时间动态和细粒度的空间结构，导致丢失了区分相似动作的关键微动线索。为了解决这一问题，我们提出了一种名为FineTec的统一框架，用于在时间腐蚀下的细粒度动作识别。FineTec首先使用具有多样时间掩码的上下文感知完成从受腐蚀输入中恢复基础骨架序列。接着，一个基于骨架的空间分解模块将骨架划分为五个语义区域，并根据运动方差进一步划分为动态和静态子组，通过目标扰动生成两个增强的骨架序列。这些序列与基础序列一起，通过一个基于拉格朗日动力学的物理驱动估计模块进行处理，该模块利用拉格朗日动力学估计关节加速度。最后，融合后的骨架位置序列和融合后的加速度序列一起输入到基于GCN的动作识别头部。在粗粒度（NTU-60, NTU-120）和细粒度（Gym99, Gym288）基准上的广泛实验表明，FineTec在各种时间腐蚀水平下显著优于先前的方法。具体而言，FineTec在具有挑战性的Gym99-严重和Gym288-严重设置中分别实现了89.1%和78.1%的顶级准确率，展示了其鲁棒性和泛化能力。代码和数据集可在https://smartdianlab.github.io/projects-FineTec/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FineTec is a unified framework for fine-grained action recognition under temporal corruption. It first restores a base skeleton sequence using context-aware completion, then decomposes the skeleton into dynamic and static subgroups and generates augmented sequences. These sequences are processed by a physics-driven estimation module to estimate joint accelerations, which are then fed into an action recognition head. Experiments show that FineTec outperforms previous methods, achieving top-1 accuracies of 89.1% and 78.1% on Gym99-severe and Gym288-severe settings, respectively.</div>
<div class="mono" style="margin-top:8px">FineTec 是一种针对时间失真的细粒度动作识别统一框架。它首先使用上下文感知的完成方法恢复基骨架序列，然后将骨架分解为动态和静态子组并生成增强序列。这些序列通过物理驱动的估计模块估计关节加速度，然后输入到动作识别头部。实验表明，FineTec 在 Gym99-severe 和 Gym288-severe 设置中分别达到了 89.1% 和 78.1% 的 top-1 准确率，优于先前的方法。</div>
</details>
</div>
<div class="card">
<div class="title">From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing</div>
<div class="meta-line">Authors: Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu</div>
<div class="meta-line">First: 2025-12-31T18:58:30+00:00 · Latest: 2025-12-31T18:58:30+00:00</div>
<div class="meta-line">Comments: Project Page https://hjrphoebus.github.io/X-Dub</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25066v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25066v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hjrphoebus.github.io/X-Dub">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-driven visual dubbing aims to synchronize a video&#x27;s lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject&#x27;s lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从修复到编辑：一种基于上下文的视觉配音自强化框架</div>
<div class="mono" style="margin-top:8px">基于音频的视觉配音旨在使视频的唇部动作与新语音同步，但根本上受到理想训练数据的挑战：仅唇部动作不同而其他视觉条件完全相同的配对视频。现有方法通过基于掩码的修复范式绕过了这一问题，不完整的视觉条件迫使模型同时生成缺失的内容并同步唇部，导致视觉伪影、身份漂移和同步不良。在本文中，我们提出了一种新颖的自强化框架，将视觉配音重新定义为一个从病态的修复任务到一个条件良好的视频到视频编辑问题。我们的方法首先使用扩散变换器作为数据生成器，合成理想的训练数据：每个真实样本的唇部修改同伴视频，形成视觉对齐的视频对。然后，基于扩散变换器的音频驱动编辑器在这些对上端到端训练，利用完整的对齐输入视频帧专注于精确的音频驱动唇部修改。这种完整的、帧对齐的输入条件为编辑器提供了丰富的视觉上下文，提供了完整的身份线索、场景交互和连续的空间-时间动态。利用这种丰富的上下文，我们的方法能够实现高度准确的唇部同步、忠实的身份保留和对复杂野外场景的出色鲁棒性。我们还引入了一种时间步长自适应多阶段学习策略，作为必要组件以在扩散时间步长中分离相互冲突的编辑目标，从而促进稳定训练并获得增强的唇部同步和视觉保真度。此外，我们提出了ContextDubBench，一个全面的基准数据集，用于在多样且具有挑战性的实际应用场景中进行稳健评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of audio-driven visual dubbing by proposing a self-bootstrapping framework that transforms the task from an ill-posed inpainting problem into a well-conditioned video-to-video editing problem. The framework uses a Diffusion Transformer to generate ideal training data and then trains an audio-driven editor on these pairs. Key findings include highly accurate lip synchronization, faithful identity preservation, and robustness in challenging scenarios.</div>
<div class="mono" style="margin-top:8px">研究旨在通过解决理想训练数据缺乏的问题来改进音频驱动的视觉配音。提出的自提升框架使用扩散变换器生成理想训练数据，然后用于训练音频驱动的编辑器。这种方法提供了丰富的视觉上下文，从而实现了高精度的唇同步、忠实的身份保留以及在挑战性场景中的稳健性能。</div>
</details>
</div>
<div class="card">
<div class="title">Many Minds from One Model: Bayesian Transformers for Population Intelligence</div>
<div class="meta-line">Authors: Diji Yang, Yi Zhang</div>
<div class="meta-line">First: 2025-12-31T18:56:02+00:00 · Latest: 2025-12-31T18:56:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25063v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.
  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个模型多个心智：贝叶斯变换器与群体智能</div>
<div class="mono" style="margin-top:8px">尽管现代变换器规模庞大且取得巨大成功，但它们几乎无一例外地被训练为单一目标系统：优化产生一组确定性的参数，代表对数据的单一功能假设。受智能源自多个心智的想法启发，我们提出了群体贝叶斯变换器（B-Trans），将标准大型语言模型转换为贝叶斯变换器模型，以支持从一组预训练权重中采样多样且连贯的模型实例。
B-Trans 通过将归一化层中的偏置类似偏移视为具有高斯变分近似的随机变量，引入了一个贝叶斯动机的后验代理，从而在不训练完整的贝叶斯神经网络的情况下诱导模型行为的分布。从这个代理中采样产生一组具有不同行为但保持一般能力的模型实例。为了在每次生成中保持连贯性，我们在序列级别冻结采样的噪声，确保在各个标记之间的时间一致性。B-Trans 允许群体级别的决策，其中跨采样个体汇总预测显著增强了探索性。在零样本生成、具有可验证奖励的强化学习（RLVR）以及无需显式标签的强化学习实验中，B-Trans 有效地利用了群体的智慧，提供了更好的语义多样性，同时在任务性能上优于确定性基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper proposes Population Bayesian Transformers (B-Trans) to address the limitation of single-minded transformers by introducing a Bayesian approach to generate diverse yet coherent model instances from a single set of pre-trained weights. B-Trans treats bias-like offsets in normalization layers as stochastic variables and uses a Gaussian variational approximation to induce a distribution over model behavior. Experiments show that B-Trans enhances semantic diversity and task performance in zero-shot generation, Reinforcement Learning with Verifiable Rewards, and reinforcement learning without explicit labels, outperforming deterministic baselines.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出Population Bayesian Transformers (B-Trans)，探索智能可以从多种视角涌现。该方法将归一化层中的偏置项视为随机变量，并使用高斯变分近似来诱导模型行为的分布。实验表明，B-Trans 在零样本生成、具有可验证奖励的强化学习（RLVR）和无显式标签的强化学习中，增强了语义多样性并优于确定性基线，表现出更好的任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings</div>
<div class="meta-line">Authors: Tianzhi He, Farrokh Jazizadeh</div>
<div class="meta-line">First: 2025-12-31T18:51:19+00:00 · Latest: 2025-12-31T18:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25055v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25055v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype&#x27;s performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能建筑能源管理系统中面向情境的大型语言模型（LLM）基AI代理</div>
<div class="mono" style="margin-top:8px">本研究提出了一种概念框架和原型评估，用于通过自然语言交互促进智能建筑中面向情境的能源管理的大型语言模型（LLM）基建筑能源管理系统（BEMS）AI代理。所提出的框架包括三个模块：感知（传感）、中央控制（大脑）和行动（执行和用户交互），形成一个闭环反馈回路，捕捉、分析和解释能源数据，以智能响应用户查询并管理连接的电器。通过利用LLM的自主数据分析能力，BEMS AI代理旨在提供有关能源消耗、成本预测和设备调度的面向情境的见解，从而解决现有能源管理系统中的局限性。原型的性能使用120个用户查询和四个不同的真实住宅能源数据集以及不同的评估指标（包括延迟、功能、能力、准确性和成本效益）进行了评估。通过ANOVA测试展示了该框架的普适性。结果表明，通过设备控制（86%）、记忆相关任务（97%）、调度和自动化（74%）和能源分析（77%）的响应准确性衡量，性能表现出色，而更复杂的成本估算任务则指出了改进的领域，准确率为49%。这项基准研究朝着正式评估LLM基BEMS AI代理并确定未来研究方向的方向迈进，强调了响应准确性和计算效率之间的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study introduces a conceptual framework and prototype for LLM-based BEMS AI agents to enhance context-aware energy management in smart buildings via natural language interaction. The framework includes perception, central control, and action modules, forming a closed loop for energy data analysis and response to user queries. Performance was evaluated using 120 user queries across four residential datasets, with results showing high accuracy in memory-related tasks (97%) and device control (86%), but lower accuracy in cost estimation (49%).</div>
<div class="mono" style="margin-top:8px">本研究提出了一种概念框架和原型，旨在通过自然语言交互增强智能建筑的能源管理，采用基于大型语言模型（LLM）的建筑能源管理系统（BEMS）AI代理。该框架包括感知、中央控制和行动模块，形成一个闭环来分析和解释能源数据。原型使用了四个住宅能源数据集中的120个用户查询进行了评估，结果显示在设备控制（86%）、记忆相关任务（97%）、调度和自动化（74%）以及能源分析（77%）方面表现出色，但成本估算任务需要进一步改进，准确率为49%。ANOVA测试展示了该框架的普适性。本研究旨在规范基于LLM的BEMS AI代理的评估，并确定未来的研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Classifiers Avoid Shortcut Solutions</div>
<div class="meta-line">Authors: Alexander C. Li, Ananya Kumar, Deepak Pathak</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2025-12-31T18:31:46+00:00 · Latest: 2025-12-31T18:31:46+00:00</div>
<div class="meta-line">Comments: ICLR 2025. Code: https://github.com/alexlioralexli/generative-classifiers</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25034v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25034v1">PDF</a> · <a href="https://github.com/alexlioralexli/generative-classifiers">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式分类器避免捷径解决方案</div>
<div class="mono" style="margin-top:8px">分类中的判别方法往往学习在分布内有效但在轻微分布偏移时失效的捷径。这种失败模式源于对与标签偶然相关的特征的过度依赖。我们表明，使用条件生成模型的生成式分类器可以通过建模所有特征，而不是主要建模偶然特征，从而避免此问题。生成式分类器易于训练，无需特殊增强、强正则化、额外超参数或避免特定偶然相关性的知识。我们发现，基于扩散和自回归的生成式分类器在五个标准图像和文本分布偏移基准测试中达到最先进的性能，并在医疗或卫星数据集等现实应用中减少了偶然相关性的影响。最后，我们仔细分析了一个高斯玩具设置，以理解生成式分类器的归纳偏置，以及哪些数据属性决定了生成式分类器何时优于判别式分类器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the issue of shortcut solutions in discriminative classifiers, which can fail under distribution shifts. It proposes using generative classifiers that model all features, both core and spurious, to avoid this problem. Experiments show that generative classifiers, particularly those based on diffusion and autoregressive models, outperform discriminative classifiers on various benchmarks and reduce the impact of spurious correlations in real-world datasets. The study also provides insights into the inductive biases of generative classifiers and the conditions under which they perform better than discriminative classifiers.</div>
<div class="mono" style="margin-top:8px">论文探讨了判别分类器学习虚假相关性的问题，这些相关性在分布变化时会失效。它提出使用生成分类器来建模所有特征，避免学习捷径。实验表明，基于扩散和自回归模型的生成分类器在各种基准测试中表现优于判别分类器，并且在医疗和卫星图像等实际数据集中减少了虚假相关性的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Plan Verification for LLM-Based Embodied Task Completion Agents</div>
<div class="meta-line">Authors: Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-Tür, Gokhan Tur</div>
<div class="meta-line">First: 2025-09-02T19:06:56+00:00 · Latest: 2025-12-31T18:31:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02761v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.02761v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于LLM的具身任务完成代理计划验证</div>
<div class="mono" style="margin-top:8px">基于大型语言模型（LLM）的任务计划及其对应的具身AI人类示范可能是嘈杂的，包含不必要的动作、冗余导航和逻辑错误，这些都会降低策略质量。我们提出了一种迭代验证框架，在该框架中，一个法官LLM批判动作序列，一个规划LLM应用修订，从而逐步产生更清洁且更具空间连贯性的轨迹。与基于规则的方法不同，我们的方法依赖于自然语言提示，能够广泛泛化到各种错误类型，包括无关动作、矛盾和缺失步骤。在TEACh具身AI数据集中手动标注的动作集上，我们的框架在四个最先进的LLM（GPT o4-mini、DeepSeek-R1、Gemini 2.5、LLaMA 4 Scout）上实现了高达90%的召回率和100%的精确率。改进循环收敛迅速，96.5%的序列最多需要三轮迭代，同时提高了时间效率和空间动作组织。至关重要的是，该方法保留了人类错误恢复模式，而不是将其消除，支持未来关于鲁棒纠正行为的工作。通过将计划验证确立为空间规划和动作细化的可靠LLM能力，我们为具身AI中的模仿学习提供了可扩展的高质量训练数据路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of noisy task plans from large language models (LLMs) in embodied AI, proposing an iterative verification framework involving a Judge LLM and a Planner LLM to refine action sequences. The method uses natural language prompting to correct errors such as irrelevant actions, contradictions, and missing steps, achieving high recall and precision across multiple LLMs. The refinement loop converges quickly, improving temporal efficiency and spatial action organization while preserving human error-recovery patterns.</div>
<div class="mono" style="margin-top:8px">研究针对大型语言模型（LLMs）在 embodied AI 中生成的嘈杂任务计划问题，提出了一种迭代验证框架，该框架包括一个裁判 LLM 和一个规划 LLM 来细化行动序列。该方法使用自然语言提示来纠正无关动作、矛盾和缺失步骤等错误，多个 LLM 实现了高召回率和精确率。细化循环快速收敛，提高了时间效率和空间动作组织，同时保留了人类的错误恢复模式。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Generalisable Foundation Models for Brain MRI</div>
<div class="meta-line">Authors: Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander</div>
<div class="meta-line">First: 2025-10-27T15:19:46+00:00 · Latest: 2025-12-31T18:26:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23415v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23415v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anatomy by incorporating volumetric information from sequential MRI slices, moving beyond conventional single-slice paradigms. It supports both single- and multimodal inputs, enabling a broad range of downstream tasks, including disease detection and image segmentation, while generalising across varied imaging protocols and clinical scenarios. We show that BrainFound consistently outperforms existing self-supervised pretraining strategies and supervised baselines, particularly in label-scarce and multi-contrast settings. By integrating information from diverse 3D MRI modalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces dependency on extensive expert annotations. This flexibility makes BrainFound a scalable and practical solution for 3D neuroimaging pipelines, with significant potential for clinical deployment and research innovation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用基础模型在脑MRI中的应用</div>
<div class="mono" style="margin-top:8px">人工智能（AI）中的基础模型正在通过从大规模未标记数据集中学习通用特征来改变医学成像。在本研究中，我们介绍了BrainFound，这是一种用于脑MRI的自监督基础模型，通过扩展最初为2D自然图像设计的DINO-v2视觉变换器构建。BrainFound将DINO-v2适应于建模完整的3D脑解剖结构，通过纳入连续MRI切片的体素信息，超越了传统的单切片范式。它支持单模态和多模态输入，能够支持一系列下游任务，包括疾病检测和图像分割，同时在不同的成像协议和临床场景中具有泛化能力。我们表明，BrainFound在标签稀缺和多对比度设置中始终优于现有的自监督预训练策略和监督基线。通过整合多种3D MRI模态（如T1、T2、FLAIR）的信息，它提高了诊断准确性并减少了对大量专家注释的依赖。这种灵活性使BrainFound成为3D神经成像管道的可扩展和实用解决方案，具有在临床部署和研究创新方面的巨大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a generalisable foundation model for brain MRI by extending DINO-v2, a vision transformer, to handle 3D brain anatomy. BrainFound, the proposed model, incorporates volumetric information from sequential MRI slices and supports both single- and multimodal inputs, enhancing its applicability to various downstream tasks. Key findings show that BrainFound outperforms existing self-supervised pretraining strategies and supervised baselines, especially in label-scarce and multi-contrast settings, improving diagnostic accuracy and reducing the need for extensive expert annotations.</div>
<div class="mono" style="margin-top:8px">研究旨在通过将DINO-v2扩展到处理3D脑部解剖结构，开发一种通用的基础模型BrainFound。BrainFound整合了来自连续MRI切片的体积信息，并支持单模态和多模态输入，增强了其在各种下游任务中的适用性。关键发现表明，BrainFound在标签稀缺和多对比度设置中优于现有自监督预训练策略和监督基线，提高了诊断准确性并减少了对专家注释的依赖。</div>
</details>
</div>
<div class="card">
<div class="title">ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning</div>
<div class="meta-line">Authors: Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke Hüllermeier</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-31T18:21:52+00:00 · Latest: 2025-12-31T18:21:52+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25023v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResponseRank：通过偏好强度学习实现高效的数据利用奖励建模</div>
<div class="mono" style="margin-top:8px">二元选择，如强化学习从人类反馈（RLHF）中常用的方式，只能传达偏好的方向。一个人可能选择苹果而不是橙子，香蕉而不是葡萄，但哪种偏好更强烈？强度对于在不确定性下做决策和偏好模型的一般化至关重要，但很难可靠地测量。元数据如响应时间以及注释者间的一致性可以作为强度的代理，但往往噪声较大且混杂。我们提出ResponseRank以应对来自噪声强度信号的学习挑战。我们的方法使用代理信号的相对差异来对成对比较的响应进行排序，以推断其偏好强度。为了控制系统性变化，我们仅在精心构建的层内局部比较信号。这使得我们可以稳健地学习与强度推导出的排名一致的效用差异，同时对强度信号的假设最少。我们的贡献包括三个方面：(1) ResponseRank，一种新颖的方法，通过利用局部有效的相对强度信号稳健地学习偏好强度；(2) 在合成偏好学习（使用模拟响应时间）、语言建模（使用注释者一致性）和RL控制任务（使用模拟回合回报）等多样任务中，证明了改进的样本效率和稳健性；(3) Pearson距离相关性（PDC），一种新颖的度量标准，能够隔离序数准确性与基数效用学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ResponseRank is a method for learning preference strength from noisy signals, addressing the challenge of binary choices in reinforcement learning from human feedback. It ranks responses based on inferred preference strength using relative differences in proxy signals within carefully constructed strata, leading to improved sample efficiency and robustness across various tasks, including synthetic preference learning, language modeling, and RL control tasks.</div>
<div class="mono" style="margin-top:8px">ResponseRank 是一种从嘈杂信号中学习偏好强度的方法，解决从人类反馈进行强化学习的挑战。它通过相对差异的代理信号来排名响应，并通过在局部构造的层内比较信号来控制系统性变化。该方法在合成偏好学习、语言建模和RL控制任务等多种任务中展示了改进的样本效率和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</div>
<div class="meta-line">Authors: Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</div>
<div class="meta-line">First: 2025-12-31T17:57:45+00:00 · Latest: 2025-12-31T17:57:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25008v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FoundationSLAM：利用基础深度模型释放流基础方法的潜力以实现端到端密集视觉SLAM</div>
<div class="mono" style="margin-top:8px">我们提出了FoundationSLAM，一种基于学习的单目密集SLAM系统，解决了先前基于流的方法中缺乏几何一致性的问题，以实现准确和鲁棒的跟踪和建图。我们的核心思想是通过利用基础深度模型的指导，将流估计与几何推理相结合。为此，我们首先开发了一种混合流网络，生成几何感知的对应关系，使跨不同关键帧的一致深度和姿态推断成为可能。为了确保全局一致性，我们提出了一种双向一致束调整层，该层在多视图约束下联合优化关键帧姿态和深度。此外，我们引入了一种可靠性感知精化机制，通过区分可靠和不确定区域动态适应流更新过程，形成匹配与优化之间的闭环。广泛的实验表明，FoundationSLAM在多个具有挑战性的数据集上实现了卓越的轨迹精度和密集重建质量，同时以每秒18帧的速度实时运行，展示了我们方法在各种场景下的强大泛化能力和实际应用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FoundationSLAM is a learning-based monocular dense SLAM system that improves upon previous flow-based approaches by incorporating geometric consistency through foundation depth models. It uses a Hybrid Flow Network to generate geometry-aware correspondences and a Bi-Consistent Bundle Adjustment Layer to enforce global consistency. Additionally, it includes a Reliability-Aware Refinement mechanism to adapt the flow update process. Experiments show that FoundationSLAM provides superior trajectory accuracy and dense reconstruction quality, running at real-time speeds of 18 FPS across various datasets.</div>
<div class="mono" style="margin-top:8px">FoundationSLAM 是一种基于学习的单目密集 SLAM 系统，通过结合几何推理和使用基础深度模型的流估计来提高跟踪和建图的准确性和鲁棒性。它使用 Hybrid Flow Network 生成几何感知的对应关系，并使用 Bi-Consistent Bundle Adjustment Layer 来确保全局一致性。此外，它还引入了 Reliability-Aware Refinement 机制，以动态适应流更新过程。实验表明，FoundationSLAM 在多个挑战性数据集上实现了更好的轨迹准确性和密集重建质量，同时保持实时性能，每秒 18 帧。</div>
</details>
</div>
<div class="card">
<div class="title">Bi-C2R: Bidirectional Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification</div>
<div class="meta-line">Authors: Zhenyu Cui, Jiahuan Zhou, Yuxin Peng</div>
<div class="meta-line">First: 2025-12-31T17:50:05+00:00 · Latest: 2025-12-31T17:50:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25000v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as &quot;re-indexing&quot;. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. We verify our proposed Bi-C2R method through theoretical analysis and extensive experiments on multiple benchmarks, which demonstrate that the proposed method can achieve leading performance on both the introduced RFL-ReID task and the traditional L-ReID task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bi-C2R：双向持续兼容表示以实现无需再索引的终身行人重识别</div>
<div class="mono" style="margin-top:8px">终身行人重识别（L-ReID）利用按顺序收集的数据进行持续训练和更新重识别模型，关注所有数据的整体性能。其主要挑战是在训练新数据时避免旧知识的灾难性遗忘问题。现有L-ReID方法通常在每次更新后重新提取所有历史画廊图像的新特征进行推理，这被称为“再索引”。然而，由于数据隐私问题和大规模画廊图像的高再索引成本，历史画廊数据通常直接保存。因此，不可避免地导致更新模型提取的查询特征与更新前模型提取的画廊特征之间的不兼容检索，严重影响重识别性能。为了解决上述问题，本文关注一个名为无需再索引的终身行人重识别（RFL-ReID）的新任务，该任务要求在不重新索引历史画廊图像的情况下进行终身行人重识别。因此，RFL-ReID 比 L-ReID 更具挑战性，需要在多样化的流式数据中持续学习并平衡新旧知识，使新旧模型输出的特征相互兼容。为此，我们提出了一种双向持续兼容表示（Bi-C2R）框架，以兼容的方式持续更新旧模型提取的画廊特征，实现高效的L-ReID。我们通过理论分析和在多个基准上的广泛实验验证了我们提出的Bi-C2R方法，结果表明该方法在引入的RFL-ReID任务和传统L-ReID任务上均能实现领先性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of lifelong person re-identification (L-ReID) by proposing a new task called Re-index Free Lifelong person Re-identification (RFL-ReID), which aims to perform re-identification without re-indexing historical gallery images. To tackle the issue of catastrophic forgetting and incompatibility between query and gallery features, the authors introduce a Bidirectional Continuous Compatible Representation (Bi-C2R) framework. The Bi-C2R framework updates the gallery features extracted by the old model to ensure compatibility with features from the updated model. Experimental results on multiple benchmarks show that Bi-C2R outperforms existing methods on both RFL-ReID and traditional L-ReID tasks.</div>
<div class="mono" style="margin-top:8px">论文提出了一个新的任务——无需重新索引历史库图像的终身行人重识别（RFL-ReID），旨在无需重新索引历史库图像的情况下进行行人重识别。为此，作者提出了双向连续兼容表示（Bi-C2R）框架，该框架通过更新旧模型提取的库特征，确保与更新后模型提取的特征兼容。实验结果表明，Bi-C2R在RFL-ReID和传统L-ReID任务上均取得了领先性能。</div>
</details>
</div>
<div class="card">
<div class="title">Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis</div>
<div class="meta-line">Authors: Seunghoon Paik, Kangjie Zhou, Matus Telgarsky, Ryan J. Tibshirani</div>
<div class="meta-line">First: 2025-12-31T17:49:37+00:00 · Latest: 2025-12-31T17:49:37+00:00</div>
<div class="meta-line">Comments: 47 pages, 3 figures (7 subfigures)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24999v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24999v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $θ_0$ with current iterate $θ_T$, the basic inequality upper bounds $f(θ_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $θ_0$, $θ_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一阶优化的基本不等式及其在统计风险分析中的应用</div>
<div class="mono" style="margin-top:8px">我们引入了一阶迭代优化算法的\textit{基本不等式}，形成一个简单且多功能的框架，将隐式和显式正则化联系起来。虽然文献中存在相关的不等式，但我们隔离并强调了一种特定形式，并将其发展为统计分析中一个完善的工具。令$f$表示要优化的目标函数。给定一个初始于$θ_0$的一阶迭代算法，当前迭代为$θ_T$，基本不等式以累积步长和$θ_0$、$θ_T$与$z$之间的距离为界，上界$f(θ_T)-f(z)$对于任何参考点$z$。该界将迭代次数转化为损失函数中的有效正则化系数。我们通过训练动力学分析和预测风险界展示了这一框架。除了重新审视和改进已知的梯度下降结果外，我们还提供了镜像下降与Bregman散度投影、广义线性模型通过梯度下降和指数梯度下降训练、以及随机预测的新结果。我们通过广义线性模型的实验展示了并补充了这些理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes</div>
<div class="meta-line">Authors: Luca Collorone, Mert Kiray, Indro Spinelli, Fabio Galasso, Benjamin Busam</div>
<div class="meta-line">First: 2025-12-31T17:32:31+00:00 · Latest: 2025-12-31T17:32:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24986v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24986v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a &quot;render and wait&quot; paradigm toward an interactive dialogue with a modern, physics-informed pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysTalk: 3D 高斯场景中的语言驱动实时物理</div>
<div class="mono" style="margin-top:8px">逼真的视觉模拟无处不在，但其创建需要计算时间、渲染和专家动画知识。基于开放词汇的视觉效果生成从文本输入中脱颖而出，成为一种有望释放巨大创意潜力的解决方案。然而，当前的工作流程缺乏物理真实感和有效的语言接口，需要缓慢的离线优化。相比之下，PhysTalk 将 3D 高斯点绘（3DGS）场景作为输入，并将任意用户提示翻译成基于实时物理的 4D 交互式动画。一个大型语言模型（LLM）生成可执行代码，直接通过轻量级代理和粒子动力学修改 3DGS 参数。值得注意的是，PhysTalk 是第一个直接将 3DGS 与物理模拟器结合而无需依赖耗时的网格提取的框架。尽管保持开放词汇，这种设计使得通过碰撞感知的基于物理的操纵任意多材料对象的交互式 3D 高斯动画成为可能。最后，PhysTalk 是无训练的且计算量轻：这使得 4D 动画广泛可及，并将这些工作流程从“渲染和等待”的范式转向与现代、基于物理的管道进行互动对话。</div>
</details>
</div>
<div class="card">
<div class="title">DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments</div>
<div class="meta-line">Authors: Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh</div>
<div class="meta-line">First: 2025-12-31T17:31:29+00:00 · Latest: 2025-12-31T17:31:29+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Robotics and Automation Letters (RA-L)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24985v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24985v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline. We demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs&#x27; limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DarkEQA：在低光室内环境中的视觉语言模型体态问答基准测试</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）越来越多地被用作体态代理的核心推理模块。现有的基准测试在理想、光线充足的条件下评估其能力，但全天候24/7运行需要在各种视觉退化条件下表现出色，包括夜间或黑暗环境中的低光条件——这一核心需求已被很大程度上忽视。为应对这一未充分探索的挑战，我们提出了DarkEQA，这是一个开源基准测试，用于在多级低光条件下评估与体态问答相关的感知基本能力。DarkEQA通过在受控退化条件下从第一人称观察进行问答评估，隔离感知瓶颈，使可归因的鲁棒性分析成为可能。DarkEQA的一个关键设计特点是其物理保真度：视觉退化在线性RAW空间中建模，模拟基于物理的照明下降和传感器噪声，随后通过ISP启发式的渲染管道。我们通过评估一系列最先进的VLMs和低光图像增强（LLIE）模型展示了DarkEQA的实用性。我们的分析系统地揭示了这些视觉条件下的体态操作限制。我们的代码和基准数据集将在接受后发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">DarkEQA is a benchmark designed to evaluate the performance of Vision-Language Models (VLMs) in low-light indoor environments, addressing the underexplored challenge of robust 24/7 operation. The method involves degrading egocentric observations in a physically faithful manner, simulating low-light conditions through a linear RAW space model and an ISP-inspired rendering pipeline. Key findings show that state-of-the-art VLMs struggle with perceptual tasks under these conditions, highlighting their limitations in low-light environments.</div>
<div class="mono" style="margin-top:8px">DarkEQA 是一个基准，旨在评估 Vision-Language 模型在低光条件下的性能，解决了 24/7 运行中鲁棒性的不足挑战。它通过退化第一人称观察来隔离感知瓶颈，并使用物理保真的模型来模拟低光条件。关键发现表明，当前的 VLM 在这些条件下进行问答时表现不佳，突显了它们在低光环境中的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</div>
<div class="meta-line">Authors: Brandon Huang, Hang Hua, Zhuoran Yu, Trevor Darrell, Rogerio Feris, Roei Herzig</div>
<div class="meta-line">First: 2025-12-19T04:09:24+00:00 · Latest: 2025-12-31T17:30:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17221v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.17221v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder&#x27;s alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DAVE：一种用于文档理解和网络代理的VLM视觉编码器</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）在多模态任务中表现出色，但它们所选择的视觉编码器存在根本性弱点：其低级特征缺乏文档理解和网络代理所需的稳健的结构和空间信息。为弥补这一差距，我们引入了DAVE，一种专为VLMs设计并针对这些任务定制的视觉编码器。我们的训练管道旨在利用大量未标注数据，以绕过对文档和网络图像的大规模注释成本。我们首先在未标注图像上进行自我监督预训练阶段，然后在监督自回归预训练阶段，模型从有限的高质量数据中学习解析和定位等任务。在监督阶段内，我们采用了两种策略来提高编码器与通用视觉知识和多样化文档及网络代理任务的对齐：(i) 我们引入了一种新的模型合并方案，将使用不同文本解码器训练的编码器结合在一起，以确保与不同网络代理架构的广泛兼容性。(ii) 我们使用集成训练将预训练的通用编码器（例如SigLIP2）的特征与我们自己的文档和网络特定表示融合在一起。在经典文档任务、VQAs、网络定位和基于代理的基准测试中的广泛实验验证了我们方法的有效性，确立了DAVE作为文档和网络应用的强大视觉编码器的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the limitations of existing vision-language models (VLMs) in capturing structural and spatial information necessary for document understanding and web agents. DAVE, a specialized vision encoder, is introduced, trained through a self-supervised pretraining phase on unlabeled data and a supervised autoregressive phase using high-quality data. Key improvements include a model-merging scheme and ensemble training to enhance compatibility and performance. Experiments show DAVE outperforms existing models on document tasks, VQAs, web localization, and agent-based benchmarks, making it a robust vision encoder for these applications.</div>
<div class="mono" style="margin-top:8px">DAVE 是一种视觉编码器，旨在增强 Vision-language 模型（VLMs）在文档理解和网页代理任务中的结构和空间信息。它通过无监督和有监督预训练分别利用未标注和高质量数据。DAVE 结合了模型合并方案和集成训练，以确保广泛的兼容性和性能提升。实验结果表明，DAVE 在各种文档和网页任务中表现出色，使其成为这些应用中的稳健视觉编码器。</div>
</details>
</div>
<div class="card">
<div class="title">SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets</div>
<div class="meta-line">Authors: Barna Zajzon, Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Noah Ostendorf, Emre Neftci, Abigail Morrison, Renato Duarte</div>
<div class="meta-line">First: 2025-12-31T17:18:26+00:00 · Latest: 2025-12-31T17:18:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24977v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sequential structure is a key feature of multiple domains of natural cognition and behavior, such as language, movement and decision-making. Likewise, it is also a central property of tasks to which we would like to apply artificial intelligence. It is therefore of great importance to develop frameworks that allow us to evaluate sequence learning and processing in a domain agnostic fashion, whilst simultaneously providing a link to formal theories of computation and computability. To address this need, we introduce two complementary software tools: SymSeq, designed to rigorously generate and analyze structured symbolic sequences, and SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks to evaluate the performance of artificial learning systems in cognitively relevant domains. In combination, SymSeqBench offers versatility in investigating sequential structure across diverse knowledge domains, including experimental psycholinguistics, cognitive psychology, behavioral analysis, neuromorphic computing and artificial intelligence. Due to its basis in Formal Language Theory (FLT), SymSeqBench provides researchers in multiple domains with a convenient and practical way to apply the concepts of FLT to conceptualize and standardize their experiments, thus advancing our understanding of cognition and behavior through shared computational frameworks and formalisms. The tool is modular, openly available and accessible to the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SymSeqBench：基于规则的符号序列及其数据集生成与分析的统一框架</div>
<div class="mono" style="margin-top:8px">序列结构是自然认知和行为多个领域的关键特征，如语言、运动和决策。同样，这也是我们希望应用于人工智能任务的核心属性。因此，开发一种在不同领域中评估序列学习和处理的框架，同时与计算和可计算性的形式理论建立联系，是非常重要的。为满足这一需求，我们引入了两个互补的软件工具：SymSeq，用于严格生成和分析结构化的符号序列；SeqBench，一个基于规则的序列处理任务综合基准套件，用于评估人工学习系统在认知相关领域的性能。结合使用，SymSeqBench 提供了在不同知识领域研究序列结构的灵活性，包括实验心理语言学、认知心理学、行为分析、神经形态计算和人工智能。由于基于形式语言理论（FLT），SymSeqBench 为多个领域的研究人员提供了一种方便且实用的方法，将形式语言理论的概念应用于实验设计和标准化，从而通过共享的计算框架和形式化方法推进我们对认知和行为的理解。该工具是模块化的，公开可用，并且对研究界开放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">SymSeqBench is a unified framework for generating and analyzing rule-based symbolic sequences and datasets, addressing the need for domain-agnostic evaluation of sequence learning and processing. It consists of SymSeq for rigorous generation and analysis of structured symbolic sequences, and SeqBench as a comprehensive benchmark suite for evaluating artificial learning systems. Key findings include the versatility of SymSeqBench in investigating sequential structure across various domains such as psycholinguistics, cognitive psychology, and artificial intelligence, providing a practical way to apply Formal Language Theory concepts to standardize experiments and advance cognitive understanding through shared computational frameworks.</div>
<div class="mono" style="margin-top:8px">SymSeqBench 是一个统一框架，用于生成和分析基于规则的符号序列和数据集，旨在为序列学习和处理提供跨领域的评估。它包括 SymSeq 用于生成和分析结构化的符号序列，以及 SeqBench 作为全面的基准套件来评估人工学习系统的性能。关键发现包括 SymSeqBench 在语言学、认知心理学和人工智能等各个领域的灵活性，提供了一种实用的方法来应用形式语言理论的概念来标准化实验，从而通过共享的计算框架和形式化方法推进认知理解。</div>
</details>
</div>
<div class="card">
<div class="title">Distribution-Dependent Rates for Multi-Distribution Learning</div>
<div class="meta-line">Authors: Rafael Hanashiro, Patrick Jaillet</div>
<div class="meta-line">First: 2023-12-20T15:50:16+00:00 · Latest: 2025-12-31T17:05:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.13130v2">Abs</a> · <a href="https://arxiv.org/pdf/2312.13130v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To address the needs of modeling uncertainty in sensitive machine learning applications, the setup of distributionally robust optimization (DRO) seeks good performance uniformly across a variety of tasks. The recent multi-distribution learning (MDL) framework tackles this objective in a dynamic interaction with the environment, where the learner has sampling access to each target distribution. Drawing inspiration from the field of pure-exploration multi-armed bandits, we provide distribution-dependent guarantees in the MDL regime, that scale with suboptimality gaps and result in superior dependence on the sample size when compared to the existing distribution-independent analyses. We investigate two non-adaptive strategies, uniform and non-uniform exploration, and present non-asymptotic regret bounds using novel tools from empirical process theory. Furthermore, we devise an adaptive optimistic algorithm, LCB-DR, that showcases enhanced dependence on the gaps, mirroring the contrast between uniform and optimistic allocation in the multi-armed bandit literature. We also conduct a small synthetic experiment illustrating the comparative strengths of each strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>依赖分布的学习速率</div>
<div class="mono" style="margin-top:8px">为应对敏感机器学习应用中建模不确定性的需求，分布鲁棒优化（DRO）设置旨在实现各种任务上的统一良好性能。最近的多分布学习（MDL）框架通过与环境的动态交互来实现这一目标，其中学习者可以对每个目标分布进行采样访问。受到纯探索多臂bandit领域的启发，我们在MDL框架下提供了依赖分布的保证，这些保证与次优差距成比例，并且在样本量依赖性上优于现有的独立于分布的分析。我们研究了两种非自适应策略，均匀探索和非均匀探索，并使用经验过程理论中的新工具给出了非渐近的后悔界。此外，我们设计了一种自适应乐观算法LCB-DR，展示了对差距的增强依赖性，类似于多臂bandit文献中均匀分配和乐观分配之间的对比。我们还进行了一个小规模的合成实验，以说明每种策略的比较优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to improve the performance of machine learning models in uncertain environments by addressing the multi-distribution learning (MDL) framework. The authors provide distribution-dependent guarantees for MDL, which are more favorable in terms of sample size requirements compared to existing distribution-independent analyses. They explore two non-adaptive strategies, uniform and non-uniform exploration, and introduce an adaptive optimistic algorithm, LCB-DR, which demonstrates better performance in terms of suboptimality gaps. Experimental results show that LCB-DR outperforms the non-adaptive strategies in synthetic settings.</div>
<div class="mono" style="margin-top:8px">该论文旨在通过解决多分布学习（MDL）框架来提高机器学习模型在不确定环境中的性能。作者提供了针对MDL的分布依赖性保证，与现有的分布独立分析相比，这些保证在样本大小要求上更为有利。他们探索了两种非自适应策略，均匀探索和非均匀探索，并引入了一种自适应乐观算法LCB-DR，该算法在亚最优差距方面表现出更好的性能。实验结果表明，在合成环境中，LCB-DR优于非自适应策略。</div>
</details>
</div>
<div class="card">
<div class="title">ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands</div>
<div class="meta-line">Authors: Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou</div>
<div class="meta-line">First: 2025-12-31T16:51:14+00:00 · Latest: 2025-12-31T16:51:14+00:00</div>
<div class="meta-line">Comments: 17 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24965v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24965v1">PDF</a> · <a href="https://github.com/showlab/showui-pi">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-$π$, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents&#x27; drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-$π$ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShowUI-$π$: 基于流的生成模型作为GUI灵巧之手</div>
<div class="mono" style="margin-top:8px">构建能够进行灵巧操作的智能代理对于实现机器人和数字环境中的类人自动化至关重要。然而，现有的GUI代理依赖于离散的点击预测(x,y)，这禁止了自由形式、闭环轨迹（例如拖动进度条）的实现，这些轨迹需要连续的、实时的感知和调整。在本工作中，我们开发了ShowUI-$π$，这是第一个基于流的生成模型作为GUI灵巧之手，其设计包括：(i) 统一的离散-连续动作，将离散点击和连续拖动整合到一个共享模型中，以适应多种交互模式；(ii) 基于流的动作生成，用于拖动建模，通过轻量级的动作专家从连续的视觉观察中预测增量光标调整，确保平滑和稳定的轨迹；(iii) 拖动训练数据和基准，我们手动收集并合成了跨越五个领域（例如PowerPoint，Adobe Premiere Pro）的20,000条拖动轨迹，并引入了ScreenDrag基准，该基准具有全面的在线和离线评估协议，用于评估GUI代理的拖动能力。我们的实验表明，专有的GUI代理在ScreenDrag上仍然存在困难（例如Operator得分为13.27，而最好的Gemini-2.5-CUA达到22.18）。相比之下，ShowUI-$π$仅使用4.5亿参数就达到了26.98，这突显了任务的难度和我们方法的有效性。我们希望这项工作能够推动GUI代理向数字世界中的类人灵巧控制发展。代码可在https://github.com/showlab/showui-pi/获取。</div>
</details>
</div>
<div class="card">
<div class="title">Learning quadratic neural networks in high dimensions: SGD dynamics and scaling laws</div>
<div class="meta-line">Authors: Gérard Ben Arous, Murat A. Erdogdu, Nuri Mert Vural, Denny Wu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-08-05T17:57:56+00:00 · Latest: 2025-12-31T16:43:30+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03688v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.03688v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the optimization and sample complexity of gradient-based training of a two-layer neural network with quadratic activation function in the high-dimensional regime, where the data is generated as $f_*(\boldsymbol{x}) \propto \sum_{j=1}^{r}λ_j σ\left(\langle \boldsymbol{θ_j}, \boldsymbol{x}\rangle\right), \boldsymbol{x} \sim N(0,\boldsymbol{I}_d)$, $σ$ is the 2nd Hermite polynomial, and $\lbrace\boldsymbolθ_j \rbrace_{j=1}^{r} \subset \mathbb{R}^d$ are orthonormal signal directions. We consider the extensive-width regime $r \asymp d^β$ for $β\in [0, 1)$, and assume a power-law decay on the (non-negative) second-layer coefficients $λ_j\asymp j^{-α}$ for $α\geq 0$. We present a sharp analysis of the SGD dynamics in the feature learning regime, for both the population limit and the finite-sample (online) discretization, and derive scaling laws for the prediction risk that highlight the power-law dependencies on the optimization time, sample size, and model width. Our analysis combines a precise characterization of the associated matrix Riccati differential equation with novel matrix monotonicity arguments to establish convergence guarantees for the infinite-dimensional effective dynamics.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the optimization and sample complexity of training a two-layer neural network with quadratic activation in high dimensions. The data is generated as a sum of orthonormal signal directions with coefficients decaying according to a power-law. The research presents a detailed analysis of the stochastic gradient descent (SGD) dynamics and derives scaling laws for the prediction risk, highlighting dependencies on optimization time, sample size, and model width. The analysis combines a precise characterization of the matrix Riccati differential equation with novel matrix monotonicity arguments.</div>
<div class="mono" style="margin-top:8px">研究探讨了高维下具有二次激活函数的两层神经网络的优化和样本复杂性。数据生成为正交信号方向的线性组合，系数遵循幂律衰减。研究详细分析了随机梯度下降（SGD）的动力学，并推导出预测风险的标度律，突显了优化时间、样本量和模型宽度的依赖关系。分析结合了矩阵 Riccati 微分方程的精确表征和新型矩阵单调性论证，建立了无限维有效动力学的收敛保证。</div>
</details>
</div>
<div class="card">
<div class="title">AMAP Agentic Planning Technical Report</div>
<div class="meta-line">Authors: Yulan Hu, Xiangwen Zhang, Sheng Ouyang, Hao Yi, Lu Xu, Qinglin Lang, Lide Tan, Xiang Cheng, Tianchen Ye, Zhicong Li, Ge Chen, Wenjin Yang, Zheng Pan, Shaopan Xiong, Siran Yang, Ju Huang, Yan Zhang, Jiamang Wang, Yong Liu, Yinfeng Huang, Tucheng Lin, Xin Li, Ning Guo</div>
<div class="meta-line">First: 2025-12-31T16:39:09+00:00 · Latest: 2025-12-31T16:39:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24957v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24957v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AMAP自主规划技术报告</div>
<div class="mono" style="margin-top:8px">我们介绍了STAgent，这是一种针对时空理解定制的自主大型语言模型，旨在解决诸如受限兴趣点发现和行程规划等复杂任务。STAgent是一种专门模型，能够在时空场景中与十个不同的工具进行交互，使其能够在复杂推理过程中探索、验证和细化中间步骤。值得注意的是，STAgent有效地保留了其通用能力。我们通过三个关键贡献赋予STAgent这些能力：(1) 一个稳定的工具环境，支持超过十个领域特定工具，实现异步部署和训练；(2) 一个分层数据整理框架，能够从海量数据中识别高质量数据，筛选出高质量查询的比例为1:10,000，强调多样性和难度；(3) 一个级联训练方案，从种子SFT阶段开始，作为衡量查询难度的守护者，随后是针对高确定性查询的SFT微调阶段，最终利用低确定性数据的RL阶段。通过使用Qwen3-30B-A3B初始化以建立强大的SFT基础并利用样本难度的见解，STAgent在TravelBench上表现出色，同时在广泛的一般基准测试中保持其通用能力，从而证明了我们提出的自主模型的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">STAgent is an agentic large language model designed for spatio-temporal understanding and complex task solving, such as constrained point-of-interest discovery and itinerary planning. It integrates with ten spatio-temporal tools to explore, verify, and refine steps during reasoning. STAgent achieves this through a stable tool environment, a hierarchical data curation framework, and a cascaded training recipe. The model maintains its general capabilities while showing promising performance on TravelBench and other benchmarks.</div>
<div class="mono" style="margin-top:8px">STAgent 是一个用于时空理解及复杂任务解决的代理型大型语言模型，能够与十个工具交互以探索、验证和细化推理过程中的步骤。STAgent 通过稳定工具环境、分层数据整理框架和级联训练配方实现这一目标。该模型在 TravelBench 上表现出色，同时在多种通用基准测试中保持了其通用能力。</div>
</details>
</div>
<div class="card">
<div class="title">MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control</div>
<div class="meta-line">Authors: Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She</div>
<div class="meta-line">First: 2025-12-31T16:36:44+00:00 · Latest: 2025-12-31T16:36:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24955v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MSACL：利用李亚普诺夫证书的多步actor-critic学习方法以实现指数稳定控制</div>
<div class="mono" style="margin-top:8px">在无模型强化学习（RL）中实现可证明的稳定性仍然是一个挑战，特别是在探索与严格安全之间的平衡。本文介绍了一种MSACL框架，该框架通过多步李亚普诺夫证书学习将指数稳定性理论与最大熵RL相结合。与依赖复杂奖励工程的方法不同，MSACL利用离策略多步数据来学习满足理论稳定性条件的李亚普诺夫证书。通过引入指数稳定性标签（ESL）和λ加权聚合机制，该框架有效地在多步学习中平衡了偏差-方差权衡。通过稳定性意识的优势函数指导策略优化，确保学习到的策略促进快速的李亚普诺夫下降。我们在六个基准测试中评估了MSACL，包括稳定化和非线性跟踪任务，证明了其在最先进的基于李亚普诺夫的RL算法中的优越性。MSACL在简单奖励下实现了指数稳定性和快速收敛，并且对不确定性具有显著的鲁棒性，并且能够泛化到未见过的轨迹。敏感性分析确定了多步时间范围n=20为各种系统中的稳健默认值。通过将李亚普诺夫理论与离策略actor-critic框架相结合，MSACL为验证安全的学习控制奠定了基础。源代码和基准环境将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">MSACL is a framework that combines exponential stability theory with maximum entropy reinforcement learning to achieve provable stability in model-free RL. It uses off-policy multi-step data to learn Lyapunov certificates and introduces Exponential Stability Labels and a $λ$-weighted aggregation mechanism to balance bias and variance. MSACL outperforms state-of-the-art Lyapunov-based RL algorithms in six benchmarks, showing exponential stability, rapid convergence, and robustness to uncertainties. The multi-step horizon $n=20$ is found to be robust across various systems.</div>
<div class="mono" style="margin-top:8px">MSACL 是一种结合指数稳定性理论与最大熵强化学习的框架，以实现无模型自由学习中的可证明稳定性。它使用离策略多步数据来学习李雅普诺夫证书，并引入了指数稳定性标签和 $λ$ 加权聚合机制来平衡偏差和方差。MSACL 在六个基准测试中表现出色，包括指数稳定性、快速收敛和对不确定性的鲁棒性。20 步的多步时间间隔被发现是一个在不同系统中都稳健的默认值。通过将李雅普诺夫理论与离策略演员-评论家方法联系起来，MSACL 为安全的学习控制提供了基础。</div>
</details>
</div>
<div class="card">
<div class="title">A Geometric Theory of Cognition</div>
<div class="meta-line">Authors: Laha Ale</div>
<div class="meta-line">First: 2025-12-13T07:39:53+00:00 · Latest: 2025-12-31T16:33:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12225v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12225v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>认知的几何理论</div>
<div class="mono" style="margin-top:8px">人类的认知涵盖了感知、记忆、直觉判断、推理、行动选择和社会推理，但这些能力通常通过不同的计算理论来解释。在这里，我们提出了一种统一的数学框架，其中多种认知过程源自单一的几何原理。我们将认知状态表示为一个流形上的点，该流形上装备了由表示约束、计算成本和认知变量之间结构关系的学习黎曼度量。一个标量认知势能结合了预测准确性、结构简约性、任务效用和规范或逻辑要求。认知表现为这种势能的黎曼梯度流，提供了一个普遍的动力学定律，从中可以产生广泛的心理现象。经典的双重过程效应——快速的直觉反应和较慢的推理——自然地从度量诱导的各向异性中产生，从而产生内在的时间尺度分离和几何相变，而无需引入模块化或混合架构。我们推导了这些状态的分析条件，并通过模拟经典认知任务的行为特征来证明它们。这些结果为认知建立了一个几何基础，并为开发更通用和类人的人工智能系统提供了指导原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to unify various cognitive processes under a single geometric framework. The method involves representing cognitive states as points on a manifold with a learned Riemannian metric, and the cognitive potential combines predictive accuracy and task utility. Key findings include the emergence of dual-process effects from metric-induced anisotropies, without modular architectures, and the demonstration of these effects through simulations of cognitive tasks.</div>
<div class="mono" style="margin-top:8px">论文旨在通过单一几何框架统一各种认知过程。认知状态被表示为具有学习到黎曼度量的流形上的点，而认知则被描述为认知潜力的梯度流，该潜力包括预测准确性和计算成本。研究表明，快速直觉反应和较慢的审慎推理自然地从度量诱导的各向异性中产生，无需模块化架构。通过认知任务的模拟验证了理论预测。</div>
</details>
</div>
<div class="card">
<div class="title">VIPER: Process-aware Evaluation for Generative Video Reasoning</div>
<div class="meta-line">Authors: Yifan Li, Yukai Gu, Yingqian Min, Zikang Liu, Yifan Du, Kun Zhou, Min Yang, Wayne Xin Zhao, Minghui Qiu</div>
<div class="meta-line">First: 2025-12-31T16:31:59+00:00 · Latest: 2025-12-31T16:31:59+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24952v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24952v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve only about 20% POC@1.0 and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark will be publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIPER：基于过程的生成视频推理评估</div>
<div class="mono" style="margin-top:8px">近期在视频生成方面的突破展示了新兴的能力，称为连续帧推理（CoF），模型通过生成连续帧来解决复杂任务。尽管这些模型在生成视频推理（GVR）方面显示出潜力，但现有的评估框架通常依赖于单帧评估，这可能导致结果作弊，即模型通过错误的过程得出正确的结论。为了解决这一问题，我们提出了一种基于过程的评估范式。我们引入了VIPER，这是一个涵盖16个任务的全面基准，涉及时间、结构、符号、空间、物理和规划推理。此外，我们提出了过程-结果一致性（POC@r）这一新指标，该指标利用VLM作为评判者并采用分层评分标准，评估中间步骤的有效性和最终结果。我们的实验表明，最先进的视频模型在POC@1.0上的表现仅约为20%，显示出显著的结果作弊。我们进一步探讨了测试时缩放和采样鲁棒性的影响，突显了当前视频生成与真正泛化的视觉推理之间存在的巨大差距。我们的基准将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the issue of outcome-hacking in Generative Video Reasoning models by proposing a process-aware evaluation paradigm. VIPER, a comprehensive benchmark, evaluates 16 tasks across various reasoning types, and introduces POC@r, a new metric that assesses both intermediate steps and final results. Experiments show that state-of-the-art models achieve only about 20% POC@1.0, indicating significant outcome-hacking. The study also explores the impact of test-time scaling and sampling robustness, revealing a large gap between current video generation and true generalized visual reasoning.</div>
<div class="mono" style="margin-top:8px">研究旨在通过提出过程感知评估范式来解决生成视频推理（GVR）模型中的结果作弊问题。VIPER是一个新的基准，评估模型在涉及各种推理类型的16个任务上的表现。研究引入了POC@r指标，使用层次评判标准评估中间步骤的有效性和最终结果。实验表明，最先进的模型在POC@1.0上的得分仅为约20%，表明存在显著的结果作弊。工作还探讨了测试时缩放和采样鲁棒性的影响，揭示了当前视频生成与真正视觉推理能力之间的巨大差距。</div>
</details>
</div>
<div class="card">
<div class="title">ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT</div>
<div class="meta-line">Authors: Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou</div>
<div class="meta-line">First: 2025-12-31T16:29:05+00:00 · Latest: 2025-12-31T16:29:05+00:00</div>
<div class="meta-line">Comments: 21 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24948v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coronary artery calcium (CAC) scoring from chest CT is a well-established tool to stratify and refine clinical cardiovascular disease risk estimation. CAC quantification relies on the accurate delineation of calcified lesions, but is oftentimes affected by artifacts introduced by cardiac and respiratory motion. ECG-gated cardiac CTs substantially reduce motion artifacts, but their use in population screening and routine imaging remains limited due to gating requirements and lack of insurance coverage. Although identification of incidental CAC from non-gated chest CT is increasingly considered for it offers an accessible and widely available alternative, this modality is limited by more severe motion artifacts. We present ProDM (Property-aware Progressive Correction Diffusion Model), a generative diffusion framework that restores motion-free calcified lesions from non-gated CTs. ProDM introduces three key components: (1) a CAC motion simulation data engine that synthesizes realistic non-gated acquisitions with diverse motion trajectories directly from cardiac-gated CTs, enabling supervised training without paired data; (2) a property-aware learning strategy incorporating calcium-specific priors through a differentiable calcium consistency loss to preserve lesion integrity; and (3) a progressive correction scheme that reduces artifacts gradually across diffusion steps to enhance stability and calcium fidelity. Experiments on real patient datasets show that ProDM significantly improves CAC scoring accuracy, spatial lesion fidelity, and risk stratification performance compared with several baselines. A reader study on real non-gated scans further confirms that ProDM suppresses motion artifacts and improves clinical usability. These findings highlight the potential of progressive, property-aware frameworks for reliable CAC quantification from routine chest CT imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProDM：基于合成现实的属性感知渐进扩散模型用于非门控胸部CT冠状动脉钙化运动校正</div>
<div class="mono" style="margin-top:8px">冠状动脉钙化(CAC)评分来自胸部CT是一种成熟的工具，用于分层和细化临床心血管疾病风险估计。CAC量化依赖于钙化病灶的准确勾勒，但常常受到心脏和呼吸运动引入的伪影影响。心电图门控心脏CT显著减少了运动伪影，但由于门控要求和缺乏保险覆盖，其在人群筛查和常规成像中的应用受到限制。尽管从非门控胸部CT中识别偶然的CAC越来越多地被认为是一种可行的替代方案，因为它提供了更易于获取和广泛可用的替代方案，但该模态受限于更严重的运动伪影。我们提出了ProDM（属性感知渐进校正扩散模型），这是一种生成扩散框架，可以从非门控CT中恢复无运动的钙化病灶。ProDM 引入了三个关键组件：(1) 一种CAC运动模拟数据引擎，直接从心脏门控CT中合成具有多种运动轨迹的现实非门控获取，从而实现无需配对数据的监督训练；(2) 一种属性感知学习策略，通过可微分的钙化一致性损失整合钙化特定先验，以保持病灶完整性；(3) 一种渐进校正方案，在扩散步骤中逐步减少伪影，以增强稳定性和钙化准确性。在真实患者数据集上的实验表明，与几个基线相比，ProDM 显著提高了CAC评分准确性、空间病灶保真度和风险分层性能。在真实非门控扫描上的读者研究进一步证实，ProDM 抑制了运动伪影并提高了临床可用性。这些发现突显了渐进、属性感知框架在常规胸部CT成像中可靠CAC量化方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">ProDM is a generative diffusion framework designed to correct motion artifacts in non-gated chest CT scans for accurate coronary artery calcium (CAC) scoring. It includes a motion simulation engine, a calcium-specific learning strategy, and a progressive correction scheme. Experiments show that ProDM enhances CAC scoring accuracy and lesion fidelity, outperforming baseline methods and improving clinical usability.</div>
<div class="mono" style="margin-top:8px">ProDM 是一种生成扩散框架，旨在通过纠正非门控胸部 CT 扫描中的运动伪影来实现准确的冠状动脉钙化 (CAC) 评分。它包括运动模拟引擎、钙化特定的学习策略和渐进校正方案。实验表明，ProDM 提高了 CAC 评分的准确性和病灶保真度，优于基线方法并提高了临床实用性。</div>
</details>
</div>
<div class="card">
<div class="title">RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment</div>
<div class="meta-line">Authors: Chenji Lu, Zhuo Chen, Hui Zhao, Zhenyi Wang, Pengjie Wang, Jian Xu, Bo Zheng</div>
<div class="meta-line">First: 2025-12-31T16:09:08+00:00 · Latest: 2025-12-31T16:09:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24943v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24943v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAIR：一种结合长尾挑战和视觉显著性子集的规则意识基准，用于电子商务相关性评估</div>
<div class="mono" style="margin-top:8px">搜索相关性在网络电子商务中起着核心作用。虽然大型语言模型（LLMs）在相关性任务上取得了显著成果，但现有基准缺乏足够的复杂性，无法进行全面的模型评估，导致行业内缺乏标准化的相关性评估指标。为解决这一局限，我们提出了规则意识基准与图像相关性评估（RAIR），这是一个源自现实场景的中文数据集。RAIR 建立了一个标准化的相关性评估框架，并提供了一套通用规则，为标准化评估奠定了基础。此外，RAIR 分析了当前相关性模型所需的关键能力，并引入了一个综合数据集，包含三个子集：（1）一个行业平衡采样的通用子集，用于评估基本模型能力；（2）一个长尾难题子集，专注于具有挑战性的案例，以评估性能极限；（3）一个视觉显著性子集，用于评估多模态理解能力。我们在 RAIR 上使用了 14 个开源和闭源模型进行了实验。结果表明，即使对于表现最佳的 GPT-5，RAIR 也提出了足够的挑战。RAIR 数据现已可用，作为相关性评估的行业基准，同时为通用大语言模型（LLM）和视觉语言模型（VLM）评估提供了新的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper introduces RAIR, a benchmark for e-commerce search relevance assessment, addressing the lack of complexity in existing benchmarks. It consists of three subsets: a general subset for fundamental model competencies, a long-tail hard subset for challenging cases, and a visual salience subset for multimodal understanding. Experiments on 14 models, including GPT-5, show that RAIR presents significant challenges, even for the best-performing models.</div>
<div class="mono" style="margin-top:8px">论文提出了RAIR基准，用于电商搜索相关性评估，解决了现有基准缺乏复杂性的问题。该基准包含三个子集：一个通用子集用于评估基本模型能力、一个长尾难题子集用于评估极限性能、一个视觉显著性子集用于评估多模态理解能力。实验结果显示，即使对于表现最佳的模型GPT-5，RAIR也提供了显著的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Deployment Improves Planning Skills in LLMs</div>
<div class="meta-line">Authors: Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal</div>
<div class="meta-line">First: 2025-12-31T16:03:14+00:00 · Latest: 2025-12-31T16:03:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24940v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models&#x27; deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代部署提升大语言模型的规划技能</div>
<div class="mono" style="margin-top:8px">我们展示了对大型语言模型（LLM）进行迭代部署，每个模型都在用户精心挑选的数据上进行微调，可以显著改变模型的性质。通过在各种规划领域进行测试，我们观察到规划技能有了显著提高，后期模型通过发现更长的规划方案展示了潜在的泛化能力。我们还提供了理论分析，表明迭代部署实际上在外循环中实现了强化学习（RL）训练（即，不是作为有意图的模型训练的一部分），并隐含了一个奖励函数。与RL的联系有两个重要含义：首先，对于AI安全领域而言，由于反复部署所隐含的奖励函数没有明确定义，可能会对未来的模型部署产生意想不到的影响。其次，这里突出的机制可以被视为一种替代的训练方案，依赖于数据的挑选而非明确的奖励。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study demonstrates that iterative deployment of large language models, where each model is fine-tuned on data curated by users from the previous model&#x27;s deployment, significantly enhances planning skills. This approach, tested across various planning domains, results in later models discovering much longer plans than the initial models. Theoretical analysis suggests that this iterative process effectively implements reinforcement learning in the outer-loop, with an implicit reward function, which has implications for AI safety and offers an alternative training regime to explicit reinforcement learning.</div>
<div class="mono" style="margin-top:8px">研究展示了通过迭代部署大型语言模型，每个模型都基于用户从前一个模型部署中精心挑选的数据进行微调，可以显著提升模型的规划能力。后期模型表现出通过发现更长的计划而产生的泛化能力。理论分析表明，这种迭代部署实际上在外循环中实现了强化学习训练，具有隐含的奖励函数，这对人工智能安全有重要影响，并暗示了一种不同于显式强化学习的数据策展训练机制。</div>
</details>
</div>
<div class="card">
<div class="title">PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects</div>
<div class="meta-line">Authors: Huiming Yang, Linglin Liao, Fei Ding, Sibo Wang, Zijian Zeng</div>
<div class="meta-line">First: 2025-12-28T15:52:58+00:00 · Latest: 2025-12-31T15:59:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22979v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22979v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PoseStreamer is a multi-modal framework for 6DoF pose estimation of unseen moving objects, addressing the limitations of standard RGB cameras in high-speed and low-light scenarios. It integrates an Adaptive Pose Memory Queue, an Object-centric 2D Tracker, and a Ray Pose Filter to enhance temporal consistency, 2D to 3D translation, and geometric refinement, respectively. Experiments show that PoseStreamer outperforms existing methods in high-speed moving scenarios and demonstrates strong generalizability for unseen objects.</div>
<div class="mono" style="margin-top:8px">PoseStreamer 是一个用于高速移动物体的 6DoF 姿态估计的多模态框架，特别适用于高光速和低光照条件。它结合了自适应姿态记忆队列、对象中心的 2D 跟踪器和射线姿态滤波器，以增强时间一致性、2D 到 3D 转换和几何精化。实验表明，PoseStreamer 在高速移动场景中表现出色，并且对于未见过的移动物体具有很强的通用性。</div>
</details>
</div>
<div class="card">
<div class="title">ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</div>
<div class="meta-line">Authors: Abhijit Mishra, Mingda Li, Hsiang Fu, Richard Noh, Minji Kim</div>
<div class="meta-line">First: 2025-02-20T18:01:41+00:00 · Latest: 2025-12-31T15:43:05+00:00</div>
<div class="meta-line">Comments: Accepted and to appear in IJCNLP-AACL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14780v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.14780v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (&lt;500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReVision：一种用于隐私保护任务导向视觉指令重写的数据集和基线VLM</div>
<div class="mono" style="margin-top:8px">随着AR、VR和配备强大摄像头的现代智能手机成为人机通信的主要接口，高效的隐私保护多模态交互变得至关重要。现有的强大视觉-语言模型（VLMs）支持多模态交互，通常依赖于基于云的处理，这引发了（1）视觉隐私问题，即传输敏感的视觉数据到服务器，以及（2）其有限的实时、设备端可用性。本文探讨了视觉指令重写这一新颖的方法，即将多模态指令转换为纯文本命令，允许轻量级设备端指令重写VLM（参数量250M）与现有对话AI系统的无缝集成，增强视觉数据隐私。为此，我们提供了一个涵盖14个领域的超过39,000个示例的数据集，并开发了一个紧凑的VLM，该模型在图像字幕数据集上进行预训练，并针对指令重写进行了微调。实验结果通过NLG指标（如BLEU、METEOR和ROUGE）以及语义解析分析评估，表明即使是最小量化版本的模型（存储占用&lt;500MB）也能实现有效的指令重写，从而实现隐私导向的多模态AI应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for efficient and privacy-preserving multimodal interaction by introducing ReVision, a dataset and baseline vision-language model for visual instruction rewriting. The method involves transforming multimodal instructions into text-only commands to enhance privacy and on-device usability. Key experimental findings show that even a quantized version of the model can effectively rewrite instructions, achieving good performance on NLG metrics and enabling privacy-focused, multimodal AI applications.</div>
<div class="mono" style="margin-top:8px">该论文通过引入ReVision数据集和视觉指令重写的基本VLM，解决了高效且隐私保护的多模态交互需求。方法是使用一个预训练在图像描述数据集上并微调用于指令重写的紧凑型VLM（250M参数），将多模态指令转换为纯文本命令。实验结果表明，即使量化后的模型也能有效重写指令，从而增强视觉语言交互中的隐私保护。</div>
</details>
</div>
<div class="card">
<div class="title">HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition</div>
<div class="meta-line">Authors: Wang Lu, Yao Zhu, Jindong Wang</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-12-11T16:52:50+00:00 · Latest: 2025-12-31T15:41:01+00:00</div>
<div class="meta-line">Comments: Accepted by KDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10807v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.10807v3">PDF</a> · <a href="https://github.com/AIFrontierLab/HAROOD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HAROOD：基于传感器的人类活动识别离分布泛化基准</div>
<div class="mono" style="margin-top:8px">基于传感器的人类活动识别（HAR）从时间序列传感数据中挖掘活动模式。在现实场景中，个体、设备、环境和时间的变化为同一活动引入了显著的分布变化。最近的努力试图通过应用或适应现有的离分布（OOD）算法来解决这一挑战，但仅限于某些分布变化场景（例如，跨设备或跨位置），缺乏对这些算法有效性的全面了解。例如，HAR是否需要离分布？哪种OOD算法表现最佳？在本文中，我们通过提出HAROOD，一个全面的离分布设置下HAR的基准来填补这一空白。我们定义了4种离分布场景：跨个体、跨位置、跨数据集和跨时间，并构建了一个涵盖6个数据集、16种比较方法（使用CNN和Transformer架构实现）和两种模型选择协议的测试平台。然后，我们进行了广泛的实验并提出了几个未来研究的发现，例如，没有单一方法始终优于其他方法，突显了巨大的改进机会。我们的代码库高度模块化，易于扩展以添加新数据集、算法、比较和分析，以促进基于离分布的HAR研究。我们的实现已发布并可在https://github.com/AIFrontierLab/HAROOD/找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces HAROOD, a benchmark for evaluating out-of-distribution generalization in sensor-based human activity recognition. It defines four OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and evaluates 16 comparative methods using CNN and Transformer architectures. Extensive experiments reveal that no single method consistently outperforms others, indicating significant room for improvement in OOD generalization for HAR.</div>
<div class="mono" style="margin-top:8px">研究旨在评估在不同分布变化下的传感器基于的人体活动识别（HAR）中，现有脱机（OOD）算法的有效性。作者提出了HAROOD，一个全面的基准，涵盖了四种OOD场景：跨个体、跨位置、跨数据集和跨时间。他们测试了16种比较方法，使用了CNN和Transformer架构，并发现没有一种方法能够始终优于其他方法，表明在OOD-HAR领域存在巨大的改进空间。该基准设计为模块化且易于扩展，以便于新数据集、算法、比较和分析，旨在促进未来的研究。实现已公开，可在https://github.com/AIFrontierLab/HAROOD找到。</div>
</details>
</div>
<div class="card">
<div class="title">Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach</div>
<div class="meta-line">Authors: Yuchen Jiao, Na Li, Changxiao Cai, Gen Li</div>
<div class="meta-line">First: 2025-12-31T15:35:53+00:00 · Latest: 2025-12-31T15:35:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24927v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24927v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Higher-order ODE solvers have become a standard tool for accelerating diffusion probabilistic model (DPM) sampling, motivating the widespread view that first-order methods are inherently slower and that increasing discretization order is the primary path to faster generation. This paper challenges this belief and revisits acceleration from a complementary angle: beyond solver order, the placement of DPM evaluations along the reverse-time dynamics can substantially affect sampling accuracy in the low-neural function evaluation (NFE) regime.
  We propose a novel training-free, first-order sampler whose leading discretization error has the opposite sign to that of DDIM. Algorithmically, the method approximates the forward-value evaluation via a cheap one-step lookahead predictor. We provide theoretical guarantees showing that the resulting sampler provably approximates the ideal forward-value trajectory while retaining first-order convergence. Empirically, across standard image generation benchmarks (CIFAR-10, ImageNet, FFHQ, and LSUN), the proposed sampler consistently improves sample quality under the same NFE budget and can be competitive with, and sometimes outperform, state-of-the-art higher-order samplers. Overall, the results suggest that the placement of DPM evaluations provides an additional and largely independent design angle for accelerating diffusion sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一阶扩散采样器真的更慢吗？一种快速前向值方法</div>
<div class="mono" style="margin-top:8px">高阶ODE求解器已成为加速扩散概率模型(DPM)采样的标准工具，这促使人们普遍认为一阶方法本质上更慢，并且提高离散化阶数是实现更快生成的主要途径。本文挑战了这一观点，并从互补的角度重新审视加速：除了求解器阶数外，DPM评估在反向时间动力学中的位置会在低神经网络评估次数(NFE)区间内显著影响采样精度。
我们提出了一种新的无需训练的一阶采样器，其主要离散化误差与DDIM相反。算法上，该方法通过廉价的一步前瞻预测器近似前向值评估。我们提供了理论保证，表明该采样器能够证明地逼近理想的前向值轨迹，同时保持一阶收敛性。实验上，在标准图像生成基准(CIFAR-10、ImageNet、FFHQ和LSUN)上，所提出的采样器在相同的NFE预算下始终能提高样本质量，并且有时可以与最先进的高阶采样器竞争。总体而言，结果表明，DPM评估的位置提供了加速扩散采样的另一个独立设计角度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper challenges the notion that first-order diffusion samplers are inherently slower than higher-order methods. It introduces a novel first-order sampler that approximates forward-value evaluation through a cheap one-step lookahead predictor, providing theoretical guarantees of first-order convergence and accurate approximation of the ideal forward-value trajectory. Experiments on standard image generation benchmarks show that the proposed sampler consistently improves sample quality under the same neural function evaluation budget and can outperform state-of-the-art higher-order samplers in some cases.</div>
<div class="mono" style="margin-top:8px">本文挑战了一阶扩散采样器本质上比高阶方法更慢的观点。提出了一种新型的一阶采样器，通过廉价的一步前瞻预测器近似前向值评估，实现了一阶收敛性的同时，在相同的神经函数评估预算下提高了样本质量。实验结果显示，该采样器在各种图像生成基准上优于或匹配最先进的高阶采样器。</div>
</details>
</div>
<div class="card">
<div class="title">Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</div>
<div class="meta-line">Authors: Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak</div>
<div class="meta-line">First: 2025-12-31T15:26:09+00:00 · Latest: 2025-12-31T15:26:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24922v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24922v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>半监督多样性感知领域适应性调整方法在3D物体检测中的应用</div>
<div class="mono" style="margin-top:8px">3D物体检测器是自动驾驶感知系统中的基本组件。尽管这些检测器在标准的自动驾驶基准测试中表现出色，但在不同领域中的泛化能力却常常不足——例如，一个在美国训练的模型可能在亚洲或欧洲地区表现不佳。本文提出了一种基于神经元激活模式的激光雷达领域适应方法，表明通过正确选择一小部分具有代表性和多样性的目标域样本进行标注，可以实现最先进的性能。所提出的方法需要非常小的标注预算，并且当与由持续学习启发的后训练技术结合使用时，可以防止权重从原始模型中漂移。实证评估表明，所提出的方法在领域适应性能上优于线性探针和最先进的领域适应技术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of 3D object detectors generalizing across different domains, such as from the U.S. to Asia or Europe. The authors propose a semi-supervised diversity-aware domain adaptation method that uses neuron activation patterns to annotate a small, representative, and diverse subset of target domain samples. This approach requires minimal annotation effort and integrates post-training techniques to prevent weight drift from the original model. Experimental results show that this method outperforms both linear probing and existing state-of-the-art domain adaptation techniques.</div>
<div class="mono" style="margin-top:8px">该论文提出了一种基于神经元激活模式的半监督方法，用于解决自动驾驶车辆中3D物体检测模型在不同域之间的适应性问题。该方法仅需对目标域的少量多样样本进行标注，并结合连续学习启发的后训练技术防止模型漂移。实验结果表明，该适应性方法在小标注预算下优于线性探针和现有最先进的域适应技术，实现了最佳性能。</div>
</details>
</div>
<div class="card">
<div class="title">Frequent subgraph-based persistent homology for graph classification</div>
<div class="meta-line">Authors: Xinyang Chen, Amaël Broustet, Guoting Chen</div>
<div class="meta-line">First: 2025-12-31T15:21:15+00:00 · Latest: 2025-12-31T15:21:15+00:00</div>
<div class="meta-line">Comments: Preprint. 18 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24917v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Persistent homology (PH) has recently emerged as a powerful tool for extracting topological features. Integrating PH into machine learning and deep learning models enhances topology awareness and interpretability. However, most PH methods on graphs rely on a limited set of filtrations, such as degree-based or weight-based filtrations, which overlook richer features like recurring information across the dataset and thus restrict expressive power. In this work, we propose a novel graph filtration called Frequent Subgraph Filtration (FSF), which is derived from frequent subgraphs and produces stable and information-rich frequency-based persistent homology (FPH) features. We study the theoretical properties of FSF and provide both proofs and experimental validation. Beyond persistent homology itself, we introduce two approaches for graph classification: an FPH-based machine learning model (FPH-ML) and a hybrid framework that integrates FPH with graph neural networks (FPH-GNNs) to enhance topology-aware graph representation learning. Our frameworks bridge frequent subgraph mining and topological data analysis, offering a new perspective on topology-aware feature extraction. Experimental results show that FPH-ML achieves competitive or superior accuracy compared with kernel-based and degree-based filtration methods. When integrated into graph neural networks, FPH yields relative performance gains ranging from 0.4 to 21 percent, with improvements of up to 8.2 percentage points over GCN and GIN backbones across benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于频繁子图的持久同调法在图分类中的应用</div>
<div class="mono" style="margin-top:8px">持久同调（PH）最近已成为提取拓扑特征的强大工具。将PH集成到机器学习和深度学习模型中增强了拓扑意识和可解释性。然而，大多数图上的PH方法依赖于有限的滤波器集，如度数基或权重基滤波器，这忽略了数据集中反复出现的信息，从而限制了表达能力。在本文中，我们提出了一种新的图滤波器，称为频繁子图滤波器（FSF），它源自频繁子图并产生稳定且信息丰富的基于频率的持久同调（FPH）特征。我们研究了FSF的理论性质并提供了证明和实验验证。除了持久同调本身，我们还介绍了两种图分类方法：基于FPH的机器学习模型（FPH-ML）和将FPH与图神经网络（FPH-GNNs）结合的混合框架，以增强拓扑感知的图表示学习。我们的框架将频繁子图挖掘与拓扑数据分析相结合，提供了拓扑感知特征提取的新视角。实验结果表明，FPH-ML在与核基和度数基滤波器方法相比时，实现了竞争力或更优的准确性。当集成到图神经网络中时，FPH在基准测试中相对性能增益范围从0.4%到21%，并在GCN和GIN主干上最高提高了8.2个百分点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces Frequent Subgraph Filtration (FSF) for graph classification, which derives persistent homology features from frequent subgraphs, offering richer and more stable information compared to traditional degree or weight-based filtrations. The method is integrated into both machine learning models (FPH-ML) and graph neural networks (FPH-GNNs), demonstrating competitive or superior accuracy to kernel-based and degree-based methods. Integrating FPH into GNNs improves performance by up to 8.2 percentage points across various benchmarks.</div>
<div class="mono" style="margin-top:8px">本文提出了用于图分类的频繁子图过滤（FSF）方法，通过关注重复的子图模式来增强持久同调（PH）。该方法生成稳定且信息丰富的频率基PH特征。提出了两种图分类框架：FPH-ML用于机器学习，FPH-GNNs用于将PH与图神经网络结合。实验表明，FPH-ML在与核基和度基方法相比时达到了竞争力的准确度，而FPH-GNNs在GCN和GIN基线上的相对性能增益高达21％。</div>
</details>
</div>
<div class="card">
<div class="title">AI-Driven Cloud Resource Optimization for Multi-Cluster Environments</div>
<div class="meta-line">Authors: Vinoth Punniyamoorthy, Akash Kumar Agarwal, Bikesh Kumar, Abhirup Mazumder, Kabilan Kannan, Sumit Saha</div>
<div class="meta-line">First: 2025-12-31T15:15:46+00:00 · Latest: 2025-12-31T15:15:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24914v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24914v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多集群环境下的AI驱动云资源优化</div>
<div class="mono" style="margin-top:8px">现代云原生系统越来越多地依赖多集群部署以支持可扩展性、弹性和地理分布。然而，现有的资源管理方法仍然主要具有反应性和集群中心化的特点，限制了它们在动态工作负载下优化系统级行为的能力。这些限制导致资源利用效率低下、适应延迟和分布式环境中的操作开销增加。本文提出了一种用于多集群云系统自适应资源优化的AI驱动框架。所提出的方法将预测学习、策略感知决策和持续反馈相结合，以实现跨集群的主动和协调资源管理。通过分析跨集群遥测数据和历史执行模式，该框架动态调整资源分配以平衡性能、成本和可靠性目标。原型实现表明，与传统的反应性方法相比，该方法在资源效率、工作负载波动期间更快的稳定性和降低的性能变异性方面具有优势。结果突显了智能、自适应基础设施管理对于可扩展和弹性云平台的关键作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the inefficiencies in resource management of multi-cluster cloud systems by proposing an AI-driven framework. This framework uses predictive learning, policy-aware decision-making, and continuous feedback to optimize resource allocation across clusters. Experimental results show enhanced resource efficiency, faster stabilization during workload changes, and reduced performance variability compared to traditional reactive methods.</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于AI的框架来解决多集群云系统中的资源管理效率问题。该框架利用预测学习、策略感知决策和持续反馈来跨集群优化资源分配。实验结果表明，与传统的反应式方法相比，该框架能够提高资源效率、加快工作负载波动期间的稳定速度，并减少性能波动。</div>
</details>
</div>
<div class="card">
<div class="title">FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</div>
<div class="meta-line">Authors: Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-12-31T15:00:03+00:00 · Latest: 2025-12-31T15:00:03+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI-26 Main Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24903v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24903v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FinMMDocR：基于情景意识、文档理解和多步计算的金融多模态推理基准</div>
<div class="mono" style="margin-top:8px">我们介绍了FinMMDocR，这是一种新的双语多模态基准，用于评估多模态大型语言模型（MLLMs）在现实世界金融数值推理中的表现。与现有基准相比，我们的工作带来了三项重大进步。(1) 情景意识：1200个专家标注的问题中有57.9%包含12种类型的隐含金融情景（例如，投资组合管理），挑战模型进行基于假设的专家级推理；(2) 文档理解：837份中文/英文文档涵盖9种类型（例如，公司研究），平均每份50.8页，包含丰富的视觉元素，显著超越现有基准在金融文档的广度和深度上；(3) 多步计算：问题平均需要11步推理（5.3步提取+5.7步计算步骤），其中65.0%需要跨页证据（平均2.4页）。表现最好的MLLM仅达到58.0%的准确率，不同的检索增强生成（RAG）方法在该任务上的表现存在显著差异。我们期望FinMMDocR能够推动MLLMs和增强推理方法在现实世界复杂多模态推理任务中的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">FinMMDocR is a new bilingual multimodal benchmark for evaluating MLLMs on real-world financial reasoning. It introduces scenario awareness, document understanding, and multi-step computation challenges. The benchmark includes 1,200 expert-annotated problems with 57.9% involving financial scenarios, 837 documents with rich visual elements, and multi-step reasoning tasks requiring 11 steps on average. The best MLLM achieves only 58.0% accuracy, highlighting the need for improved reasoning capabilities in MLLMs.</div>
<div class="mono" style="margin-top:8px">FinMMDocR 是一个新颖的双语多模态基准，用于评估 MLLMs 在现实世界金融推理中的表现。它引入了情景意识、文档理解和多步计算，挑战模型处理 12 种隐含的金融情景、837 份包含丰富视觉元素的文档以及 11 步推理。最佳 MLLM 的准确率仅为 58.0%，突显了对 MLLMs 和推理增强方法改进的需求。</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: A hierarchical multiscale approach for time series forecasting</div>
<div class="meta-line">Authors: Zihao Chen, Alexandre Andre, Wenrui Ma, Ian Knight, Sergey Shuvaev, Eva Dyer</div>
<div class="meta-line">First: 2025-12-31T14:51:12+00:00 · Latest: 2025-12-31T14:51:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24898v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24898v1">PDF</a> · <a href="https://github.com/nerdslab/prism">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Forecasting is critical in areas such as finance, biology, and healthcare. Despite the progress in the field, making accurate forecasts remains challenging because real-world time series contain both global trends, local fine-grained structure, and features on multiple scales in between. Here, we present a new forecasting method, PRISM (Partitioned Representation for Iterative Sequence Modeling), that addresses this challenge through a learnable tree-based partitioning of the signal. At the root of the tree, a global representation captures coarse trends in the signal, while recursive splits reveal increasingly localized views of the signal. At each level of the tree, data are projected onto a time-frequency basis (e.g., wavelets or exponential moving averages) to extract scale-specific features, which are then aggregated across the hierarchy. This design allows the model to jointly capture global structure and local dynamics of the signal, enabling accurate forecasting. Experiments across benchmark datasets show that our method outperforms state-of-the-art methods for forecasting. Overall, these results demonstrate that our hierarchical approach provides a lightweight and flexible framework for forecasting multivariate time series. The code is available at https://github.com/nerdslab/prism.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRISM：一种分层多尺度方法用于时间序列预测</div>
<div class="mono" style="margin-top:8px">预测在金融、生物学和医疗保健等领域至关重要。尽管该领域取得了进展，但准确预测仍然具有挑战性，因为真实世界的时间序列包含全局趋势、局部精细结构以及介于两者之间的多种尺度特征。为此，我们提出了一种新的预测方法——PRISM（Partitioned Representation for Iterative Sequence Modeling），该方法通过可学习的树状分割信号来应对这一挑战。树的根节点捕获信号中的粗略趋势，而递归分割则揭示信号的越来越局部化的视图。在树的每一层，数据被投影到时频基（例如小波或指数移动平均）上以提取特定尺度的特征，然后在层次结构中进行聚合。这种设计使模型能够同时捕捉信号的全局结构和局部动态，从而实现准确的预测。在基准数据集上的实验表明，我们的方法在预测方面优于最先进的方法。总体而言，这些结果表明，我们的分层方法为多变量时间序列预测提供了一个轻量级且灵活的框架。代码可在https://github.com/nerdslab/prism/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">PRISM is a hierarchical multiscale approach for time series forecasting that addresses the challenge of capturing both global trends and local fine-grained structures. It uses a learnable tree-based partitioning of the signal, where global trends are captured at the root and increasingly localized views are revealed through recursive splits. Scale-specific features are extracted at each level and aggregated to enable accurate forecasting. Experiments show that PRISM outperforms state-of-the-art methods on benchmark datasets, demonstrating its effectiveness for forecasting multivariate time series.</div>
<div class="mono" style="margin-top:8px">PRISM 是一种用于时间序列预测的分层多尺度方法，旨在捕捉全局趋势和局部精细结构。该方法通过可学习的树状分割信号，在根节点捕获全局趋势，并通过递归分割逐步揭示越来越局部的视图。在每一层中提取特定尺度的特征并进行聚合，以实现准确的预测。实验表明，PRISM 在基准数据集上的表现优于最先进的方法，证明了其在多变量时间序列预测中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Large Multimodal Models for Low-Resource Languages: A Survey</div>
<div class="meta-line">Authors: Marian Lupascu, Ana-Cristina Rogoz, Mihai Sorin Stupariu, Radu Tudor Ionescu</div>
<div class="meta-line">First: 2025-02-08T13:29:44+00:00 · Latest: 2025-12-31T14:45:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.05568v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.05568v2">PDF</a> · <a href="https://github.com/marianlupascu/LMM4LRL-Survey">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模多模态模型在低资源语言中的应用：综述</div>
<div class="mono" style="margin-top:8px">在本文综述中，我们系统地分析了用于适应低资源语言（LR）的大规模多模态模型（LMMs）的技术，涵盖了从视觉增强和数据生成到跨模态转移和融合策略的各种方法。通过对96种LR语言的117项研究进行全面分析，我们识别出研究人员在应对数据和计算资源有限的挑战时的关键模式。我们将工作分为资源导向和方法导向两类，并进一步细分为相关子类别。我们按性能和效率比较了方法导向类别的贡献，讨论了代表性研究的优势和局限性。我们发现，视觉信息通常在LR环境中作为提高模型性能的关键桥梁，尽管在幻觉抑制和计算效率等方面仍存在重大挑战。总之，我们为研究人员提供了关于当前方法和剩余挑战的清晰理解，使LMMs更易于被LR（未充分研究）语言的使用者使用。我们还通过以下链接提供了开源仓库：https://github.com/marianlupascu/LMM4LRL-Survey。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey analyzes techniques for adapting large multimodal models to low-resource languages, examining visual enhancement, data creation, cross-modal transfer, and fusion strategies. By reviewing 117 studies across 96 low-resource languages, the authors identify key patterns and categorize contributions into resource-oriented and method-oriented categories. They find that visual information significantly improves model performance but highlight challenges like hallucination mitigation and computational efficiency. The survey provides a comprehensive understanding of current approaches and remaining challenges in making large multimodal models more accessible to speakers of low-resource languages.</div>
<div class="mono" style="margin-top:8px">本文综述了将大型多模态模型应用于低资源语言的技术，分析了视觉增强、数据创建、跨模态转移和融合策略。通过对96种低资源语言的117项研究进行综合分析，作者识别了关键模式，并将工作分为资源导向和方法导向两类。他们发现视觉信息对于提高模型性能至关重要，但在幻觉抑制和计算效率等方面仍面临挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing</div>
<div class="meta-line">Authors: Andrii Gamalii, Daniel Górniak, Robert Nowak, Bartłomiej Olber, Krystian Radlak, Jakub Winter</div>
<div class="meta-line">First: 2025-12-31T14:43:48+00:00 · Latest: 2025-12-31T14:43:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24896v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24896v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project&#x27;s standardized format, strengthening the technological base for autonomous vehicle research in Poland.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多传感器数据集在自动驾驶车辆测试中的半自动化数据标注</div>
<div class="mono" style="margin-top:8px">本报告介绍了在DARTS项目中设计和实现的一种半自动化数据标注流水线，其目标是在波兰条件下创建一个大规模的多模态驾驶场景数据集。手动标注此类异构数据既昂贵又耗时。为应对这一挑战，所提出的解决方案采用了一种人工在环的方法，结合人工智能与人类专业知识，以降低标注成本和时间。系统自动生成初始标注，支持迭代模型重新训练，并采用数据匿名化和领域适应技术。该工具的核心依赖于3D物体检测算法以生成初步标注。总体而言，开发的工具和方法在确保不同传感器模态之间一致性和高质量标注的同时，节省了大量时间。该解决方案直接支持DARTS项目，通过加速准备符合项目标准化格式的大规模标注数据集，加强了波兰自动驾驶车辆研究的技术基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This report describes a semi-automated data annotation pipeline designed to reduce the cost and time of annotating multimodal driving scenario datasets for autonomous vehicle testing. The pipeline combines AI with human oversight to generate initial annotations, iteratively refine them, and include anonymization and domain adaptation techniques. Key findings show that this approach significantly saves time while maintaining high-quality annotations across various sensor types, supporting the DARTS project&#x27;s goal of creating a large-scale annotated dataset.</div>
<div class="mono" style="margin-top:8px">研究旨在解决手动标注异构数据在自动驾驶车辆测试中的高成本和耗时问题。它提出了一种半自动数据标注流水线，结合AI与人工监督，生成初始标注，迭代优化，并采用匿名化和领域适应技术。系统使用3D物体检测算法生成初步标注，显著减少标注时间，同时保持不同传感器模态下高质量、一致的标注。该方法支持DARTS项目，通过加速创建大规模多模态数据集并符合项目标准化格式来加强波兰的自动驾驶技术基础。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260102_0335.html">20260102_0335</a>
<a href="archive/20260101_0325.html">20260101_0325</a>
<a href="archive/20251231_0331.html">20251231_0331</a>
<a href="archive/20251230_0328.html">20251230_0328</a>
<a href="archive/20251229_0326.html">20251229_0326</a>
<a href="archive/20251228_0329.html">20251228_0329</a>
<a href="archive/20251227_0325.html">20251227_0325</a>
<a href="archive/20251226_0326.html">20251226_0326</a>
<a href="archive/20251225_0325.html">20251225_0325</a>
<a href="archive/20251224_0328.html">20251224_0328</a>
<a href="archive/20251223_0327.html">20251223_0327</a>
<a href="archive/20251222_0324.html">20251222_0324</a>
<a href="archive/20251221_0326.html">20251221_0326</a>
<a href="archive/20251220_0327.html">20251220_0327</a>
<a href="archive/20251219_0327.html">20251219_0327</a>
<a href="archive/20251218_0339.html">20251218_0339</a>
<a href="archive/20251217_0331.html">20251217_0331</a>
<a href="archive/20251216_0329.html">20251216_0329</a>
<a href="archive/20251215_0331.html">20251215_0331</a>
<a href="archive/20251214_0324.html">20251214_0324</a>
<a href="archive/20251213_0324.html">20251213_0324</a>
<a href="archive/20251212_0329.html">20251212_0329</a>
<a href="archive/20251211_0326.html">20251211_0326</a>
<a href="archive/20251210_0323.html">20251210_0323</a>
<a href="archive/20251209_0326.html">20251209_0326</a>
<a href="archive/20251208_0324.html">20251208_0324</a>
<a href="archive/20251207_0323.html">20251207_0323</a>
<a href="archive/20251206_0325.html">20251206_0325</a>
<a href="archive/20251205_0326.html">20251205_0326</a>
<a href="archive/20251204_0326.html">20251204_0326</a>
<a href="archive/20251203_0328.html">20251203_0328</a>
<a href="archive/20251202_0331.html">20251202_0331</a>
<a href="archive/20251201_0324.html">20251201_0324</a>
<a href="archive/20251130_0323.html">20251130_0323</a>
<a href="archive/20251129_0323.html">20251129_0323</a>
<a href="archive/20251128_0324.html">20251128_0324</a>
<a href="archive/20251127_0324.html">20251127_0324</a>
<a href="archive/20251126_0325.html">20251126_0325</a>
<a href="archive/20251125_0322.html">20251125_0322</a>
<a href="archive/20251124_0323.html">20251124_0323</a>
<a href="archive/20251123_0323.html">20251123_0323</a>
<a href="archive/20251122_0325.html">20251122_0325</a>
<a href="archive/20251121_0324.html">20251121_0324</a>
<a href="archive/20251120_0326.html">20251120_0326</a>
<a href="archive/20251119_0325.html">20251119_0325</a>
<a href="archive/20251118_0324.html">20251118_0324</a>
<a href="archive/20251117_0322.html">20251117_0322</a>
<a href="archive/20251116_0322.html">20251116_0322</a>
<a href="archive/20251115_0324.html">20251115_0324</a>
<a href="archive/20251114_0325.html">20251114_0325</a>
<a href="archive/20251113_0326.html">20251113_0326</a>
<a href="archive/20251112_0326.html">20251112_0326</a>
<a href="archive/20251111_0318.html">20251111_0318</a>
<a href="archive/20251110_0322.html">20251110_0322</a>
<a href="archive/20251109_0323.html">20251109_0323</a>
<a href="archive/20251108_0321.html">20251108_0321</a>
<a href="archive/20251107_0320.html">20251107_0320</a>
<a href="archive/20251106_0322.html">20251106_0322</a>
<a href="archive/20251105_0321.html">20251105_0321</a>
<a href="archive/20251104_0324.html">20251104_0324</a>
<a href="archive/20251103_0317.html">20251103_0317</a>
<a href="archive/20251102_0321.html">20251102_0321</a>
<a href="archive/20251101_0317.html">20251101_0317</a>
<a href="archive/20251031_0318.html">20251031_0318</a>
<a href="archive/20251030_0328.html">20251030_0328</a>
<a href="archive/20251029_0325.html">20251029_0325</a>
<a href="archive/20251028_0324.html">20251028_0324</a>
<a href="archive/20251027_0320.html">20251027_0320</a>
<a href="archive/20251026_0328.html">20251026_0328</a>
<a href="archive/20251025_0320.html">20251025_0320</a>
<a href="archive/20251024_0328.html">20251024_0328</a>
<a href="archive/20251023_1235.html">20251023_1235</a>
<a href="archive/20251023_0316.html">20251023_0316</a>
<a href="archive/20251022_0319.html">20251022_0319</a>
<a href="archive/20251021_1916.html">20251021_1916</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
